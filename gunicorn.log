nohup: ignoring input
[2025-02-23 06:33:31 +0000] [70108] [INFO] Starting gunicorn 23.0.0
[2025-02-23 06:33:31 +0000] [70108] [INFO] Listening at: http://0.0.0.0:8000 (70108)
[2025-02-23 06:33:31 +0000] [70108] [INFO] Using worker: sync
[2025-02-23 06:33:31 +0000] [70109] [INFO] Booting worker with pid: 70109
[2025-02-23 06:33:31 +0000] [70110] [INFO] Booting worker with pid: 70110
[2025-02-23 06:33:31 +0000] [70111] [INFO] Booting worker with pid: 70111

=== Starting Quiz: AWS_CCP_M5 ===
Created new attempt ID: 72e34c28-4130-448c-9170-55e18c798bb4

=== Quiz Result Debug ===
Question: According to the AWS Shared Responsibility Model, which of the following are the responsibilities of AWS? (Select two)
Has explanation: True
Explanation: Data center security
Has option_explanations: True
Option explanations: ['Encrypting application data - The customers are responsible for encrypting application data.', 'Configuring IAM Roles - The customers are responsible for configuring IAM Roles. \n\nReference: \n\nhttps://aws.amazon.com/compliance/shared-responsibility-model/', 'Data center security', 'Installing security patches of the guest operating system (OS) - The customers are responsible for patching their guest operating system. \n\nPlease review the IT controls under the AWS Shared Responsibility Model: via - https://aws.amazon.com/compliance/shared-responsibility-model/', 'Network operability \n\nAWS responsibility \x93Security OF the Cloud\x94 - AWS is responsible for protecting the infrastructure that runs all of the services offered in the AWS Cloud. This infrastructure is composed of the hardware, software, networking, and facilities that run AWS Cloud services.']
Question: A Cloud Practitioner would like to deploy identical resources across all AWS regions and accounts using templates while estimating costs. Which AWS service can assist with this task?
Has explanation: True
Explanation: AWS CloudFormation

AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.

You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. This provides a single source of truth for all your resources and helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting.

AWS CloudFormation templates allow you to estimate the cost of your resources.

How AWS CloudFormation works: via - https://aws.amazon.com/cloudformation/
Has option_explanations: True
Option explanations: ['AWS CodeDeploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. Unlike AWS CloudFormation, it does not deal with infrastructure configuration and orchestration. \n\nReference: \n\nhttps://aws.amazon.com/cloudformation/', 'Amazon LightSail - Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server with AWS. It is not best suited when deploying more complex resources, while AWS CloudFormation can.', 'AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) - AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD), also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. It is not used to deploy resources.', 'AWS CloudFormation\n\nAWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\n\nYou can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. This provides a single source of truth for all your resources and helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting.\n\nAWS CloudFormation templates allow you to estimate the cost of your resources.\n\nHow AWS CloudFormation works: via - https://aws.amazon.com/cloudformation/']
Question: Which AWS service can be used to subscribe to an RSS feed to be notified of the status of all AWS service interruptions?
Has explanation: True
Explanation: AWS Health Dashboard - Service Health 

The AWS Health Dashboard  Service health is the single place to learn about the availability and operations of AWS services. You can view the overall status of AWS services, and you can sign in to view personalized communications about your particular AWS account or organization. 

You can check on this page https://health.aws.amazon.com/health/status to get current status information. 

The AWS Health Dashboard  Service health offers the possibility to subscribe to an RSS feed to be notified of interruptions to each service.
Has option_explanations: True
Option explanations: ["Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. It can be used to deliver notifications, but it does not provide the current services' status.", 'AWS Health Dashboard - Your Account Health - Your AWS Health Dashboard \x96 Your Account Health provides alerts and remediation guidance when AWS is experiencing events that may impact you.', 'AWS Health Dashboard - Service Health \n\nThe AWS Health Dashboard \x96 Service health is the single place to learn about the availability and operations of AWS services. You can view the overall status of AWS services, and you can sign in to view personalized communications about your particular AWS account or organization. \n\nYou can check on this page https://health.aws.amazon.com/health/status to get current status information. \n\nThe AWS Health Dashboard \x96 Service health offers the possibility to subscribe to an RSS feed to be notified of interruptions to each service.', "AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It does not provide all AWS services' status. \n\nReference: \n\nhttps://health.aws.amazon.com/health/status"]
Question: A production company would like to establish an AWS managed virtual private network (VPN) service between its on-premises network and AWS. Which item needs to be set up on the company's side?
Has explanation: True
Explanation: A customer gateway 

A customer gateway device is a physical or software appliance on your side of a Site-to-Site VPN connection. You or your network administrator must configure the device to work with the Site-to-Site VPN connection. 

You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. More on customer gateway device: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html 


Has option_explanations: True
Option explanations: ['A VPC endpoint interface - An interface VPC endpoint (interface endpoint) enables you to connect to services powered by AWS PrivateLink. It is not a component of a connection between on-premises network and AWS.', 'A virtual private gateway (VGW) - A virtual private gateway (VGW) device is a physical or software appliance on AWS side of a Site-to-Site VPN connection. \n\nReferences: \n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html \n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html', 'A security group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. It is not a component of a connection between on-premises network and AWS.', 'A customer gateway \n\nA customer gateway device is a physical or software appliance on your side of a Site-to-Site VPN connection. You or your network administrator must configure the device to work with the Site-to-Site VPN connection. \n\nYou can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. More on customer gateway device: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html \n\n']
Question: Which types of monitoring can be provided by Amazon CloudWatch? (Select TWO)
Has explanation: True
Explanation: Resource utilization 

Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon DynamoDB tables, and Amazon Amazon Relational Database Service (Amazon RDS) DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. 

You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. 

How Amazon CloudWatch works: via - https://aws.amazon.com/cloudwatch/
Has option_explanations: True
Option explanations: ['API access - Recording API calls is a feature of AWS CloudTrail, not Amazon CloudWatch.', 'Performance and availability of AWS services - The AWS Health - Your Account Health Dashboard gives you a personalized view of the performance and availability of the AWS services underlying your AWS resources, not Amazon CloudWatch.', 'Resource utilization \n\nAmazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon DynamoDB tables, and Amazon Amazon Relational Database Service (Amazon RDS) DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. \n\nYou can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. \n\nHow Amazon CloudWatch works: via - https://aws.amazon.com/cloudwatch/', 'Application performance', 'Account management - AWS Identity and Access Management (AWS IAM) is usually used to manage accounts, not Amazon CloudWatch.\n\nReferences:\n\nhttps://aws.amazon.com/cloudwatch/features/\n\nhttps://aws.amazon.com/cloudwatch/']
Question: Which of the following are the best practices when using AWS Organizations? (Select TWO)
Has explanation: True
Explanation: Create AWS accounts per department
Has option_explanations: True
Option explanations: ['Do not use AWS Organizations to automate AWS account creation - AWS Organizations helps you simplify IT operations by automating AWS account creation and management. The AWS Organizations APIs enable you to create new accounts programmatically and to add new accounts to a group. The policies attached to the group are automatically applied to the new account. \n\nReference: \n\nhttps://aws.amazon.com/organizations/', 'Never use tags for billing - You should use tags standards to categorize AWS resources for billing purposes.', 'Disable AWS CloudTrail on several accounts - You should enable AWS CloudTrail to monitor activity on all accounts for governance, compliance, risk, and auditing purposes.', 'Create AWS accounts per department', 'Restrict account privileges using Service Control Policies (SCP) \n\nAWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Whether you are a growing startup or a large enterprise, AWS Organizations help you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. \n\nUsing AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use AWS Organizations to define central configurations and resource sharing across accounts in your organization. AWS Organizations is available to all AWS customers at no additional charge. \n\nYou should create accounts per department based on regulatory restrictions (using Service Control Policies (SCP)) for better resource isolation, and to have separate per-account service limits. \n\nAWS Organizations allows you to restrict what services and actions are allowed in your accounts. You can use the Service Control Policies (SCP) to apply permission guardrails on AWS Identity and Access Management (IAM) users and roles.']
Question: The development team at a company manages 300 microservices and it is now trying to automate the code reviews to improve the code quality. Which tool/service is the right fit for this requirement?
Has explanation: True
Explanation: Amazon CodeGuru 

Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an applications most expensive lines of code. Integrate Amazon CodeGuru into your existing software development workflow to automate code reviews during application development, continuously monitor application performance in production, provide recommendations and visual clues for improving code quality and application performance, and reduce overall cost. 

Amazon CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. 

Amazon CodeGuru Profiler pinpoints an applications most expensive lines of code by helping developers understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. 

How Amazon CodeGuru works: via - https://aws.amazon.com/codeguru/
Has option_explanations: True
Option explanations: ['AWS CodeBuild - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With AWS CodeBuild, you don\x92t need to provision, manage, and scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.', 'AWS X-Ray - AWS X-Ray helps developers analyze and debug production, and distributed applications, such as those built using a microservices architecture. With AWS X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\x92s underlying components.', 'Amazon CodeGuru \n\nAmazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application\x92s most expensive lines of code. Integrate Amazon CodeGuru into your existing software development workflow to automate code reviews during application development, continuously monitor application performance in production, provide recommendations and visual clues for improving code quality and application performance, and reduce overall cost. \n\nAmazon CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. \n\nAmazon CodeGuru Profiler pinpoints an application\x92s most expensive lines of code by helping developers understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. \n\nHow Amazon CodeGuru works: via - https://aws.amazon.com/codeguru/', 'AWS Trusted Advisor - AWS Trusted Advisors provides recommendations that help you follow AWS best practices. AWS Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. \n\nReference: \n\nhttps://aws.amazon.com/codeguru/']
Question: Which of the following AWS Support plans is the MOST cost-effective when getting enhanced technical support by Cloud Support Engineers?
Has explanation: True
Explanation: AWS Business Support 

AWS recommends AWS Business Support if you have production workloads on AWS and want 24x7 phone, email, and chat access to technical support and architectural guidance in the context of your specific use cases. You get full access to AWS Trusted Advisor Best Practice Checks. It is also the cheapest support plan to provide enhanced technical support by Cloud Support Engineers.
Has option_explanations: True
Option explanations: ['AWS Developer Support - AWS recommends AWS Developer Support if you are testing or doing early development on AWS and want the ability to get technical support during business hours as well as general architectural guidance as you build and test. It provides enhanced technical support by Cloud Support Associates.', 'AWS Basic Support - The AWS Basic Support plan is included for all AWS customers. It does not provide enhanced technical support.', 'AWS Business Support \n\nAWS recommends AWS Business Support if you have production workloads on AWS and want 24x7 phone, email, and chat access to technical support and architectural guidance in the context of your specific use cases. You get full access to AWS Trusted Advisor Best Practice Checks. It is also the cheapest support plan to provide enhanced technical support by Cloud Support Engineers.', 'AWS Enterprise Support - AWS Enterprise Support provides customers with concierge-like service where the main focus is helping the customer achieve their outcomes and find success in the cloud. With Enterprise Support, you get 24x7 technical support from high-quality engineers, tools, and technology to automatically manage the health of your environment, consultative architectural guidance delivered in the context of your applications and use cases, and a designated Technical Account Manager (TAM) to coordinate access to proactive/preventative programs and AWS subject matter experts. It provides enhanced technical support by Cloud Support Engineers but is more expensive than the Business support plan. \n\nReferences: \n\nhttps://aws.amazon.com/premiumsupport/plans/ \n\nhttps://aws.amazon.com/premiumsupport/plans/business/']
Question: Which AWS service allows you to quickly and easily add user sign-up, sign-in, and access control to web and mobile applications?
Has explanation: True
Explanation: Amazon Cognito 

Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system.
Has option_explanations: True
Option explanations: ['AWS Organizations - AWS Organizations offers policy-based management for multiple AWS accounts. With AWS Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. Organizations enable you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It does not provide user sign-up, sign-in, and access control to web and mobile applications. \n\nReference: \n\nhttps://aws.amazon.com/cognito/', 'AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (AWS IAM) enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. It does not provide user sign-up, sign-in, and access control to web and mobile applications.', 'Amazon Cognito \n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system.', 'AWS IAM Identity Center - AWS IAM Identity Center is the successor to AWS Single Sign-On. It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create, or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both.']
Question: Which of the following AWS services can be used to generate, use, and manage encryption keys on the AWS Cloud?
Has explanation: True
Explanation: AWS CloudHSM 

The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. 

AWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only to you. 

How AWS CloudHSM works: via - https://aws.amazon.com/cloudhsm/
Has option_explanations: True
Option explanations: ['AWS GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It cannot be used to generate, use, and manage encryption keys.', 'AWS CloudHSM \n\nThe AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. \n\nAWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only to you. \n\nHow AWS CloudHSM works: via - https://aws.amazon.com/cloudhsm/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. It cannot be used to generate, use, and manage encryption keys.', 'AWS Secrets Manager - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It is integrated with AWS CloudHSM to generate, use, and manage encryption keys. \n\nReference: \n\nhttps://aws.amazon.com/cloudhsm/']
Question: A company would like to audit requests made to an Amazon Simple Storage Service (Amazon S3) bucket. As a Cloud Practitioner, which Amazon Simple Storage Service (Amazon S3) feature would you recommend addressing this use-case?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) Access Logs 

Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. 

It can also help you learn about your customer base and understand your Amazon S3 bill.
Has option_explanations: True
Option explanations: ['Amazon S3 Bucket Policies - Amazon S3 Bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates. It does not help with auditing requests made to your bucket.', 'S3 cross-region replication (S3 CRR) - S3 cross-region replication (S3 CRR) enables automatic, asynchronous copying of objects across Amazon S3 buckets. Cross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. It does not help with auditing requests made to your bucket.', 'Amazon Simple Storage Service (Amazon S3) Access Logs \n\nServer access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. \n\nIt can also help you learn about your customer base and understand your Amazon S3 bill.', 'S3 Versioning - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. It does not help with auditing requests made to your bucket. \n\nReference: \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html']
Question: A company would like to move its infrastructure to AWS Cloud. Which of the following should be included in the Total Cost of Ownership (TCO) estimate? (Select TWO)
Has explanation: True
Explanation: Power/Cooling 

AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator can be accessed at https://calculator.aws/#/. 

AWS Pricing Calculator compares the cost of your applications in an on-premises or traditional hosting environment to AWS: server, storage, network, and IT labor. Therefore, you need to include every element relevant to these points of comparison. 

Server administration is included in the IT labor costs. 

Power/Cooling are included in the server, storage, and network cost.
Has option_explanations: True
Option explanations: ['Power/Cooling \n\nAWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator can be accessed at https://calculator.aws/#/. \n\nAWS Pricing Calculator compares the cost of your applications in an on-premises or traditional hosting environment to AWS: server, storage, network, and IT labor. Therefore, you need to include every element relevant to these points of comparison. \n\nServer administration is included in the IT labor costs. \n\nPower/Cooling are included in the server, storage, and network cost.', 'Electronic equipment at office - The electronic equipment at the office is not relevant for a Total Cost of Ownership (TCO) estimate. \n\nReferences: \n\nhttps://calculator.aws/#/ \n\nhttps://aws.amazon.com/blogs/aws/new-cloud-tco-comparison-calculator-for-web-applications/', 'Server administration', 'Application advertising - The application advertising is not relevant for a Total Cost of Ownership (TCO) estimate.', 'Number of end-users - The number of end-users is not relevant for a Total Cost of Ownership (TCO) estimate.']
Question: An e-commerce company would like to build a chatbot for its customer service using Natural Language Understanding (NLU). As a Cloud Practitioner, which AWS service would you use?
Has explanation: True
Explanation: Amazon Lex

Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Amazon Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language chatbots to new and existing applications.

Amazon Lex Use Cases: via - https://aws.amazon.com/lex/
Has option_explanations: True
Option explanations: ['Amazon SageMaker - Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.', 'Amazon Lex\n\nAmazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Amazon Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language \x91chatbots\x92 to new and existing applications.\n\nAmazon Lex Use Cases: via - https://aws.amazon.com/lex/', 'Amazon Comprehend - Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing Natural Language Processing (NLP), you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text. \n\nReference: \n\nhttps://aws.amazon.com/lex/', 'Amazon Rekognition - With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos and also detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.']
Question: A growing start-up has trouble identifying and protecting sensitive data at scale. Which AWS fully managed service can assist with this task?
Has explanation: True
Explanation: Amazon Macie 

Amazon Macie is a fully managed data security and data privacy service that uses Machine Learning and pattern matching to discover and protect your sensitive data in AWS. 

Amazon Macie uses Machine Learning and pattern matching to cost-efficiently discover sensitive data at scale. Amazon Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. 

How Amazon Macie works: via - https://aws.amazon.com/macie/
Has option_explanations: True
Option explanations: ['AWS Key Management Service (AWS KMS) - AWS Key Management Service (AWS KMS) makes it easy for you to create and manage keys and control the use of encryption across a wide range of AWS services and in your applications. It is not used to discover and protect sensitive data in AWS. \n\nReference: \n\nhttps://aws.amazon.com/macie/', 'AWS Artifact - AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS\x92 security and compliance reports and selects online agreements. It is not used to discover and protect sensitive data in AWS.', 'Amazon Macie \n\nAmazon Macie is a fully managed data security and data privacy service that uses Machine Learning and pattern matching to discover and protect your sensitive data in AWS. \n\nAmazon Macie uses Machine Learning and pattern matching to cost-efficiently discover sensitive data at scale. Amazon Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. \n\nHow Amazon Macie works: via - https://aws.amazon.com/macie/', 'AWS Secrets Manager - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It is not used to discover and protect sensitive data in AWS.']
Question: An organization would like to copy data across different Availability Zones (AZs) using Amazon EBS snapshots. Where are Amazon EBS snapshots stored in the AWS Cloud?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) 

You can create a point-in-time snapshot of an Amazon EBS Elastic Volume and use it as a baseline for new volumes or data backup. If you make periodic snapshots of a volume, the snapshots are incrementalthe new snapshot saves only the blocks that have changed since your last snapshot. 

You can back up the data on your Amazon EBS Elastic Volumes to Amazon Simple Storage Service (Amazon S3) by taking point-in-time snapshots.
Has option_explanations: True
Option explanations: ['Amazon Elastic File System (Amazon EFS) - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, elastic file system for Linux-based workloads for use with AWS Cloud services and on-premises resources. Amazon EBS snapshots cannot be stored on Amazon EFS. \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html', 'Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. Amazon EBS snapshots cannot be stored on Amazon RDS.', 'Amazon Simple Storage Service (Amazon S3) \n\nYou can create a point-in-time snapshot of an Amazon EBS Elastic Volume and use it as a baseline for new volumes or data backup. If you make periodic snapshots of a volume, the snapshots are incremental\x97the new snapshot saves only the blocks that have changed since your last snapshot. \n\nYou can back up the data on your Amazon EBS Elastic Volumes to Amazon Simple Storage Service (Amazon S3) by taking point-in-time snapshots.', 'Amazon Elastic Compute Cloud (Amazon EC2) - Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Amazon EBS snapshots cannot be stored on Amazon EC2.']
Question: Which AWS serverless service allows you to prepare data for analytics?
Has explanation: True
Explanation: AWS Glue 

AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. 

How AWS Glue works: via - https://aws.amazon.com/glue/
Has option_explanations: True
Option explanations: ['AWS Glue \n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. \n\nHow AWS Glue works: via - https://aws.amazon.com/glue/', 'Amazon EMR - Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. EMR is used for analytics and not to prepare data for analytics. \n\nReference: \n\nhttps://aws.amazon.com/glue/', 'Amazon Redshift - Amazon Redshift is a fast and scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift is used for analytics and not to prepare data for analytics.', 'Amazon Athena - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Amazon Athena is used for analytics and not to prepare data for analytics.']
Question: Which of the following statements is an AWS best practice when architecting for the Cloud?
Has explanation: True
Explanation: Automation 

Automation should be implemented to improve both your system's stability and the efficiency of your organization. There are many services to automate application architecture (AWS Elastic Beanstalk, Auto Scaling, AWS Lambda, etc.) to ensure more resiliency, scalability, and performance.
Has option_explanations: True
Option explanations: ["Automation \n\nAutomation should be implemented to improve both your system's stability and the efficiency of your organization. There are many services to automate application architecture (AWS Elastic Beanstalk, Auto Scaling, AWS Lambda, etc.) to ensure more resiliency, scalability, and performance.", 'Servers, not services - The correct best practice is: "Services, not servers". AWS recommends developing, managing, and operating applications, especially at scale, using the broad set of compute, storage, database, analytics, applications, and deployment services offered by AWS to move faster and lower IT costs.', 'Close coupling - The correct best practice is: "Loose coupling". AWS recommends that, as application complexity increases, IT systems should be designed in a way that reduces interdependencies. Therefore, a change or a failure in one component should not cascade to other components.', 'Security comes last - AWS allows you to improve your security in many, more simple ways. Therefore, you should take advantage of this and implement a high level of security. \n\nReference: \n\nhttps://aws.amazon.com/architecture/well-architected/']
Question: A Cloud Practitioner would like to get operational insights of its resources to quickly identify any issues that might impact applications using those resources. Which AWS service can help with this task?
Has explanation: True
Explanation: AWS Systems Manager 

AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. 

With AWS Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. You can also take action on each resource group depending on your operational needs. AWS Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. 

How AWS Systems Manager works: via - https://aws.amazon.com/systems-manager/
Has option_explanations: True
Option explanations: ['AWS Health Dashboard - Your Account Health - AWS Health Dashboard - Your Account Health provides alerts and remediation guidance when AWS is experiencing events that may impact you. It is not used to get operational insights of AWS resources.', 'AWS Systems Manager \n\nAWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. \n\nWith AWS Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. You can also take action on each resource group depending on your operational needs. AWS Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. \n\nHow AWS Systems Manager works: via - https://aws.amazon.com/systems-manager/', 'AWS Trusted Advisor - AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. It is not used to get operational insights of AWS resources. \n\nReference: \n\nhttps://aws.amazon.com/systems-manager/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not used to get operational insights of AWS resources.']
Question: A research lab needs to be notified in case of a configuration change for security and compliance reasons. Which AWS service can assist with this task?
Has explanation: True
Explanation: AWS Config 

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. 

How AWS Config works: via - https://aws.amazon.com/config/
Has option_explanations: True
Option explanations: ['AWS Config \n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. \n\nHow AWS Config works: via - https://aws.amazon.com/config/', 'AWS Secrets Manager - AWS Secrets Manager helps you protect the secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It cannot notify configuration changes. \n\nReference: \n\nhttps://aws.amazon.com/config/', 'AWS Trusted Advisor - AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. It cannot notify configuration changes.', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on your Amazon EC2 instances. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. It cannot notify configuration changes.']
Question: Which of the following criteria are used to calculate the charge for Amazon EBS Volumes? (Select Two)
Has explanation: True
Explanation: Volume type 

Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone (AZ) to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutesall while paying a low price for only what you provision. 

The fundamental charges for EBS volumes are the volume type (based on performance), the storage volume in GB per month provisioned, the number of IOPS provisioned per month, the storage consumed by snapshots, and outbound data transfer.
Has option_explanations: True
Option explanations: ['Volume type \n\nAmazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone (AZ) to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutes\x97all while paying a low price for only what you provision. \n\nThe fundamental charges for EBS volumes are the volume type (based on performance), the storage volume in GB per month provisioned, the number of IOPS provisioned per month, the storage consumed by snapshots, and outbound data transfer.', 'The Amazon EC2 instance type the Amazon EBS Elastic volume is attached to - The Amazon EC2 instance type the Amazon EBS volume is attached to does not influence the EBS volume pricing.', 'Provisioned IOPS', 'Data type - The type of data stored on EBS volumes does not influence the price. \n\nReference: \n\nhttps://aws.amazon.com/ebs/pricing/', 'Data transfer IN - Data transfer-in is always free, including for Amazon EBS Elastic Volumes.']
Question: Which AWS service can be used to send, store, and receive messages between software components at any volume to decouple application tiers?
Has explanation: True
Explanation: Amazon Simple Queue Service (Amazon SQS)

Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.

Using Amazon Simple Queue Service (Amazon SQS), you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.
Has option_explanations: True
Option explanations: ['AWS Organizations - AWS Organizations offers policy-based management for multiple AWS accounts. With AWS Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. AWS Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It is not used to send, store, and receive messages between software components. \n\nReference: \n\nhttps://aws.amazon.com/sqs/', 'AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring. It is not used to send, store, and receive messages between software components.', 'Amazon Simple Queue Service (Amazon SQS)\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.\n\nUsing Amazon Simple Queue Service (Amazon SQS), you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.', 'Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\n\nPlease review this reference architecture for building a decoupled order processing system using Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS): via - https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/']
Question: A production company with predictable usage would like to reduce the cost of its Amazon Elastic Compute Cloud (Amazon EC2) instances by using reserved instances (RI). Which of the following length terms are available for Amazon EC2 reserved instances (RI)? (Select Two)
Has explanation: True
Explanation: 1 year
Has option_explanations: True
Option explanations: ['2 years - It is not possible to reserve instances for 2 years. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/', '6 months - It is not possible to reserve instances for 6 months.', '5 years - It is not possible to reserve instances for 5 years.', '1 year', '3 years\n\nReserved Instances (RI) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. Besides, when Reserved Instances (RI) are assigned to a specific Availability Zone (AZ), they provide a capacity reservation, giving you additional confidence in your ability to launch instances when you need them.\n\nStandard and Convertible reserved instances can be purchased for a 1-year or 3-year term.\n\nEC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/']
Question: A data science team would like to build Machine Learning models for its projects. Which AWS service can it use?
Has explanation: True
Explanation: Amazon SageMaker 

Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.
Has option_explanations: True
Option explanations: ['Amazon SageMaker \n\nAmazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.', 'Amazon Comprehend - Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing natural language processing (NLP), you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text.', 'Amazon Connect - Amazon Connect is an omnichannel cloud contact center. You can set up a contact center in a few steps, add agents who are located anywhere, and start engaging with your customers. You can create personalized experiences for your customers using omnichannel communications. Amazon Connect is an open platform that you can integrate with other enterprise applications. \n\nReference: \n\nhttps://aws.amazon.com/sagemaker/', "Amazon Polly - You can use Amazon Polly to turn text into lifelike speech thereby allowing you to create applications that talk. Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech."]
Question: Which service/tool will you use to create and provide trusted users with temporary security credentials that can control access to your AWS resources?
Has explanation: True
Explanation: AWS Security Token Service (AWS STS) 

AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (AWS IAM) users or for users that you authenticate (federated users). 

You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: 

Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. 

Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so. 

Temporary security credentials are generated by AWS Security Token Service (AWS STS). By default, AWS STS is a global service with a single endpoint at https://sts.amazonaws.com. However, you can also choose to make AWS STS API calls to endpoints in any other supported Region.
Has option_explanations: True
Option explanations: ['Amazon Cognito - Amazon Cognito is a higher level of abstraction than AWS Security Token Service (AWS STS). Amazon Cognito supports the same identity providers as AWS STS, and also supports unauthenticated (guest) access, and lets you migrate user data when a user signs in. Amazon Cognito also provides API operations for synchronizing user data so that it is preserved as users move between devices. Amazon Cognito helps create the user database, which is not possible with STS.', 'AWS Web Application Firewall (AWS WAF) - AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. \n\nReference: \n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html', 'AWS Security Token Service (AWS STS) \n\nAWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (AWS IAM) users or for users that you authenticate (federated users). \n\nYou can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: \n\nTemporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. \n\nTemporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so. \n\nTemporary security credentials are generated by AWS Security Token Service (AWS STS). By default, AWS STS is a global service with a single endpoint at https://sts.amazonaws.com. However, you can also choose to make AWS STS API calls to endpoints in any other supported Region.', 'AWS IAM Identity Center - AWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (AWS IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In AWS IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both.']
Question: A company needs to use a secure online data transfer tool/service that can automate the ongoing transfers from on-premises systems into AWS while providing support for incremental data backups. 

Which AWS tool/service is an optimal fit for this requirement?
Has explanation: True
Explanation: AWS DataSync 

AWS DataSync is a secure online data transfer service that simplifies, automates, and accelerates copying terabytes of data to and from AWS storage services. Easily migrate or replicate large data sets without having to build custom solutions or oversee repetitive tasks. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. 

You can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, and big data analytics in financial services. AWS DataSync provides timely delivery to ensure dependent processes are not delayed. You can specify exclude filters, include filters, or both, to determine which files, folders, or objects get transferred each time your task runs. 

AWS DataSync employs an AWS-designed transfer protocoldecoupled from the storage protocolto accelerate data movement. The protocol performs optimizations on how, when, and what data is sent over the network. Network optimizations performed by DataSync include incremental transfers, in-line compression, and sparse file detection, as well as in-line data validation and encryption. 

Data Transfer between on-premises and AWS using AWS DataSync: via - https://aws.amazon.com/datasync/
Has option_explanations: True
Option explanations: ['AWS Snowmobile - AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot-long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration.', 'AWS Storage Gateway - AWS Storage Gateway is a set of hybrid cloud services that give you on-premises access to virtually unlimited cloud storage. Customers use AWS Storage Gateway to integrate AWS Cloud storage with existing on-site workloads so they can simplify storage management and reduce costs for key hybrid cloud storage use cases. These include moving backups to the cloud, using on-premises file shares backed by cloud storage, and providing low latency access to data in AWS for on-premises applications.', 'AWS DataSync \n\nAWS DataSync is a secure online data transfer service that simplifies, automates, and accelerates copying terabytes of data to and from AWS storage services. Easily migrate or replicate large data sets without having to build custom solutions or oversee repetitive tasks. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. \n\nYou can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, and big data analytics in financial services. AWS DataSync provides timely delivery to ensure dependent processes are not delayed. You can specify exclude filters, include filters, or both, to determine which files, folders, or objects get transferred each time your task runs. \n\nAWS DataSync employs an AWS-designed transfer protocol\x97decoupled from the storage protocol\x97to accelerate data movement. The protocol performs optimizations on how, when, and what data is sent over the network. Network optimizations performed by DataSync include incremental transfers, in-line compression, and sparse file detection, as well as in-line data validation and encryption. \n\nData Transfer between on-premises and AWS using AWS DataSync: via - https://aws.amazon.com/datasync/', 'AWS Snowcone - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing, edge storage, and data transfer devices. Weighing in at 4.5 pounds (2.1 kg), AWS Snowcone is equipped with 8 terabytes of usable storage, while AWS Snowcone Solid State Drive (SSD) supports 14 terabytes of usable storage. Both referred to as AWS Snowcone, the device is ruggedized, secure, and purpose-built for use outside of a traditional data center. Its small form factor makes it a perfect fit for tight spaces or where portability is a necessity and network connectivity is unreliable. You can use AWS Snowcone in backpacks for first responders, or for IoT, vehicular, and drone use cases. You can execute compute applications at the edge, and you can ship the device with data to AWS for offline data transfer, or you can transfer data online with AWS DataSync from edge locations. \n\nReferences: \n\nhttps://aws.amazon.com/datasync/ \n\nhttps://aws.amazon.com/datasync/features/']
Question: An engineering team would like to cost-effectively run hundreds of thousands of batch computing workloads on AWS. As a Cloud Practitioner, which AWS service would you use for this task?
Has explanation: True
Explanation: AWS Batch 

AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. 

You can use AWS Batch to plan, schedule, and execute your batch computing workloads across the full range of AWS compute services. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example - memory optimized instance or CPU) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch provisions compute resources and optimize the job distribution based on the volume and resource requirements of the submitted batch jobs. 

Please review the common use cases for AWS Batch: via - https://aws.amazon.com/batch/
Has option_explanations: True
Option explanations: ['AWS Batch \n\nAWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. \n\nYou can use AWS Batch to plan, schedule, and execute your batch computing workloads across the full range of AWS compute services. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example - memory optimized instance or CPU) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch provisions compute resources and optimize the job distribution based on the volume and resource requirements of the submitted batch jobs. \n\nPlease review the common use cases for AWS Batch: via - https://aws.amazon.com/batch/', 'AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It can be used to run batch jobs but has a time limit and limited runtimes. It is usually used for smaller batch jobs.', 'Amazon Lightsail - Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project \x96 a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address \x96 for a low, predictable price. It is not used to run batch jobs.', 'AWS Fargate - AWS Fargate is a compute engine for Amazon Elastic Container Service (Amazon ECS) that allows you to run containers without having to manage servers or clusters. You can run batch jobs on AWS Fargate, but it is more expensive than AWS Batch. \n\nReference: \n\nhttps://aws.amazon.com/batch/']
Question: A company would like to define a set of rules to manage objects cost-effectively between Amazon Simple Storage Service (Amazon S3) storage classes. As a Cloud Practitioner, which Amazon S3 feature would you use?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) Lifecycle configuration 

To manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions (define when objects transition to another storage class) and expiration actions (define when objects expire. Amazon S3 deletes expired objects on your behalf). 

In this particular use case, you would use a transition action.
Has option_explanations: True
Option explanations: ['Amazon Simple Storage Service (Amazon S3) Bucket policies - An S3 bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates. It is not used to move objects between storage classes.', 'S3 Cross-Region Replication (S3 CRR) - S3 Cross-Region Replication (S3 CRR) enables automatic, asynchronous copying of objects across Amazon S3 buckets. Cross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. It is not used to move objects between storage classes. \n\nReferences: \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html \n\nhttps://aws.amazon.com/s3/', 'Amazon S3 Transfer Acceleration (Amazon S3TA) - Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. It is not used to move objects between storage classes.', 'Amazon Simple Storage Service (Amazon S3) Lifecycle configuration \n\nTo manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions (define when objects transition to another storage class) and expiration actions (define when objects expire. Amazon S3 deletes expired objects on your behalf). \n\nIn this particular use case, you would use a transition action.']
Question: Which AWS tool/service will help you define your cloud infrastructure using popular programming languages such as Python and JavaScript?
Has explanation: True
Explanation: AWS Cloud Development Kit (AWS CDK)

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.

AWS Cloud Development Kit (AWS CDK) uses the familiarity and expressive power of programming languages for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.

In short, you use the AWS CDK framework to author AWS CDK projects which are executed to generate AWS CloudFormation templates.

How Cloud Development Kit (AWS CDK) works:via - https://aws.amazon.com/cdk/
Has option_explanations: True
Option explanations: ['AWS CodeBuild - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With AWS CodeBuild, you don\x92t need to provision, manage, and scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue. \n\nReference: \n\nhttps://aws.amazon.com/cdk/', 'AWS CloudFormation - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources, and provision and manage them in an orderly and predictable fashion. AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictably, and safely while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions. AWS Cloud Development Kit (AWS CDK) helps code the same in higher-level languages and converts them into AWS CloudFormation templates.', "AWS Cloud Development Kit (AWS CDK)\n\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.\n\nAWS Cloud Development Kit (AWS CDK) uses the familiarity and expressive power of programming languages for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.\n\nIn short, you use the AWS CDK framework to author AWS CDK projects which are executed to generate AWS CloudFormation templates.\n\nHow Cloud Development Kit (AWS CDK) works:via - https://aws.amazon.com/cdk/", 'AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, etc. You can simply upload your code in a programming language of your choice and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring.']
Question: A company would like to separate cost for AWS services by the department for cost allocation. Which of the following is the simplest way to achieve this task?
Has explanation: True
Explanation: Create tags for each department 

You can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria. 

Typically, you use business tags such as cost center/business unit, customer, or project to associate AWS costs with traditional cost-allocation dimensions. But a cost allocation report can include any tag. This lets you associate costs with technical or security dimensions, such as specific applications, environments, or compliance programs. 

Example of tagging for cost optimization: via - https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html
Has option_explanations: True
Option explanations: ['Create tags for each department \n\nYou can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria. \n\nTypically, you use business tags such as cost center/business unit, customer, or project to associate AWS costs with traditional cost-allocation dimensions. But a cost allocation report can include any tag. This lets you associate costs with technical or security dimensions, such as specific applications, environments, or compliance programs. \n\nExample of tagging for cost optimization: via - https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html', 'Create one account for all departments and share this account - Sharing accounts is not a security best practice, and is not recommended.', 'Create different virtual private cloud (VPCs) for different departments - Creating different VPCs will not help with separating costs. \n\nReference: \n\nhttps://docs.aws.amazon.com/general/latest/gr/aws_tagging.html', 'Create different accounts for different departments - Users can belong to several departments. Therefore, having different accounts for different departments would imply some users having several accounts. This is contrary to the security best practice: one physical user = one account. Also, it is much simpler to set up tags for tracking costs for each department.']
Question: An engineering team is new to the AWS Cloud and it would like to launch a dev/test environment with low monthly pricing. Which AWS service can address this use case?
Has explanation: True
Explanation: Amazon LightSail 

Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project  a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address  for a low, predictable price. 

It is great for people with little cloud experience to launch quickly a popular IT solution ready to use immediately.
Has option_explanations: True
Option explanations: ['AWS CloudFormation - AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. Using AWS CloudFormation requires experience as resources are deployed within a virtual private cloud (VPC).', 'Amazon Elastic Compute Cloud (Amazon EC2) - Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Deploying a dev/test environment with Amazon EC2 requires experience as instances are deployed within a virtual private cloud (VPC).', 'Amazon LightSail \n\nAmazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project \x96 a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address \x96 for a low, predictable price. \n\nIt is great for people with little cloud experience to launch quickly a popular IT solution ready to use immediately.', 'Amazon Elastic Container Service (Amazon ECS) - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS. Amazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines. Using Amazon ECS requires experience. Reference: \n\nhttps://aws.amazon.com/lightsail/']
Question: A company would like to create a private, high bandwidth network connection between its on-premises data centers and AWS Cloud. As a Cloud Practitioner, which of the following options would you recommend?
Has explanation: True
Explanation: AWS Direct Connect 

AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. 

How AWS Direct Connect works: via - https://aws.amazon.com/directconnect/
Has option_explanations: True
Option explanations: ["AWS Site-to-Site VPN - By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection and configuring routing to pass traffic through the connection. It uses the public internet and is therefore not suited for this use case.", 'AWS Direct Connect \n\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. \n\nHow AWS Direct Connect works: via - https://aws.amazon.com/directconnect/', 'VPC Endpoints - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. It does not connect your on-premises data centers and AWS Cloud.', 'VPC peering connection - A VPC peering connection is a networking connection between two virtual private clouds (VPCs) that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. It is used to connect virtual private clouds (VPCs) together, and not on-premises data centers and AWS Cloud. \n\nReference: \n\nhttps://aws.amazon.com/directconnect/']
Question: A media company wants to enable customized content suggestions for the users of its movie streaming platform. Which AWS service can provide these personalized recommendations based on historic data?
Has explanation: True
Explanation: Amazon Personalize 

Amazon Personalize enables developers to build applications with the same machine learning (ML) technology used by Amazon.com for real-time personalized recommendations. Amazon Personalize can be used to personalize the end-user experience over any digital channel. Examples include product recommendations for e-commerce, news articles and content recommendation for publishing, media, and social networks, hotel recommendations for travel websites, credit card recommendations for banks, and match recommendations for dating sites. These recommendations and personalized experiences can be delivered over websites, mobile apps, or email/messaging. Amazon Personalize can also be used to customize the user experience when user interaction is over a physical channel, e.g., a meal delivery company could personalize weekly meals to users in a subscription plan. 

Amazon Personalize supports the following key use cases: 

Personalized recommendations 
Similar items 
Personalized reranking i.e. rerank a list of items for a user 
Personalized promotions/notifications
Has option_explanations: True
Option explanations: ['Amazon Personalize \n\nAmazon Personalize enables developers to build applications with the same machine learning (ML) technology used by Amazon.com for real-time personalized recommendations. Amazon Personalize can be used to personalize the end-user experience over any digital channel. Examples include product recommendations for e-commerce, news articles and content recommendation for publishing, media, and social networks, hotel recommendations for travel websites, credit card recommendations for banks, and match recommendations for dating sites. These recommendations and personalized experiences can be delivered over websites, mobile apps, or email/messaging. Amazon Personalize can also be used to customize the user experience when user interaction is over a physical channel, e.g., a meal delivery company could personalize weekly meals to users in a subscription plan. \n\nAmazon Personalize supports the following key use cases: \n\nPersonalized recommendations \nSimilar items \nPersonalized reranking i.e. rerank a list of items for a user \nPersonalized promotions/notifications', 'Amazon SageMaker - Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models.', 'Amazon Customize - There is no such service as Amazon Customize. This option has been added as a distractor.', 'Amazon Comprehend - Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover information in unstructured data. Instead of combing through documents, the process is simplified and unseen information is easier to understand. \n\nThe service can identify critical elements in data, including references to language, people, and places, and the text files can be categorized by relevant topics. In real-time, you can automatically and accurately detect customer sentiment in your content. \n\nReference: \n\nhttps://aws.amazon.com/personalize/']
Question: Which of the following options is NOT a feature of Amazon Inspector?
Has explanation: True
Explanation: Track configuration changes

Tracking configuration changes is a feature of AWS Config.

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.

How AWS Config works: via - https://aws.amazon.com/config/
Has option_explanations: True
Option explanations: ['Inspect running operating systems (OS) against known vulnerabilities \n\nThese options are all features of Amazon Inspector. \n\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. \n\nAmazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. \n\nAmazon Inspector also offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess. The agent monitors the behavior of the EC2 instances, including network, file system, and process activity. It also collects a wide set of behavior and configuration data (telemetry). \n\nReferences: \n\nhttps://aws.amazon.com/config/ \n\nhttps://aws.amazon.com/inspector/', 'Track configuration changes\n\nTracking configuration changes is a feature of AWS Config.\n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\n\nHow AWS Config works: via - https://aws.amazon.com/config/', 'Analyze against unintended network accessibility', 'Automate security assessments']
Question: Which of the following statements is the MOST accurate when describing AWS Elastic Beanstalk?
Has explanation: True
Explanation: It is a Platform as a Service (PaaS) that allows you to deploy and scale web applications and services 

AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their applications, and AWS Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. 

It is a Platform as a Service (PaaS) as you only manage the applications and the data. 

Please review this overview of the types of Cloud Computing: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['It is an Infrastructure as Code (IaC) that allows you to model and provision resources needed for an application - This is the definition of AWS CloudFormation. AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application.', 'It is an Infrastructure as a Service (IaaS) that allows you to deploy and scale web applications and services - AWS Elastic Beanstalk allows you to deploy and scale web applications and services, but it is not an Infrastructure as a Service (IaaS). With AWS Elastic Beanstalk, you do not manage the runtime, the middleware, and the operating system. \n\nReference: \n\nhttps://aws.amazon.com/elasticbeanstalk/', 'It is a Platform as a Service (PaaS) that allows you to model and provision resources needed for an application - AWS Elastic Beanstalk is a Platform as a Service (PaaS). However, the service that allows you to model and provision resources needed for an application is AWS CloudFormation.', 'It is a Platform as a Service (PaaS) that allows you to deploy and scale web applications and services \n\nAWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their applications, and AWS Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. \n\nIt is a Platform as a Service (PaaS) as you only manage the applications and the data. \n\nPlease review this overview of the types of Cloud Computing: via - https://aws.amazon.com/types-of-cloud-computing/']
Question: A start-up would like to quickly deploy a popular technology on AWS. As a Cloud Practitioner, which AWS tool would you use for this task?
Has explanation: True
Explanation: AWS Partner Solutions (formerly Quick Starts) 

AWS Partner Solutions are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes. 

AWS Partner Solutions are automated reference deployments for key workloads on the AWS Cloud. Each Partner Solution launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Has option_explanations: True
Option explanations: ['AWS Whitepapers - AWS Whitepapers are technical content authored by AWS and the AWS community to expand your knowledge of the cloud. They include technical whitepapers, technical guides, reference material, and reference architecture diagrams. You can find useful content for your deployment, but it is not a service that will deploy technologies. \n\nReference: \n\nhttps://aws.amazon.com/quickstart/', 'AWS CodeDeploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. It is not suited to rapidly deploy popular technologies on AWS ready to be used immediately.', 'AWS Partner Solutions (formerly Quick Starts) \n\nAWS Partner Solutions are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes. \n\nAWS Partner Solutions are automated reference deployments for key workloads on the AWS Cloud. Each Partner Solution launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.', 'AWS Forums - AWS Forums is an AWS community platform where people can help each other. It is not used to deploy technologies on AWS.']
Question: A company is planning to implement Chaos Engineering to expose any blind spots that can disrupt the resiliency of the application. 

Which AWS service will help implement this requirement with the least effort? 


Has explanation: True
Explanation: AWS Fault Injection Simulator (AWS FIS) 

AWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an applications performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as a sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, and monitor blind spots, and performance bottlenecks that are difficult to find in distributed systems. 

AWS Fault Injection Simulator (AWS FIS) simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With AWS Fault Injection Simulator (AWS FIS), teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Simulator (AWS FIS) provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real-world conditions necessary to find hidden weaknesses.
Has option_explanations: True
Option explanations: ['AWS Fault Injection Simulator (AWS FIS) \n\nAWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application\x92s performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as a sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, and monitor blind spots, and performance bottlenecks that are difficult to find in distributed systems. \n\nAWS Fault Injection Simulator (AWS FIS) simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With AWS Fault Injection Simulator (AWS FIS), teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Simulator (AWS FIS) provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real-world conditions necessary to find hidden weaknesses.', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity.', 'AWS Trusted Advisor - AWS Trusted Advisors provides recommendations that help you follow AWS best practices. AWS Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. You can then follow the check recommendations to optimize your services and resources.', 'Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. \n\nReference: \n\nhttps://aws.amazon.com/fis/features/']
Question: A brand-new startup would like to remove its need to manage the underlying infrastructure and focus on the deployment and management of its applications. Which type of cloud computing does this refer to?
Has explanation: True
Explanation: Platform as a Service (PaaS) 

Cloud Computing can be broadly divided into three types - Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS). 

Platform as a Service (PaaS) removes the need to manage underlying infrastructure (usually hardware and operating systems) and allows you to focus on the deployment and management of your applications. You dont need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application. 

Please review this overview of the types of cloud computing: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['On-premises - When an enterprise opts for on-premises, it needs to create, upgrade, and scale the on-premise IT infrastructure by investing in sophisticated hardware, compatible software, and robust services. Also, the business needs to deploy dedicated IT staff to upkeep, scale, and manage the on-premise infrastructure continuously. \n\nReference: \n\nhttps://aws.amazon.com/types-of-cloud-computing/', 'Software as a Service (SaaS) - Software as a Service (SaaS) provides you with a complete product that is run and managed by the service provider. With a Software as a Service (SaaS) offering, you don\x92t have to think about how the service is maintained or how the underlying infrastructure is managed. You only need to think about how you will use that particular software. Amazon Rekognition is an example of a SaaS service.', 'Platform as a Service (PaaS) \n\nCloud Computing can be broadly divided into three types - Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS). \n\nPlatform as a Service (PaaS) removes the need to manage underlying infrastructure (usually hardware and operating systems) and allows you to focus on the deployment and management of your applications. You don\x92t need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application. \n\nPlease review this overview of the types of cloud computing: via - https://aws.amazon.com/types-of-cloud-computing/', 'Infrastructure as a Service (IaaS) - Infrastructure as a Service (IaaS) contains the basic building blocks for cloud IT. It typically provides access to networking features, computers (virtual or on dedicated hardware), and data storage space. Infrastructure as a Service (IaaS) gives the highest level of flexibility and management control over IT resources.']
Question: The IT infrastructure at a university is deployed on AWS Cloud and it's experiencing a read-intensive workload. As a Cloud Practitioner, which AWS service would you use to take the load off databases?
Has explanation: True
Explanation: Amazon ElastiCache 

Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. 

If Amazon EC2 instances are intensively reading data from a database, ElastiCache can cache some values to take the load off the database. 

How Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/))
Has option_explanations: True
Option explanations: ['Amazon EMR - Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. It cannot be used to take the load off the databases. \n\nReference: \n\nhttps://aws.amazon.com/elasticache/', 'Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security, and compatibility they need. It cannot be used to take the load off databases. However, Amazon ElastiCache is often used with Amazon RDS to take the load off RDS.', 'Amazon ElastiCache \n\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. \n\nIf Amazon EC2 instances are intensively reading data from a database, ElastiCache can cache some values to take the load off the database. \n\nHow Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/))', 'AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. It cannot be used to take the load off the databases.']
Question: Which security control tool can be used to deny traffic from a specific IP address?
Has explanation: True
Explanation: Network Access Control List (network ACL) 

A Network Access Control List (network ACL) is an optional layer of security for your virtual private cloud (VPC) that acts as a firewall for controlling traffic in and out of one or more subnets (i.e. it works at the subnet level). A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. 

Network Access Control List (network ACL) Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html
Has option_explanations: True
Option explanations: ['Security Group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not at the subnet level. You can specify allow rules, but not deny rules. You can specify separate rules for inbound and outbound traffic.', "VPC Flow Logs - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon Simple Storage Service (Amazon S3). After you've created a flow log, you can retrieve and view its data in the chosen destination. However, it cannot deny traffic from a specific IP address. \n\nReference: \n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html", 'Network Access Control List (network ACL) \n\nA Network Access Control List (network ACL) is an optional layer of security for your virtual private cloud (VPC) that acts as a firewall for controlling traffic in and out of one or more subnets (i.e. it works at the subnet level). A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. \n\nNetwork Access Control List (network ACL) Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html', 'Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. Amazon GuardDuty also detects potentially compromised instances or reconnaissance by attackers. It cannot deny traffic from a specific IP address. \n\n']
Question: A company would like to reserve Amazon Elastic Compute Cloud (Amazon EC2) compute capacity for three years to reduce costs. The company also plans to increase their workloads during this period. As a Cloud Practitioner, which Amazon Elastic Compute Cloud (Amazon EC2) reserved instance (RI) type would you recommend?
Has explanation: True
Explanation: Convertible reserved instance (RI)

Purchase convertible reserved instance (RI) if you need additional flexibility, such as the ability to use different instance families, operating systems, or tenancies over the reserved instance (RI) term. Convertible reserved instance (RI) provides you with a significant discount (up to 54%) compared to an on-demand instance and can be purchased for a 1-year or 3-year term.

Convertible reserved instance (RI) can be useful when workloads are likely to change. In this case, a convertible reserved instance (RI) enables you to adapt as needs evolve while still obtaining discounts and capacity reservation.

Amazon EC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/
Has option_explanations: True
Option explanations: ['Convertible reserved instance (RI)\n\nPurchase convertible reserved instance (RI) if you need additional flexibility, such as the ability to use different instance families, operating systems, or tenancies over the reserved instance (RI) term. Convertible reserved instance (RI) provides you with a significant discount (up to 54%) compared to an on-demand instance and can be purchased for a 1-year or 3-year term.\n\nConvertible reserved instance (RI) can be useful when workloads are likely to change. In this case, a convertible reserved instance (RI) enables you to adapt as needs evolve while still obtaining discounts and capacity reservation.\n\nAmazon EC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/', 'Standard reserved instance (RI) - Standard reserved instance (RI) provides you with a significant discount (up to 72%) compared to on-demand instance pricing, and can be purchased for a 1-year or 3-year term. Standard reserved instance (RI) do not offer as much flexibility as convertible reserved instance (RI), such as not being able to change the instance family type; and therefore are not best-suited for this use case. \n\nReview the differences between standard reserved instance (RI) and convertible reserved instance (RI): https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/standard-vs.-convertible-offering-classes.html', 'Scheduled reserved instance (RI) - AWS does not support scheduled reserved instance (RI), so this option is ruled out.', 'Adaptable reserved instances (RI) - Adaptable reserved instance (RI) is not a valid type of reserved instance (RI). It is a distractor. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/pricing/']
Question: Which of the following statements is INCORRECT regarding Amazon EBS Elastic Volumes?
Has explanation: True
Explanation: Amazon EBS Elastic Volumes can be bound to several Availability Zones (AZs) 

An Amazon EBS Elastic Volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. 

When using Amazon EBS Elastic Volumes, the volume, and the instance must be in the same Availability Zone (AZ).
Has option_explanations: True
Option explanations: ['Amazon EBS Elastic Volumes are bound to a specific Availability Zone (AZ) - As mentioned, when using Amazon EBS Elastic Volumes, the volume and the instance must be in the same Availability Zone(AZ).', 'Amazon EBS Elastic Volumes can persist data after their termination - Unlike an Amazon EC2 instance store, an Amazon EBS Elastic Volume is off-instance storage that can persist independently from the life of an instance. \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html', 'Amazon EBS Elastic Volumes can be mounted to one instance at a time - At the Certified Cloud Practitioner level, Amazon EBS Elastic Volumes can be mounted to one instance at a time. It is also possible that an Amazon EBS Elastic Volume is not mounted to an instance.', 'Amazon EBS Elastic Volumes can be bound to several Availability Zones (AZs) \n\nAn Amazon EBS Elastic Volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. \n\nWhen using Amazon EBS Elastic Volumes, the volume, and the instance must be in the same Availability Zone (AZ).']
Question: According to the AWS Well-Architected Framework, which of the following action is recommended in the Security pillar?
Has explanation: True
Explanation: Use AWS Key Management Service (AWS KMS) to encrypt data 

The Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. 

Encrypting data is part of the design principle "Protect data in transit and at rest": Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate. 

AWS Key Management Service (AWS KMS) makes it easy for you to create and control keys used for encryption. It is a key service of the Security pillar. 

The AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. 

The AWS Well-Architected Framework is based on six pillars  Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. 

Overview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/
Has option_explanations: True
Option explanations: ['Use AWS Key Management Service (AWS KMS) to encrypt data \n\nThe Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. \n\nEncrypting data is part of the design principle "Protect data in transit and at rest": Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate. \n\nAWS Key Management Service (AWS KMS) makes it easy for you to create and control keys used for encryption. It is a key service of the Security pillar. \n\nThe AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. \n\nThe AWS Well-Architected Framework is based on six pillars \x97 Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. \n\nOverview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/', 'Use Amazon CloudWatch to measure overall efficiency - Amazon CloudWatch is a monitoring and management service built for developers, system operators, site reliability engineers (SRE), and IT managers. Using Amazon CloudWatch to measure overall efficiency relates more to the Reliability pillar.', 'Use AWS Cost Explorer to view and track your usage in detail - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Using Cost Explorer to view and track your usage in detail relates more to the Cost Optimization pillar.', 'Use AWS CloudFormation to automate security best practices - AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. It is not used to automate security best practices. If you want to automate security best practices, you should use Amazon Inspector. \n\nReference: \n\nhttps://aws.amazon.com/architecture/well-architected/ \n\n']
Question: Which of the following statements is CORRECT regarding the scope of an Amazon Virtual Private Cloud (VPC)?
Has explanation: True
Explanation: A VPC spans all Availability Zones (AZs) within an AWS region 

Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. 

An Amazon VPC spans all Availability Zones (AZs) within a region.
Has option_explanations: True
Option explanations: ['A VPC spans all Availability Zones (AZs) within an AWS region \n\nAmazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. \n\nAn Amazon VPC spans all Availability Zones (AZs) within a region.', 'A VPC spans all Availability Zones (AZs) in all AWS regions - A VPC is located within an AWS region.', 'A VPC spans all AWS regions within an Availability Zone (AZ) - AWS has the concept of a Region, which is a physical location around the world where AWS clusters data centers. Each AWS Region consists of multiple (two or more), isolated, and physically separate Availability Zone (AZs) within a geographic area. An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. Therefore, regions cannot be within an Availability Zone. Moreover, a VPC is located within a region. \n\nAWS Regions and Availability Zones (AZs) Overview: via - https://aws.amazon.com/about-aws/global-infrastructure/regions_az/ \n\nReference: \n\nhttps://aws.amazon.com/vpc/', 'Amazon VPC spans all subnets in all AWS regions - A VPC is located within an AWS region.']
Question: A start-up would like to monitor its cost on the AWS Cloud and would like to choose an optimal Savings Plan. As a Cloud Practitioner, which AWS service would you use?
Has explanation: True
Explanation: AWS Cost Explorer 

AWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis and empowers you to dive deeper using several filtering dimensions (e.g., AWS Service, AWS Region, Linked Account, etc.). AWS Cost Explorer also gives you access to a set of default reports to help you get started, while also allowing you to create custom reports from scratch. 

Customers can receive Savings Plan recommendations at the member (linked) account level in addition to the existing AWS organization-level recommendations in AWS Cost Explorer.
Has option_explanations: True
Option explanations: ['AWS Pricing Calculator - AWS Pricing Calculator lets you explore AWS services, and create an estimate for the cost of your use cases on AWS. It does not provide Savings Plan recommendations.', 'AWS Cost Explorer \n\nAWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis and empowers you to dive deeper using several filtering dimensions (e.g., AWS Service, AWS Region, Linked Account, etc.). AWS Cost Explorer also gives you access to a set of default reports to help you get started, while also allowing you to create custom reports from scratch. \n\nCustomers can receive Savings Plan recommendations at the member (linked) account level in addition to the existing AWS organization-level recommendations in AWS Cost Explorer.', 'AWS Budgets - AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reserved instance (RI) utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. It does not provide Savings Plan recommendations.', 'AWS Cost & Usage Report (AWS CUR) - The AWS Cost & Usage Report (AWS CUR) is a single location for accessing comprehensive information about your AWS costs and usage. It does not provide Savings Plan recommendations.']
Question: Which AWS service can inspect Amazon CloudFront distributions running on any HTTP web server?
Has explanation: True
Explanation: AWS Web Application Firewall (AWS WAF)

AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting (XSS).

AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront and lets you control access to your content.

When you use the AWS web application firewall (AWS WAF) on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesnt come at the expense of performance. Blocked requests are stopped before they reach your web servers.

How AWS WAF works: via - https://aws.amazon.com/waf/
Has option_explanations: True
Option explanations: ['Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances.', 'AWS Web Application Firewall (AWS WAF)\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting (XSS).\n\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront and lets you control access to your content.\n\nWhen you use the AWS web application firewall (AWS WAF) on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn\x92t come at the expense of performance. Blocked requests are stopped before they reach your web servers.\n\nHow AWS WAF works: via - https://aws.amazon.com/waf/', 'Elastic Load Balancing (ELB) - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It does not inspect Amazon CloudFront distributions. \n\nReference: \n\nhttps://aws.amazon.com/waf/', 'AWS GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It does not inspect Amazon CloudFront distributions.']
Question: Which Amazon Elastic Compute Cloud (Amazon EC2) Auto Scaling feature can help with fault tolerance?
Has explanation: True
Explanation: Replacing unhealthy Amazon EC2 instances 

Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove Amazon EC2 instances according to the conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. 

Amazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and replace it with a new one.
Has option_explanations: True
Option explanations: ['Lower cost by adjusting the number of Amazon EC2 instances - Amazon EC2 Auto Scaling adds instances only when needed, and can scale across purchase options to optimize performance and cost. However, this will not help with fault tolerance.', 'Distributing load to Amazon EC2 instances - Even though this helps with fault tolerance and is often used with Amazon EC2 Auto Scaling, it is a feature of Elastic Load Balancing (ELB) and not an Amazon EC2 Auto Scaling. Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs).', 'Having the right amount of computing capacity - Amazon EC2 Auto Scaling ensures that your application always has the right amount of computing capacity, so your application can handle the workload. \n\nReference: \n\nhttps://aws.amazon.com/ec2/autoscaling/', 'Replacing unhealthy Amazon EC2 instances \n\nAmazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove Amazon EC2 instances according to the conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. \n\nAmazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and replace it with a new one.']
Question: A company would like to optimize Amazon Elastic Compute Cloud (Amazon EC2) costs. Which of the following actions can help with this task? (Select TWO)
Has explanation: True
Explanation: Set up Auto Scaling groups to align the number of instances with the demand
Has option_explanations: True
Option explanations: ["Build its own servers - Building your own servers is more expensive than using EC2 instances in the cloud. You're more likely to spend more money than saving money. \n\nReferences: \n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html \n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/ \n\nhttps://wa.aws.amazon.com/wat.concept.horizontal-scaling.en.html \n\nhttps://aws.amazon.com/autoscaling/", 'Vertically scale the EC2 instances - Vertically scaling EC2 instances (increasing one computer performance by adding CPUs, memory, and storage) is limited and is way more expensive than scaling horizontally (adding more computers to the system).', 'Opt for a higher AWS Support plan - The AWS Support plans do not help with EC2 costs.', 'Set up Auto Scaling groups to align the number of instances with the demand', 'Purchase Amazon EC2 Reserved instances (RIs) \n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. You can adjust its size to meet demand, either manually or by using automatic scaling. \n\nAWS Auto Scaling can help you optimize your utilization and cost efficiencies when consuming AWS services so you only pay for the resources you need. \n\nHow AWS Auto Scaling works: via - https://aws.amazon.com/autoscaling/ \n\nAmazon EC2 Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone (AZ). \n\nEC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/']
Question: Which AWS service can be used to view the most comprehensive billing details for the past month?
Has explanation: True
Explanation: AWS Cost & Usage Report (AWS CUR)

The AWS Cost & Usage Report (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself.

AWS Cost & Usage Report (AWS CUR) Overview :via - https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html
Has option_explanations: True
Option explanations: ['AWS Cost Explorer - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost Explorer includes a default report that helps you visualize the costs and usage associated with your top five cost-accruing AWS services and gives you a detailed breakdown of all services in the table view. The reports let you adjust the time range to view historical data going back up to twelve months to gain an understanding of your cost trends. AWS Cost Explorer cannot provide granular billing details for the past month.', 'AWS Pricing Calculator - AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator cannot provide billing details for the past month.', 'AWS Cost & Usage Report (AWS CUR)\n\nThe AWS Cost & Usage Report (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself.\n\nAWS Cost & Usage Report (AWS CUR) Overview :via - https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html', 'AWS Budgets - AWS Budgets gives the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reservation utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. Budgets can be created at the monthly, quarterly, or yearly level, and you can customize the start and end dates. You can further refine your budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others. AWS Budgets cannot provide billing details for the past month.']
Question: A company based in Sydney hosts its application on an Amazon Elastic Compute Cloud (Amazon EC2) instance in ap-southeast-2. They would like to deploy the same Amazon EC2 instances in eu-south-1. Which of the following AWS entities can address this use case?
Has explanation: True
Explanation: Amazon Machine Image (AMI) 

An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an Amazon Machine Image (AMI) when you launch an instance. You can launch multiple instances from a single Amazon Machine Image (AMI) when you need multiple instances with the same configuration. 

How to use an Amazon Machine Image (AMI): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html
Has option_explanations: True
Option explanations: ['Amazon EBS Elastic Volume snapshots - An Amazon EBS snapshot is a point-in-time copy of your Amazon EBS volume. EBS snapshots are one of the components of an AMI, but EBS snapshots alone cannot be used to deploy the same EC2 instances across different Availability Zones (AZs). \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html', 'AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used to deploy the same EC2 instances across different Availability Zones (AZs).', 'Amazon Machine Image (AMI) \n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an Amazon Machine Image (AMI) when you launch an instance. You can launch multiple instances from a single Amazon Machine Image (AMI) when you need multiple instances with the same configuration. \n\nHow to use an Amazon Machine Image (AMI): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html', 'Elastic Load Balancing (ELB) - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). It cannot be used to deploy the same EC2 instances across different Availability Zones (AZs).']
Question: Which of the following billing timeframes is applied when running a Windows EC2 on-demand instance?
Has explanation: True
Explanation: Pay per second 

With On-Demand instances, you only pay for the Amazon EC2 instances you use. The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs. 

When running a Windows EC2 on-demand instance, pay-per-second pricing is applied.
Has option_explanations: True
Option explanations: ['Pay per minute - Pay per minute pricing is not available for Windows EC2 on-demand instances, or any other type of on-demand EC2 instance.', 'Pay per hour - When running an Amazon Windows EC2 On-demand instance, pay-per-second pricing is applied. Windows-based EC2 instances used to follow pay-per-hour pricing earlier.', 'Pay per day - Pay per day pricing is not available for Windows EC2 on-demand instances, or any other type of on-demand EC2 instance. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/', 'Pay per second \n\nWith On-Demand instances, you only pay for the Amazon EC2 instances you use. The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs. \n\nWhen running a Windows EC2 on-demand instance, pay-per-second pricing is applied.']
Question: Adding more CPU/RAM to an Amazon Elastic Compute Cloud (Amazon EC2) instance represents which of the following?
Has explanation: True
Explanation: Vertical scaling 

A vertically scalable system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.
Has option_explanations: True
Option explanations: ['Horizontal scaling - A horizontally scalable system is one that can increase capacity by adding more computers to the system.', 'Vertical scaling \n\nA vertically scalable system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.', "Managing increasing volumes of data - Traditional data storage and analytics tools can no longer provide the agility and flexibility required to deliver relevant business insights. That\x92s why many organizations are shifting to a data lake architecture. A data lake is an architectural approach that allows you to store massive amounts of data in a central location so that it's readily available to be categorized, processed, analyzed, and consumed by diverse groups within your organization.", 'Loose coupling - As application complexity increases, a desirable attribute of an IT system is that it can be broken into smaller, loosely coupled components. This means that IT systems should be designed in a way that reduces interdependencies\x97a change or a failure in one component should not cascade to other components. \n\nReference: \n\nhttps://wa.aws.amazon.com/wat.concept.horizontal-scaling.en.html']
Question: A company needs to keep sensitive data in its own data center due to compliance but would still like to deploy resources using AWS. Which Cloud deployment model does this refer to?
Has explanation: True
Explanation: Hybrid Cloud 

A hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. 

Overview of Cloud Computing Deployment Models: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['On-premises - This is not a cloud deployment model. When an enterprise opts for on-premises, it needs to create, upgrade, and scale the on-premise IT infrastructure by investing in sophisticated hardware, compatible software, and robust services. Also, the business needs to deploy dedicated IT staff to upkeep, scale, and manage the on-premise infrastructure continuously. \n\nReference: \n\nhttps://docs.aws.amazon.com/whitepapers/latest/public-sector-cloud-transformation/selecting-the-right-cloud-for-workloads-differences-between-public-private-and-hybrid.html', 'Private Cloud - Unlike a Public cloud, a Private cloud enables businesses to avail IT services that are provisioned and customized according to their precise needs. The business can further avail the IT services securely and reliably over a private IT infrastructure.', 'Public Cloud - A public cloud-based application is fully deployed in the cloud and all parts of the application run in the cloud. Applications in the cloud have either been created in the cloud or have been migrated from an existing infrastructure to take advantage of the benefits of cloud computing.', "Hybrid Cloud \n\nA hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. \n\nOverview of Cloud Computing Deployment Models: via - https://aws.amazon.com/types-of-cloud-computing/"]
Question: Which of the following options are the benefits of using AWS Elastic Load Balancing (ELB)? (Select TWO)
Has explanation: True
Explanation: Fault tolerance 

Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). 

Elastic Load Balancing (ELB) offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant: Application Load Balancer (best suited for HTTP and HTTPS traffic), Network Load Balancer (best suited for TCP traffic), and Classic Load Balancer.
Has option_explanations: True
Option explanations: ['Less costly - AWS Elastic Load Balancing (ELB) does not help with reducing costs', 'Agility - Agility refers to new IT resources being only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes. AWS Elastic Load Balancing (ELB) does not help with agility.', 'Fault tolerance \n\nElastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). \n\nElastic Load Balancing (ELB) offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant: Application Load Balancer (best suited for HTTP and HTTPS traffic), Network Load Balancer (best suited for TCP traffic), and Classic Load Balancer.', 'Storage - AWS Elastic Load Balancing (ELB) does not offer storage benefits. It is not a storage-related service. \n\nReference: \n\nhttps://aws.amazon.com/elasticloadbalancing/', 'High availability']
Question: Which of the following services are provided by Amazon Route 53? (Select Two)
Has explanation: True
Explanation: Health checks and monitoring 

Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. 

Amazon Route 53 offers domain name registration services, where you can search for and register available domain names or transfer in existing domain names to be managed by Route 53. 

Amazon Route 53 can monitor the health and performance of your application as well as your web servers and other resources.
Has option_explanations: True
Option explanations: ['Health checks and monitoring \n\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. \n\nAmazon Route 53 offers domain name registration services, where you can search for and register available domain names or transfer in existing domain names to be managed by Route 53. \n\nAmazon Route 53 can monitor the health and performance of your application as well as your web servers and other resources.', "Transfer acceleration - Transfer acceleration is a feature of Amazon's simple storage service (Amazon S3). Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. \n\nReference: \n\nhttps://aws.amazon.com/route53/", 'Domain registration', 'IP routing - Despite its name, Amazon Route 53 does not offer IP routing. However, it can route traffic based on multiple criteria, such as endpoint health, geographic location, and latency, using routing policies.', 'Load balancing - It is a feature of Elastic Load Balancing (ELB) and not Amazon Route 53. Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs).']
Question: According to the AWS Shared Responsibility Model, which of the following is the responsibility of the customer?
Has explanation: True
Explanation: Firewall & networking configuration of Amazon Elastic Compute Cloud (Amazon EC2) 

The customers are responsible for "Security IN the cloud". It includes the configuration of the operating system, network & firewall of applications.
Has option_explanations: True
Option explanations: ['Protecting hardware infrastructure', 'Edge locations security \n\nAWS is responsible for "Security OF the cloud". It includes the infrastructure, which is composed of the hardware, software, networking, and facilities that run AWS Cloud services. \n\nReference: \n\nhttps://aws.amazon.com/compliance/shared-responsibility-model/', 'Managing Amazon DynamoDB - Amazon DynamoDB is a fully managed service. AWS operates the infrastructure layer, the operating system, and platforms, and customers access the endpoints to store and retrieve data.', 'Firewall & networking configuration of Amazon Elastic Compute Cloud (Amazon EC2) \n\nThe customers are responsible for "Security IN the cloud". It includes the configuration of the operating system, network & firewall of applications.']
Question: According to the AWS Well-Architected Framework, which of the following statements are recommendations in the Operational Excellence pillar? (Select two)
Has explanation: True
Explanation: Anticipate failure
Has option_explanations: True
Option explanations: ['Enable traceability - Monitor, alert, and audit actions and changes to your environment in real-time. Integrate logs and metrics with systems to automatically respond and take action. It is a design principle of the Security pillar.', "Automatically recover from failure - By monitoring a system for key performance indicators (KPIs), you can trigger automation when a threshold is breached. This allows for automatic notification and tracking of failures, and for automated recovery processes that work around or repair the failure. With more sophisticated automation, it's possible to anticipate and remediate failures before they occur. It is a design principle of the Reliability pillar.", 'Anticipate failure', 'Use serverless architectures - In the cloud, serverless architectures remove the need for you to run and maintain servers to carry out traditional compute activities. For example, storage services can act as static websites, removing the need for web servers, and event services can host your code for you. This not only removes the operational burden of managing these servers but also can lower transactional costs because these managed services operate at a cloud scale. It is a design principle of the Performance Efficiency pillar. \n\nReference: \n\nhttps://wa.aws.amazon.com/index.en.html', 'Make frequent, small, reversible changes \n\nThe Operational Excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures. \n\nPerform \x93pre-mortem\x94 exercises to identify potential sources of failure so that they can be removed or mitigated. Test your failure scenarios and validate your understanding of their impact. Test your response procedures to ensure that they are effective, and that teams are familiar with their execution. Set up regular game days to test workloads and team responses to simulated events. \n\nDesign workloads to allow components to be updated regularly. Make changes in small increments that can be reversed if they fail (without affecting customers when possible). \n\nThe AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. \n\nThe AWS Well-Architected Framework is based on six pillars \x97 Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. \n\nOverview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/']
Question: A corporation would like to simplify access management to multiple AWS accounts as well as facilitate AWS Single Sign-On (AWS SSO) access to its AWS accounts. As a Cloud Practitioner, which AWS service would you use for this task?
Has explanation: True
Explanation: AWS IAM Identity Center 

AWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both. 

You can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access their assigned AWS accounts or cloud applications. 

You can use IAM Identity Center to quickly and easily assign and manage your employees access to multiple AWS accounts, SAML-enabled cloud applications (such as Salesforce, Microsoft 365, and Box), and custom-built in-house applications, all from a central place. 

How AWS IAM Identity Center works: via - https://aws.amazon.com/iam/identity-center/
Has option_explanations: True
Option explanations: ['AWS IAM Identity Center \n\nAWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both. \n\nYou can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access their assigned AWS accounts or cloud applications. \n\nYou can use IAM Identity Center to quickly and easily assign and manage your employees\x92 access to multiple AWS accounts, SAML-enabled cloud applications (such as Salesforce, Microsoft 365, and Box), and custom-built in-house applications, all from a central place. \n\nHow AWS IAM Identity Center works: via - https://aws.amazon.com/iam/identity-center/', 'AWS Command Line Interface (CLI) - The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. It is not a central user portal. \n\nReference: \n\nhttps://aws.amazon.com/iam/identity-center/', 'AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (AWS IAM) enables you to securely control access to AWS services and resources for your users. Using AWS IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. It is not used to log in but to manage users and roles.', 'AWS Cognito - Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system. It is an identity management solution for customers/developers building B2C or B2B apps for their customers.']
Question: A company would like to move 50 petabytes (PBs) of data from its on-premises data centers to AWS in the MOST cost-effective way. As a Cloud Practitioner, which of the following solutions would you choose?
Has explanation: True
Explanation: AWS Snowmobile 

AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.
Has option_explanations: True
Option explanations: ['AWS Storage Gateway - AWS Storage Gateway is a hybrid storage service that enables your on-premises applications to seamlessly use AWS cloud storage. You can use the service for backup and archiving, disaster recovery, cloud data processing, storage tiering, and migration. However, data transfer through AWS Storage Gateway takes longer even with great bandwidth. Moreover, transferring 50 PBs of data will be more expensive than using AWS Snowmobile. \n\nReference: \n\nhttps://aws.amazon.com/snowmobile/', 'AWS Snowmobile \n\nAWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.', 'AWS Snowball - AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. The use of Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with AWS Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed Internet. However, one Snowball only provides up to 80 TB of capacity. Therefore, to transfer 50 PBs, AWS Snowball is not the most cost-effective option.', 'AWS Snowball Edge - AWS Snowball Edge is an edge computing and data transfer device provided by the AWS Snowball service. It has onboard storage and compute power that provides select AWS services for use in edge locations. However, one AWS Snowball Edge only provides up to 100 TB of capacity. Therefore, to transfer 50 PBs, AWS Snowball Edge is not the most cost-effective option.']
Question: Which AWS tool can provide best practice recommendations for performance, service limits, and cost optimization?
Has explanation: True
Explanation: AWS Trusted Advisor 

AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. 

How AWS Trusted Advisor works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/
Has option_explanations: True
Option explanations: ['AWS Trusted Advisor \n\nAWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. \n\nHow AWS Trusted Advisor works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on your Amazon EC2 instances. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Nevertheless, it does not provide best practice recommendations.', 'AWS Health Dashboard - Service health - AWS Health Dashboard - Service health publishes most up-to-the-minute information on the status and availability of all AWS services in tabular form for all Regions that AWS is present in. It does not provide best practice recommendations.', 'Amazon CloudWatch - Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. Amazon CloudWatch provides data and actionable insights to monitor applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. This is an excellent service for building Resilient systems. Think resource performance monitoring, events, and alerts; think Amazon CloudWatch. Amazon CloudWatch does not provide best practice recommendations. \n\nReference: \nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor//']
Question: What is the primary use case for Amazon GuardDuty?
Has explanation: True
Explanation: Detecting malicious activity and threats in your AWS accounts and workloads 

Amazon GuardDuty is a managed threat detection service that identifies potential security risks by analyzing logs, such as AWS CloudTrail, VPC Flow Logs, and DNS query logs. It helps organizations detect suspicious or malicious activity in their AWS environments. 

Amazon GuardDuty Overview: via - https://aws.amazon.com/guardduty/
Has option_explanations: True
Option explanations: ['Protecting web applications from common exploits and vulnerabilities such as SQL injection - This is the primary use case for AWS Web Application Firewall (WAF), which protects web applications from vulnerabilities like SQL injection and cross-site scripting. GuardDuty detects threats but does not directly protect applications. \n\nReference: \n\nhttps://aws.amazon.com/guardduty/', 'Encrypting data in transit between AWS services using TLS certificates. - AWS Certificate Manager (ACM) or AWS Key Management Service (KMS) is responsible for managing encryption and TLS certificates. GuardDuty does not handle encryption but focuses on identifying potential security threats.', 'Detecting malicious activity and threats in your AWS accounts and workloads \n\nAmazon GuardDuty is a managed threat detection service that identifies potential security risks by analyzing logs, such as AWS CloudTrail, VPC Flow Logs, and DNS query logs. It helps organizations detect suspicious or malicious activity in their AWS environments. \n\nAmazon GuardDuty Overview: via - https://aws.amazon.com/guardduty/', 'Enforcing secure communication between VPCs using network traffic filtering - This is a feature provided by AWS Network Firewall or Security Groups, which allow you to control network traffic into and out of your VPCs. GuardDuty detects malicious activities but does not enforce network traffic filtering.']
Question: Which of the following AWS Identity and Access Management (AWS IAM) Security Tools allows you to review permissions granted to an IAM user?
Has explanation: True
Explanation: AWS Identity and Access Management (IAM) access advisor 

IAM Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.
Has option_explanations: True
Option explanations: ['AWS Identity and Access Management (IAM) access advisor \n\nIAM Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.', 'Multi-Factor Authentication (MFA) - Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password. with Multi-Factor Authentication (MFA) enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factor\x97what they know), as well as for an authentication code from their AWS MFA device (the second factor\x97what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources. It cannot be used to review permissions granted. \n\nReference: \n\nhttps://aws.amazon.com/about-aws/whats-new/2019/06/now-use-iam-access-advisor-with-aws-organizations-to-set-permission-guardrails-confidently/', 'IAM policy - IAM policies define permissions for an action regardless of the method that you use to perform the operation.', 'IAM credentials report - You can generate and download a credential report that lists all IAM users in your account and the status of their various credentials, including passwords, access keys, and multi-factor authentication (MFA) devices. It is not used to review permissions granted to an IAM user.']
Question: Which of the following are the advantages of using the AWS Cloud? (Select TWO)
Has explanation: True
Explanation: Stop guessing about capacity
Has option_explanations: True
Option explanations: ['Stop guessing about capacity', 'Increase speed and agility', 'Limited scaling - Scaling is not limited in the cloud. You can access as much or as little capacity as you need, and scale up and down as required with only a few minutes\x92 notice.', 'AWS is responsible for security in the cloud - AWS is responsible for the security OF the cloud, which means AWS is responsible for protecting the infrastructure that runs all the services offered in the AWS Cloud.', 'Trade operational expense for capital expense - In the cloud, you trade capital expense (CAPEX) for the operational expense (OPEX). Instead of having to invest heavily in data centers and servers before you know how you\x92re going to use them, you can pay only when you consume computing resources, and pay only for how much you consume. \n\nReference: \n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html']
Question: A company using a hybrid cloud would like to store secondary backup copies of the on-premises data. Which Amazon S3 Storage Class would you use for a cost-optimal yet rapid access solution?
Has explanation: True
Explanation: Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) 

Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. Its a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region using S3 cross-region replication (S3 CRR).
Has option_explanations: True

=== Quiz Result Debug ===
Question: According to the AWS Shared Responsibility Model, which of the following are the responsibilities of AWS? (Select two)
Has explanation: True
Explanation: Data center security
Has option_explanations: True
Option explanations: ['Encrypting application data - The customers are responsible for encrypting application data.', 'Configuring IAM Roles - The customers are responsible for configuring IAM Roles. \n\nReference: \n\nhttps://aws.amazon.com/compliance/shared-responsibility-model/', 'Data center security', 'Installing security patches of the guest operating system (OS) - The customers are responsible for patching their guest operating system. \n\nPlease review the IT controls under the AWS Shared Responsibility Model: via - https://aws.amazon.com/compliance/shared-responsibility-model/', 'Network operability \n\nAWS responsibility \x93Security OF the Cloud\x94 - AWS is responsible for protecting the infrastructure that runs all of the services offered in the AWS Cloud. This infrastructure is composed of the hardware, software, networking, and facilities that run AWS Cloud services.']
Question: A Cloud Practitioner would like to deploy identical resources across all AWS regions and accounts using templates while estimating costs. Which AWS service can assist with this task?
Has explanation: True
Explanation: AWS CloudFormation

AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.

You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. This provides a single source of truth for all your resources and helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting.

AWS CloudFormation templates allow you to estimate the cost of your resources.

How AWS CloudFormation works: via - https://aws.amazon.com/cloudformation/
Has option_explanations: True
Option explanations: ['AWS CodeDeploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. Unlike AWS CloudFormation, it does not deal with infrastructure configuration and orchestration. \n\nReference: \n\nhttps://aws.amazon.com/cloudformation/', 'Amazon LightSail - Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server with AWS. It is not best suited when deploying more complex resources, while AWS CloudFormation can.', 'AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) - AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD), also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. It is not used to deploy resources.', 'AWS CloudFormation\n\nAWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\n\nYou can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. This provides a single source of truth for all your resources and helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting.\n\nAWS CloudFormation templates allow you to estimate the cost of your resources.\n\nHow AWS CloudFormation works: via - https://aws.amazon.com/cloudformation/']
Question: Which AWS service can be used to subscribe to an RSS feed to be notified of the status of all AWS service interruptions?
Has explanation: True
Explanation: AWS Health Dashboard - Service Health 

The AWS Health Dashboard  Service health is the single place to learn about the availability and operations of AWS services. You can view the overall status of AWS services, and you can sign in to view personalized communications about your particular AWS account or organization. 

You can check on this page https://health.aws.amazon.com/health/status to get current status information. 

The AWS Health Dashboard  Service health offers the possibility to subscribe to an RSS feed to be notified of interruptions to each service.
Has option_explanations: True
Option explanations: ["Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. It can be used to deliver notifications, but it does not provide the current services' status.", 'AWS Health Dashboard - Your Account Health - Your AWS Health Dashboard \x96 Your Account Health provides alerts and remediation guidance when AWS is experiencing events that may impact you.', 'AWS Health Dashboard - Service Health \n\nThe AWS Health Dashboard \x96 Service health is the single place to learn about the availability and operations of AWS services. You can view the overall status of AWS services, and you can sign in to view personalized communications about your particular AWS account or organization. \n\nYou can check on this page https://health.aws.amazon.com/health/status to get current status information. \n\nThe AWS Health Dashboard \x96 Service health offers the possibility to subscribe to an RSS feed to be notified of interruptions to each service.', "AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It does not provide all AWS services' status. \n\nReference: \n\nhttps://health.aws.amazon.com/health/status"]
Question: A production company would like to establish an AWS managed virtual private network (VPN) service between its on-premises network and AWS. Which item needs to be set up on the company's side?
Has explanation: True
Explanation: A customer gateway 

A customer gateway device is a physical or software appliance on your side of a Site-to-Site VPN connection. You or your network administrator must configure the device to work with the Site-to-Site VPN connection. 

You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. More on customer gateway device: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html 


Has option_explanations: True
Option explanations: ['A VPC endpoint interface - An interface VPC endpoint (interface endpoint) enables you to connect to services powered by AWS PrivateLink. It is not a component of a connection between on-premises network and AWS.', 'A virtual private gateway (VGW) - A virtual private gateway (VGW) device is a physical or software appliance on AWS side of a Site-to-Site VPN connection. \n\nReferences: \n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html \n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html', 'A security group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. It is not a component of a connection between on-premises network and AWS.', 'A customer gateway \n\nA customer gateway device is a physical or software appliance on your side of a Site-to-Site VPN connection. You or your network administrator must configure the device to work with the Site-to-Site VPN connection. \n\nYou can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. More on customer gateway device: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html \n\n']
Question: Which types of monitoring can be provided by Amazon CloudWatch? (Select TWO)
Has explanation: True
Explanation: Resource utilization 

Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon DynamoDB tables, and Amazon Amazon Relational Database Service (Amazon RDS) DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. 

You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. 

How Amazon CloudWatch works: via - https://aws.amazon.com/cloudwatch/
Has option_explanations: True
Option explanations: ['API access - Recording API calls is a feature of AWS CloudTrail, not Amazon CloudWatch.', 'Performance and availability of AWS services - The AWS Health - Your Account Health Dashboard gives you a personalized view of the performance and availability of the AWS services underlying your AWS resources, not Amazon CloudWatch.', 'Resource utilization \n\nAmazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon DynamoDB tables, and Amazon Amazon Relational Database Service (Amazon RDS) DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. \n\nYou can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. \n\nHow Amazon CloudWatch works: via - https://aws.amazon.com/cloudwatch/', 'Application performance', 'Account management - AWS Identity and Access Management (AWS IAM) is usually used to manage accounts, not Amazon CloudWatch.\n\nReferences:\n\nhttps://aws.amazon.com/cloudwatch/features/\n\nhttps://aws.amazon.com/cloudwatch/']
Question: Which of the following are the best practices when using AWS Organizations? (Select TWO)
Has explanation: True
Explanation: Create AWS accounts per department
Has option_explanations: True
Option explanations: ['Do not use AWS Organizations to automate AWS account creation - AWS Organizations helps you simplify IT operations by automating AWS account creation and management. The AWS Organizations APIs enable you to create new accounts programmatically and to add new accounts to a group. The policies attached to the group are automatically applied to the new account. \n\nReference: \n\nhttps://aws.amazon.com/organizations/', 'Never use tags for billing - You should use tags standards to categorize AWS resources for billing purposes.', 'Disable AWS CloudTrail on several accounts - You should enable AWS CloudTrail to monitor activity on all accounts for governance, compliance, risk, and auditing purposes.', 'Create AWS accounts per department', 'Restrict account privileges using Service Control Policies (SCP) \n\nAWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Whether you are a growing startup or a large enterprise, AWS Organizations help you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. \n\nUsing AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use AWS Organizations to define central configurations and resource sharing across accounts in your organization. AWS Organizations is available to all AWS customers at no additional charge. \n\nYou should create accounts per department based on regulatory restrictions (using Service Control Policies (SCP)) for better resource isolation, and to have separate per-account service limits. \n\nAWS Organizations allows you to restrict what services and actions are allowed in your accounts. You can use the Service Control Policies (SCP) to apply permission guardrails on AWS Identity and Access Management (IAM) users and roles.']
Question: The development team at a company manages 300 microservices and it is now trying to automate the code reviews to improve the code quality. Which tool/service is the right fit for this requirement?
Has explanation: True
Explanation: Amazon CodeGuru 

Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an applications most expensive lines of code. Integrate Amazon CodeGuru into your existing software development workflow to automate code reviews during application development, continuously monitor application performance in production, provide recommendations and visual clues for improving code quality and application performance, and reduce overall cost. 

Amazon CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. 

Amazon CodeGuru Profiler pinpoints an applications most expensive lines of code by helping developers understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. 

How Amazon CodeGuru works: via - https://aws.amazon.com/codeguru/
Has option_explanations: True
Option explanations: ['AWS CodeBuild - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With AWS CodeBuild, you don\x92t need to provision, manage, and scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.', 'AWS X-Ray - AWS X-Ray helps developers analyze and debug production, and distributed applications, such as those built using a microservices architecture. With AWS X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\x92s underlying components.', 'Amazon CodeGuru \n\nAmazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application\x92s most expensive lines of code. Integrate Amazon CodeGuru into your existing software development workflow to automate code reviews during application development, continuously monitor application performance in production, provide recommendations and visual clues for improving code quality and application performance, and reduce overall cost. \n\nAmazon CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. \n\nAmazon CodeGuru Profiler pinpoints an application\x92s most expensive lines of code by helping developers understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. \n\nHow Amazon CodeGuru works: via - https://aws.amazon.com/codeguru/', 'AWS Trusted Advisor - AWS Trusted Advisors provides recommendations that help you follow AWS best practices. AWS Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. \n\nReference: \n\nhttps://aws.amazon.com/codeguru/']
Question: Which of the following AWS Support plans is the MOST cost-effective when getting enhanced technical support by Cloud Support Engineers?
Has explanation: True
Explanation: AWS Business Support 

AWS recommends AWS Business Support if you have production workloads on AWS and want 24x7 phone, email, and chat access to technical support and architectural guidance in the context of your specific use cases. You get full access to AWS Trusted Advisor Best Practice Checks. It is also the cheapest support plan to provide enhanced technical support by Cloud Support Engineers.
Has option_explanations: True
Option explanations: ['AWS Developer Support - AWS recommends AWS Developer Support if you are testing or doing early development on AWS and want the ability to get technical support during business hours as well as general architectural guidance as you build and test. It provides enhanced technical support by Cloud Support Associates.', 'AWS Basic Support - The AWS Basic Support plan is included for all AWS customers. It does not provide enhanced technical support.', 'AWS Business Support \n\nAWS recommends AWS Business Support if you have production workloads on AWS and want 24x7 phone, email, and chat access to technical support and architectural guidance in the context of your specific use cases. You get full access to AWS Trusted Advisor Best Practice Checks. It is also the cheapest support plan to provide enhanced technical support by Cloud Support Engineers.', 'AWS Enterprise Support - AWS Enterprise Support provides customers with concierge-like service where the main focus is helping the customer achieve their outcomes and find success in the cloud. With Enterprise Support, you get 24x7 technical support from high-quality engineers, tools, and technology to automatically manage the health of your environment, consultative architectural guidance delivered in the context of your applications and use cases, and a designated Technical Account Manager (TAM) to coordinate access to proactive/preventative programs and AWS subject matter experts. It provides enhanced technical support by Cloud Support Engineers but is more expensive than the Business support plan. \n\nReferences: \n\nhttps://aws.amazon.com/premiumsupport/plans/ \n\nhttps://aws.amazon.com/premiumsupport/plans/business/']
Question: Which AWS service allows you to quickly and easily add user sign-up, sign-in, and access control to web and mobile applications?
Has explanation: True
Explanation: Amazon Cognito 

Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system.
Has option_explanations: True
Option explanations: ['AWS Organizations - AWS Organizations offers policy-based management for multiple AWS accounts. With AWS Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. Organizations enable you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It does not provide user sign-up, sign-in, and access control to web and mobile applications. \n\nReference: \n\nhttps://aws.amazon.com/cognito/', 'AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (AWS IAM) enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. It does not provide user sign-up, sign-in, and access control to web and mobile applications.', 'Amazon Cognito \n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system.', 'AWS IAM Identity Center - AWS IAM Identity Center is the successor to AWS Single Sign-On. It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create, or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both.']
Question: Which of the following AWS services can be used to generate, use, and manage encryption keys on the AWS Cloud?
Has explanation: True
Explanation: AWS CloudHSM 

The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. 

AWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only to you. 

How AWS CloudHSM works: via - https://aws.amazon.com/cloudhsm/
Has option_explanations: True
Option explanations: ['AWS GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It cannot be used to generate, use, and manage encryption keys.', 'AWS CloudHSM \n\nThe AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. \n\nAWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only to you. \n\nHow AWS CloudHSM works: via - https://aws.amazon.com/cloudhsm/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. It cannot be used to generate, use, and manage encryption keys.', 'AWS Secrets Manager - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It is integrated with AWS CloudHSM to generate, use, and manage encryption keys. \n\nReference: \n\nhttps://aws.amazon.com/cloudhsm/']
Question: A company would like to audit requests made to an Amazon Simple Storage Service (Amazon S3) bucket. As a Cloud Practitioner, which Amazon Simple Storage Service (Amazon S3) feature would you recommend addressing this use-case?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) Access Logs 

Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. 

It can also help you learn about your customer base and understand your Amazon S3 bill.
Has option_explanations: True
Option explanations: ['Amazon S3 Bucket Policies - Amazon S3 Bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates. It does not help with auditing requests made to your bucket.', 'S3 cross-region replication (S3 CRR) - S3 cross-region replication (S3 CRR) enables automatic, asynchronous copying of objects across Amazon S3 buckets. Cross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. It does not help with auditing requests made to your bucket.', 'Amazon Simple Storage Service (Amazon S3) Access Logs \n\nServer access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. \n\nIt can also help you learn about your customer base and understand your Amazon S3 bill.', 'S3 Versioning - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. It does not help with auditing requests made to your bucket. \n\nReference: \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html']
Question: A company would like to move its infrastructure to AWS Cloud. Which of the following should be included in the Total Cost of Ownership (TCO) estimate? (Select TWO)
Has explanation: True
Explanation: Power/Cooling 

AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator can be accessed at https://calculator.aws/#/. 

AWS Pricing Calculator compares the cost of your applications in an on-premises or traditional hosting environment to AWS: server, storage, network, and IT labor. Therefore, you need to include every element relevant to these points of comparison. 

Server administration is included in the IT labor costs. 

Power/Cooling are included in the server, storage, and network cost.
Has option_explanations: True
Option explanations: ['Power/Cooling \n\nAWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator can be accessed at https://calculator.aws/#/. \n\nAWS Pricing Calculator compares the cost of your applications in an on-premises or traditional hosting environment to AWS: server, storage, network, and IT labor. Therefore, you need to include every element relevant to these points of comparison. \n\nServer administration is included in the IT labor costs. \n\nPower/Cooling are included in the server, storage, and network cost.', 'Electronic equipment at office - The electronic equipment at the office is not relevant for a Total Cost of Ownership (TCO) estimate. \n\nReferences: \n\nhttps://calculator.aws/#/ \n\nhttps://aws.amazon.com/blogs/aws/new-cloud-tco-comparison-calculator-for-web-applications/', 'Server administration', 'Application advertising - The application advertising is not relevant for a Total Cost of Ownership (TCO) estimate.', 'Number of end-users - The number of end-users is not relevant for a Total Cost of Ownership (TCO) estimate.']
Question: An e-commerce company would like to build a chatbot for its customer service using Natural Language Understanding (NLU). As a Cloud Practitioner, which AWS service would you use?
Has explanation: True
Explanation: Amazon Lex

Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Amazon Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language chatbots to new and existing applications.

Amazon Lex Use Cases: via - https://aws.amazon.com/lex/
Has option_explanations: True
Option explanations: ['Amazon SageMaker - Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.', 'Amazon Lex\n\nAmazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Amazon Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language \x91chatbots\x92 to new and existing applications.\n\nAmazon Lex Use Cases: via - https://aws.amazon.com/lex/', 'Amazon Comprehend - Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing Natural Language Processing (NLP), you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text. \n\nReference: \n\nhttps://aws.amazon.com/lex/', 'Amazon Rekognition - With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos and also detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.']
Question: A growing start-up has trouble identifying and protecting sensitive data at scale. Which AWS fully managed service can assist with this task?
Has explanation: True
Explanation: Amazon Macie 

Amazon Macie is a fully managed data security and data privacy service that uses Machine Learning and pattern matching to discover and protect your sensitive data in AWS. 

Amazon Macie uses Machine Learning and pattern matching to cost-efficiently discover sensitive data at scale. Amazon Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. 

How Amazon Macie works: via - https://aws.amazon.com/macie/
Has option_explanations: True
Option explanations: ['AWS Key Management Service (AWS KMS) - AWS Key Management Service (AWS KMS) makes it easy for you to create and manage keys and control the use of encryption across a wide range of AWS services and in your applications. It is not used to discover and protect sensitive data in AWS. \n\nReference: \n\nhttps://aws.amazon.com/macie/', 'AWS Artifact - AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS\x92 security and compliance reports and selects online agreements. It is not used to discover and protect sensitive data in AWS.', 'Amazon Macie \n\nAmazon Macie is a fully managed data security and data privacy service that uses Machine Learning and pattern matching to discover and protect your sensitive data in AWS. \n\nAmazon Macie uses Machine Learning and pattern matching to cost-efficiently discover sensitive data at scale. Amazon Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. \n\nHow Amazon Macie works: via - https://aws.amazon.com/macie/', 'AWS Secrets Manager - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It is not used to discover and protect sensitive data in AWS.']
Question: An organization would like to copy data across different Availability Zones (AZs) using Amazon EBS snapshots. Where are Amazon EBS snapshots stored in the AWS Cloud?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) 

You can create a point-in-time snapshot of an Amazon EBS Elastic Volume and use it as a baseline for new volumes or data backup. If you make periodic snapshots of a volume, the snapshots are incrementalthe new snapshot saves only the blocks that have changed since your last snapshot. 

You can back up the data on your Amazon EBS Elastic Volumes to Amazon Simple Storage Service (Amazon S3) by taking point-in-time snapshots.
Has option_explanations: True
Option explanations: ['Amazon Elastic File System (Amazon EFS) - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, elastic file system for Linux-based workloads for use with AWS Cloud services and on-premises resources. Amazon EBS snapshots cannot be stored on Amazon EFS. \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html', 'Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. Amazon EBS snapshots cannot be stored on Amazon RDS.', 'Amazon Simple Storage Service (Amazon S3) \n\nYou can create a point-in-time snapshot of an Amazon EBS Elastic Volume and use it as a baseline for new volumes or data backup. If you make periodic snapshots of a volume, the snapshots are incremental\x97the new snapshot saves only the blocks that have changed since your last snapshot. \n\nYou can back up the data on your Amazon EBS Elastic Volumes to Amazon Simple Storage Service (Amazon S3) by taking point-in-time snapshots.', 'Amazon Elastic Compute Cloud (Amazon EC2) - Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Amazon EBS snapshots cannot be stored on Amazon EC2.']
Question: Which AWS serverless service allows you to prepare data for analytics?
Has explanation: True
Explanation: AWS Glue 

AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. 

How AWS Glue works: via - https://aws.amazon.com/glue/
Has option_explanations: True
Option explanations: ['AWS Glue \n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. \n\nHow AWS Glue works: via - https://aws.amazon.com/glue/', 'Amazon EMR - Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. EMR is used for analytics and not to prepare data for analytics. \n\nReference: \n\nhttps://aws.amazon.com/glue/', 'Amazon Redshift - Amazon Redshift is a fast and scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift is used for analytics and not to prepare data for analytics.', 'Amazon Athena - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Amazon Athena is used for analytics and not to prepare data for analytics.']
Question: Which of the following statements is an AWS best practice when architecting for the Cloud?
Has explanation: True
Explanation: Automation 

Automation should be implemented to improve both your system's stability and the efficiency of your organization. There are many services to automate application architecture (AWS Elastic Beanstalk, Auto Scaling, AWS Lambda, etc.) to ensure more resiliency, scalability, and performance.
Has option_explanations: True
Option explanations: ["Automation \n\nAutomation should be implemented to improve both your system's stability and the efficiency of your organization. There are many services to automate application architecture (AWS Elastic Beanstalk, Auto Scaling, AWS Lambda, etc.) to ensure more resiliency, scalability, and performance.", 'Servers, not services - The correct best practice is: "Services, not servers". AWS recommends developing, managing, and operating applications, especially at scale, using the broad set of compute, storage, database, analytics, applications, and deployment services offered by AWS to move faster and lower IT costs.', 'Close coupling - The correct best practice is: "Loose coupling". AWS recommends that, as application complexity increases, IT systems should be designed in a way that reduces interdependencies. Therefore, a change or a failure in one component should not cascade to other components.', 'Security comes last - AWS allows you to improve your security in many, more simple ways. Therefore, you should take advantage of this and implement a high level of security. \n\nReference: \n\nhttps://aws.amazon.com/architecture/well-architected/']
Question: A Cloud Practitioner would like to get operational insights of its resources to quickly identify any issues that might impact applications using those resources. Which AWS service can help with this task?
Has explanation: True
Explanation: AWS Systems Manager 

AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. 

With AWS Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. You can also take action on each resource group depending on your operational needs. AWS Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. 

How AWS Systems Manager works: via - https://aws.amazon.com/systems-manager/
Has option_explanations: True
Option explanations: ['AWS Health Dashboard - Your Account Health - AWS Health Dashboard - Your Account Health provides alerts and remediation guidance when AWS is experiencing events that may impact you. It is not used to get operational insights of AWS resources.', 'AWS Systems Manager \n\nAWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. \n\nWith AWS Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. You can also take action on each resource group depending on your operational needs. AWS Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. \n\nHow AWS Systems Manager works: via - https://aws.amazon.com/systems-manager/', 'AWS Trusted Advisor - AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. It is not used to get operational insights of AWS resources. \n\nReference: \n\nhttps://aws.amazon.com/systems-manager/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not used to get operational insights of AWS resources.']
Question: A research lab needs to be notified in case of a configuration change for security and compliance reasons. Which AWS service can assist with this task?
Has explanation: True
Explanation: AWS Config 

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. 

How AWS Config works: via - https://aws.amazon.com/config/
Has option_explanations: True
Option explanations: ['AWS Config \n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. \n\nHow AWS Config works: via - https://aws.amazon.com/config/', 'AWS Secrets Manager - AWS Secrets Manager helps you protect the secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It cannot notify configuration changes. \n\nReference: \n\nhttps://aws.amazon.com/config/', 'AWS Trusted Advisor - AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. It cannot notify configuration changes.', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on your Amazon EC2 instances. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. It cannot notify configuration changes.']
Question: Which of the following criteria are used to calculate the charge for Amazon EBS Volumes? (Select Two)
Has explanation: True
Explanation: Volume type 

Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone (AZ) to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutesall while paying a low price for only what you provision. 

The fundamental charges for EBS volumes are the volume type (based on performance), the storage volume in GB per month provisioned, the number of IOPS provisioned per month, the storage consumed by snapshots, and outbound data transfer.
Has option_explanations: True
Option explanations: ['Volume type \n\nAmazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone (AZ) to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutes\x97all while paying a low price for only what you provision. \n\nThe fundamental charges for EBS volumes are the volume type (based on performance), the storage volume in GB per month provisioned, the number of IOPS provisioned per month, the storage consumed by snapshots, and outbound data transfer.', 'The Amazon EC2 instance type the Amazon EBS Elastic volume is attached to - The Amazon EC2 instance type the Amazon EBS volume is attached to does not influence the EBS volume pricing.', 'Provisioned IOPS', 'Data type - The type of data stored on EBS volumes does not influence the price. \n\nReference: \n\nhttps://aws.amazon.com/ebs/pricing/', 'Data transfer IN - Data transfer-in is always free, including for Amazon EBS Elastic Volumes.']
Question: Which AWS service can be used to send, store, and receive messages between software components at any volume to decouple application tiers?
Has explanation: True
Explanation: Amazon Simple Queue Service (Amazon SQS)

Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.

Using Amazon Simple Queue Service (Amazon SQS), you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.
Has option_explanations: True
Option explanations: ['AWS Organizations - AWS Organizations offers policy-based management for multiple AWS accounts. With AWS Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. AWS Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It is not used to send, store, and receive messages between software components. \n\nReference: \n\nhttps://aws.amazon.com/sqs/', 'AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring. It is not used to send, store, and receive messages between software components.', 'Amazon Simple Queue Service (Amazon SQS)\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.\n\nUsing Amazon Simple Queue Service (Amazon SQS), you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.', 'Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\n\nPlease review this reference architecture for building a decoupled order processing system using Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS): via - https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/']
Question: A production company with predictable usage would like to reduce the cost of its Amazon Elastic Compute Cloud (Amazon EC2) instances by using reserved instances (RI). Which of the following length terms are available for Amazon EC2 reserved instances (RI)? (Select Two)
Has explanation: True
Explanation: 1 year
Has option_explanations: True
Option explanations: ['2 years - It is not possible to reserve instances for 2 years. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/', '6 months - It is not possible to reserve instances for 6 months.', '5 years - It is not possible to reserve instances for 5 years.', '1 year', '3 years\n\nReserved Instances (RI) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. Besides, when Reserved Instances (RI) are assigned to a specific Availability Zone (AZ), they provide a capacity reservation, giving you additional confidence in your ability to launch instances when you need them.\n\nStandard and Convertible reserved instances can be purchased for a 1-year or 3-year term.\n\nEC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/']
Question: A data science team would like to build Machine Learning models for its projects. Which AWS service can it use?
Has explanation: True
Explanation: Amazon SageMaker 

Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.
Has option_explanations: True
Option explanations: ['Amazon SageMaker \n\nAmazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.', 'Amazon Comprehend - Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing natural language processing (NLP), you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text.', 'Amazon Connect - Amazon Connect is an omnichannel cloud contact center. You can set up a contact center in a few steps, add agents who are located anywhere, and start engaging with your customers. You can create personalized experiences for your customers using omnichannel communications. Amazon Connect is an open platform that you can integrate with other enterprise applications. \n\nReference: \n\nhttps://aws.amazon.com/sagemaker/', "Amazon Polly - You can use Amazon Polly to turn text into lifelike speech thereby allowing you to create applications that talk. Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech."]
Question: Which service/tool will you use to create and provide trusted users with temporary security credentials that can control access to your AWS resources?
Has explanation: True
Explanation: AWS Security Token Service (AWS STS) 

AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (AWS IAM) users or for users that you authenticate (federated users). 

You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: 

Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. 

Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so. 

Temporary security credentials are generated by AWS Security Token Service (AWS STS). By default, AWS STS is a global service with a single endpoint at https://sts.amazonaws.com. However, you can also choose to make AWS STS API calls to endpoints in any other supported Region.
Has option_explanations: True
Option explanations: ['Amazon Cognito - Amazon Cognito is a higher level of abstraction than AWS Security Token Service (AWS STS). Amazon Cognito supports the same identity providers as AWS STS, and also supports unauthenticated (guest) access, and lets you migrate user data when a user signs in. Amazon Cognito also provides API operations for synchronizing user data so that it is preserved as users move between devices. Amazon Cognito helps create the user database, which is not possible with STS.', 'AWS Web Application Firewall (AWS WAF) - AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. \n\nReference: \n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html', 'AWS Security Token Service (AWS STS) \n\nAWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (AWS IAM) users or for users that you authenticate (federated users). \n\nYou can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: \n\nTemporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. \n\nTemporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so. \n\nTemporary security credentials are generated by AWS Security Token Service (AWS STS). By default, AWS STS is a global service with a single endpoint at https://sts.amazonaws.com. However, you can also choose to make AWS STS API calls to endpoints in any other supported Region.', 'AWS IAM Identity Center - AWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (AWS IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In AWS IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both.']
Question: A company needs to use a secure online data transfer tool/service that can automate the ongoing transfers from on-premises systems into AWS while providing support for incremental data backups. 

Which AWS tool/service is an optimal fit for this requirement?
Has explanation: True
Explanation: AWS DataSync 

AWS DataSync is a secure online data transfer service that simplifies, automates, and accelerates copying terabytes of data to and from AWS storage services. Easily migrate or replicate large data sets without having to build custom solutions or oversee repetitive tasks. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. 

You can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, and big data analytics in financial services. AWS DataSync provides timely delivery to ensure dependent processes are not delayed. You can specify exclude filters, include filters, or both, to determine which files, folders, or objects get transferred each time your task runs. 

AWS DataSync employs an AWS-designed transfer protocoldecoupled from the storage protocolto accelerate data movement. The protocol performs optimizations on how, when, and what data is sent over the network. Network optimizations performed by DataSync include incremental transfers, in-line compression, and sparse file detection, as well as in-line data validation and encryption. 

Data Transfer between on-premises and AWS using AWS DataSync: via - https://aws.amazon.com/datasync/
Has option_explanations: True
Option explanations: ['AWS Snowmobile - AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot-long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration.', 'AWS Storage Gateway - AWS Storage Gateway is a set of hybrid cloud services that give you on-premises access to virtually unlimited cloud storage. Customers use AWS Storage Gateway to integrate AWS Cloud storage with existing on-site workloads so they can simplify storage management and reduce costs for key hybrid cloud storage use cases. These include moving backups to the cloud, using on-premises file shares backed by cloud storage, and providing low latency access to data in AWS for on-premises applications.', 'AWS DataSync \n\nAWS DataSync is a secure online data transfer service that simplifies, automates, and accelerates copying terabytes of data to and from AWS storage services. Easily migrate or replicate large data sets without having to build custom solutions or oversee repetitive tasks. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. \n\nYou can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, and big data analytics in financial services. AWS DataSync provides timely delivery to ensure dependent processes are not delayed. You can specify exclude filters, include filters, or both, to determine which files, folders, or objects get transferred each time your task runs. \n\nAWS DataSync employs an AWS-designed transfer protocol\x97decoupled from the storage protocol\x97to accelerate data movement. The protocol performs optimizations on how, when, and what data is sent over the network. Network optimizations performed by DataSync include incremental transfers, in-line compression, and sparse file detection, as well as in-line data validation and encryption. \n\nData Transfer between on-premises and AWS using AWS DataSync: via - https://aws.amazon.com/datasync/', 'AWS Snowcone - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing, edge storage, and data transfer devices. Weighing in at 4.5 pounds (2.1 kg), AWS Snowcone is equipped with 8 terabytes of usable storage, while AWS Snowcone Solid State Drive (SSD) supports 14 terabytes of usable storage. Both referred to as AWS Snowcone, the device is ruggedized, secure, and purpose-built for use outside of a traditional data center. Its small form factor makes it a perfect fit for tight spaces or where portability is a necessity and network connectivity is unreliable. You can use AWS Snowcone in backpacks for first responders, or for IoT, vehicular, and drone use cases. You can execute compute applications at the edge, and you can ship the device with data to AWS for offline data transfer, or you can transfer data online with AWS DataSync from edge locations. \n\nReferences: \n\nhttps://aws.amazon.com/datasync/ \n\nhttps://aws.amazon.com/datasync/features/']
Question: An engineering team would like to cost-effectively run hundreds of thousands of batch computing workloads on AWS. As a Cloud Practitioner, which AWS service would you use for this task?
Has explanation: True
Explanation: AWS Batch 

AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. 

You can use AWS Batch to plan, schedule, and execute your batch computing workloads across the full range of AWS compute services. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example - memory optimized instance or CPU) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch provisions compute resources and optimize the job distribution based on the volume and resource requirements of the submitted batch jobs. 

Please review the common use cases for AWS Batch: via - https://aws.amazon.com/batch/
Has option_explanations: True
Option explanations: ['AWS Batch \n\nAWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. \n\nYou can use AWS Batch to plan, schedule, and execute your batch computing workloads across the full range of AWS compute services. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example - memory optimized instance or CPU) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch provisions compute resources and optimize the job distribution based on the volume and resource requirements of the submitted batch jobs. \n\nPlease review the common use cases for AWS Batch: via - https://aws.amazon.com/batch/', 'AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It can be used to run batch jobs but has a time limit and limited runtimes. It is usually used for smaller batch jobs.', 'Amazon Lightsail - Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project \x96 a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address \x96 for a low, predictable price. It is not used to run batch jobs.', 'AWS Fargate - AWS Fargate is a compute engine for Amazon Elastic Container Service (Amazon ECS) that allows you to run containers without having to manage servers or clusters. You can run batch jobs on AWS Fargate, but it is more expensive than AWS Batch. \n\nReference: \n\nhttps://aws.amazon.com/batch/']
Question: A company would like to define a set of rules to manage objects cost-effectively between Amazon Simple Storage Service (Amazon S3) storage classes. As a Cloud Practitioner, which Amazon S3 feature would you use?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) Lifecycle configuration 

To manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions (define when objects transition to another storage class) and expiration actions (define when objects expire. Amazon S3 deletes expired objects on your behalf). 

In this particular use case, you would use a transition action.
Has option_explanations: True
Option explanations: ['Amazon Simple Storage Service (Amazon S3) Bucket policies - An S3 bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates. It is not used to move objects between storage classes.', 'S3 Cross-Region Replication (S3 CRR) - S3 Cross-Region Replication (S3 CRR) enables automatic, asynchronous copying of objects across Amazon S3 buckets. Cross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. It is not used to move objects between storage classes. \n\nReferences: \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html \n\nhttps://aws.amazon.com/s3/', 'Amazon S3 Transfer Acceleration (Amazon S3TA) - Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. It is not used to move objects between storage classes.', 'Amazon Simple Storage Service (Amazon S3) Lifecycle configuration \n\nTo manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions (define when objects transition to another storage class) and expiration actions (define when objects expire. Amazon S3 deletes expired objects on your behalf). \n\nIn this particular use case, you would use a transition action.']
Question: Which AWS tool/service will help you define your cloud infrastructure using popular programming languages such as Python and JavaScript?
Has explanation: True
Explanation: AWS Cloud Development Kit (AWS CDK)

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.

AWS Cloud Development Kit (AWS CDK) uses the familiarity and expressive power of programming languages for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.

In short, you use the AWS CDK framework to author AWS CDK projects which are executed to generate AWS CloudFormation templates.

How Cloud Development Kit (AWS CDK) works:via - https://aws.amazon.com/cdk/
Has option_explanations: True
Option explanations: ['AWS CodeBuild - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With AWS CodeBuild, you don\x92t need to provision, manage, and scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue. \n\nReference: \n\nhttps://aws.amazon.com/cdk/', 'AWS CloudFormation - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources, and provision and manage them in an orderly and predictable fashion. AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictably, and safely while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions. AWS Cloud Development Kit (AWS CDK) helps code the same in higher-level languages and converts them into AWS CloudFormation templates.', "AWS Cloud Development Kit (AWS CDK)\n\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.\n\nAWS Cloud Development Kit (AWS CDK) uses the familiarity and expressive power of programming languages for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.\n\nIn short, you use the AWS CDK framework to author AWS CDK projects which are executed to generate AWS CloudFormation templates.\n\nHow Cloud Development Kit (AWS CDK) works:via - https://aws.amazon.com/cdk/", 'AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, etc. You can simply upload your code in a programming language of your choice and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring.']
Question: A company would like to separate cost for AWS services by the department for cost allocation. Which of the following is the simplest way to achieve this task?
Has explanation: True
Explanation: Create tags for each department 

You can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria. 

Typically, you use business tags such as cost center/business unit, customer, or project to associate AWS costs with traditional cost-allocation dimensions. But a cost allocation report can include any tag. This lets you associate costs with technical or security dimensions, such as specific applications, environments, or compliance programs. 

Example of tagging for cost optimization: via - https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html
Has option_explanations: True
Option explanations: ['Create tags for each department \n\nYou can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria. \n\nTypically, you use business tags such as cost center/business unit, customer, or project to associate AWS costs with traditional cost-allocation dimensions. But a cost allocation report can include any tag. This lets you associate costs with technical or security dimensions, such as specific applications, environments, or compliance programs. \n\nExample of tagging for cost optimization: via - https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html', 'Create one account for all departments and share this account - Sharing accounts is not a security best practice, and is not recommended.', 'Create different virtual private cloud (VPCs) for different departments - Creating different VPCs will not help with separating costs. \n\nReference: \n\nhttps://docs.aws.amazon.com/general/latest/gr/aws_tagging.html', 'Create different accounts for different departments - Users can belong to several departments. Therefore, having different accounts for different departments would imply some users having several accounts. This is contrary to the security best practice: one physical user = one account. Also, it is much simpler to set up tags for tracking costs for each department.']
Question: An engineering team is new to the AWS Cloud and it would like to launch a dev/test environment with low monthly pricing. Which AWS service can address this use case?
Has explanation: True
Explanation: Amazon LightSail 

Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project  a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address  for a low, predictable price. 

It is great for people with little cloud experience to launch quickly a popular IT solution ready to use immediately.
Has option_explanations: True
Option explanations: ['AWS CloudFormation - AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. Using AWS CloudFormation requires experience as resources are deployed within a virtual private cloud (VPC).', 'Amazon Elastic Compute Cloud (Amazon EC2) - Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Deploying a dev/test environment with Amazon EC2 requires experience as instances are deployed within a virtual private cloud (VPC).', 'Amazon LightSail \n\nAmazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project \x96 a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address \x96 for a low, predictable price. \n\nIt is great for people with little cloud experience to launch quickly a popular IT solution ready to use immediately.', 'Amazon Elastic Container Service (Amazon ECS) - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS. Amazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines. Using Amazon ECS requires experience. Reference: \n\nhttps://aws.amazon.com/lightsail/']
Question: A company would like to create a private, high bandwidth network connection between its on-premises data centers and AWS Cloud. As a Cloud Practitioner, which of the following options would you recommend?
Has explanation: True
Explanation: AWS Direct Connect 

AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. 

How AWS Direct Connect works: via - https://aws.amazon.com/directconnect/
Has option_explanations: True
Option explanations: ["AWS Site-to-Site VPN - By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection and configuring routing to pass traffic through the connection. It uses the public internet and is therefore not suited for this use case.", 'AWS Direct Connect \n\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. \n\nHow AWS Direct Connect works: via - https://aws.amazon.com/directconnect/', 'VPC Endpoints - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. It does not connect your on-premises data centers and AWS Cloud.', 'VPC peering connection - A VPC peering connection is a networking connection between two virtual private clouds (VPCs) that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. It is used to connect virtual private clouds (VPCs) together, and not on-premises data centers and AWS Cloud. \n\nReference: \n\nhttps://aws.amazon.com/directconnect/']
Question: A media company wants to enable customized content suggestions for the users of its movie streaming platform. Which AWS service can provide these personalized recommendations based on historic data?
Has explanation: True
Explanation: Amazon Personalize 

Amazon Personalize enables developers to build applications with the same machine learning (ML) technology used by Amazon.com for real-time personalized recommendations. Amazon Personalize can be used to personalize the end-user experience over any digital channel. Examples include product recommendations for e-commerce, news articles and content recommendation for publishing, media, and social networks, hotel recommendations for travel websites, credit card recommendations for banks, and match recommendations for dating sites. These recommendations and personalized experiences can be delivered over websites, mobile apps, or email/messaging. Amazon Personalize can also be used to customize the user experience when user interaction is over a physical channel, e.g., a meal delivery company could personalize weekly meals to users in a subscription plan. 

Amazon Personalize supports the following key use cases: 

Personalized recommendations 
Similar items 
Personalized reranking i.e. rerank a list of items for a user 
Personalized promotions/notifications
Has option_explanations: True
Option explanations: ['Amazon Personalize \n\nAmazon Personalize enables developers to build applications with the same machine learning (ML) technology used by Amazon.com for real-time personalized recommendations. Amazon Personalize can be used to personalize the end-user experience over any digital channel. Examples include product recommendations for e-commerce, news articles and content recommendation for publishing, media, and social networks, hotel recommendations for travel websites, credit card recommendations for banks, and match recommendations for dating sites. These recommendations and personalized experiences can be delivered over websites, mobile apps, or email/messaging. Amazon Personalize can also be used to customize the user experience when user interaction is over a physical channel, e.g., a meal delivery company could personalize weekly meals to users in a subscription plan. \n\nAmazon Personalize supports the following key use cases: \n\nPersonalized recommendations \nSimilar items \nPersonalized reranking i.e. rerank a list of items for a user \nPersonalized promotions/notifications', 'Amazon SageMaker - Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models.', 'Amazon Customize - There is no such service as Amazon Customize. This option has been added as a distractor.', 'Amazon Comprehend - Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover information in unstructured data. Instead of combing through documents, the process is simplified and unseen information is easier to understand. \n\nThe service can identify critical elements in data, including references to language, people, and places, and the text files can be categorized by relevant topics. In real-time, you can automatically and accurately detect customer sentiment in your content. \n\nReference: \n\nhttps://aws.amazon.com/personalize/']
Question: Which of the following options is NOT a feature of Amazon Inspector?
Has explanation: True
Explanation: Track configuration changes

Tracking configuration changes is a feature of AWS Config.

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.

How AWS Config works: via - https://aws.amazon.com/config/
Has option_explanations: True
Option explanations: ['Inspect running operating systems (OS) against known vulnerabilities \n\nThese options are all features of Amazon Inspector. \n\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. \n\nAmazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. \n\nAmazon Inspector also offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess. The agent monitors the behavior of the EC2 instances, including network, file system, and process activity. It also collects a wide set of behavior and configuration data (telemetry). \n\nReferences: \n\nhttps://aws.amazon.com/config/ \n\nhttps://aws.amazon.com/inspector/', 'Track configuration changes\n\nTracking configuration changes is a feature of AWS Config.\n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\n\nHow AWS Config works: via - https://aws.amazon.com/config/', 'Analyze against unintended network accessibility', 'Automate security assessments']
Question: Which of the following statements is the MOST accurate when describing AWS Elastic Beanstalk?
Has explanation: True
Explanation: It is a Platform as a Service (PaaS) that allows you to deploy and scale web applications and services 

AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their applications, and AWS Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. 

It is a Platform as a Service (PaaS) as you only manage the applications and the data. 

Please review this overview of the types of Cloud Computing: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['It is an Infrastructure as Code (IaC) that allows you to model and provision resources needed for an application - This is the definition of AWS CloudFormation. AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application.', 'It is an Infrastructure as a Service (IaaS) that allows you to deploy and scale web applications and services - AWS Elastic Beanstalk allows you to deploy and scale web applications and services, but it is not an Infrastructure as a Service (IaaS). With AWS Elastic Beanstalk, you do not manage the runtime, the middleware, and the operating system. \n\nReference: \n\nhttps://aws.amazon.com/elasticbeanstalk/', 'It is a Platform as a Service (PaaS) that allows you to model and provision resources needed for an application - AWS Elastic Beanstalk is a Platform as a Service (PaaS). However, the service that allows you to model and provision resources needed for an application is AWS CloudFormation.', 'It is a Platform as a Service (PaaS) that allows you to deploy and scale web applications and services \n\nAWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their applications, and AWS Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. \n\nIt is a Platform as a Service (PaaS) as you only manage the applications and the data. \n\nPlease review this overview of the types of Cloud Computing: via - https://aws.amazon.com/types-of-cloud-computing/']
Question: A start-up would like to quickly deploy a popular technology on AWS. As a Cloud Practitioner, which AWS tool would you use for this task?
Has explanation: True
Explanation: AWS Partner Solutions (formerly Quick Starts) 

AWS Partner Solutions are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes. 

AWS Partner Solutions are automated reference deployments for key workloads on the AWS Cloud. Each Partner Solution launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Has option_explanations: True
Option explanations: ['AWS Whitepapers - AWS Whitepapers are technical content authored by AWS and the AWS community to expand your knowledge of the cloud. They include technical whitepapers, technical guides, reference material, and reference architecture diagrams. You can find useful content for your deployment, but it is not a service that will deploy technologies. \n\nReference: \n\nhttps://aws.amazon.com/quickstart/', 'AWS CodeDeploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. It is not suited to rapidly deploy popular technologies on AWS ready to be used immediately.', 'AWS Partner Solutions (formerly Quick Starts) \n\nAWS Partner Solutions are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes. \n\nAWS Partner Solutions are automated reference deployments for key workloads on the AWS Cloud. Each Partner Solution launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.', 'AWS Forums - AWS Forums is an AWS community platform where people can help each other. It is not used to deploy technologies on AWS.']
Question: A company is planning to implement Chaos Engineering to expose any blind spots that can disrupt the resiliency of the application. 

Which AWS service will help implement this requirement with the least effort? 


Has explanation: True
Explanation: AWS Fault Injection Simulator (AWS FIS) 

AWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an applications performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as a sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, and monitor blind spots, and performance bottlenecks that are difficult to find in distributed systems. 

AWS Fault Injection Simulator (AWS FIS) simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With AWS Fault Injection Simulator (AWS FIS), teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Simulator (AWS FIS) provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real-world conditions necessary to find hidden weaknesses.
Has option_explanations: True
Option explanations: ['AWS Fault Injection Simulator (AWS FIS) \n\nAWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application\x92s performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as a sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, and monitor blind spots, and performance bottlenecks that are difficult to find in distributed systems. \n\nAWS Fault Injection Simulator (AWS FIS) simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With AWS Fault Injection Simulator (AWS FIS), teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Simulator (AWS FIS) provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real-world conditions necessary to find hidden weaknesses.', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity.', 'AWS Trusted Advisor - AWS Trusted Advisors provides recommendations that help you follow AWS best practices. AWS Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. You can then follow the check recommendations to optimize your services and resources.', 'Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. \n\nReference: \n\nhttps://aws.amazon.com/fis/features/']
Question: A brand-new startup would like to remove its need to manage the underlying infrastructure and focus on the deployment and management of its applications. Which type of cloud computing does this refer to?
Has explanation: True
Explanation: Platform as a Service (PaaS) 

Cloud Computing can be broadly divided into three types - Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS). 

Platform as a Service (PaaS) removes the need to manage underlying infrastructure (usually hardware and operating systems) and allows you to focus on the deployment and management of your applications. You dont need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application. 

Please review this overview of the types of cloud computing: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['On-premises - When an enterprise opts for on-premises, it needs to create, upgrade, and scale the on-premise IT infrastructure by investing in sophisticated hardware, compatible software, and robust services. Also, the business needs to deploy dedicated IT staff to upkeep, scale, and manage the on-premise infrastructure continuously. \n\nReference: \n\nhttps://aws.amazon.com/types-of-cloud-computing/', 'Software as a Service (SaaS) - Software as a Service (SaaS) provides you with a complete product that is run and managed by the service provider. With a Software as a Service (SaaS) offering, you don\x92t have to think about how the service is maintained or how the underlying infrastructure is managed. You only need to think about how you will use that particular software. Amazon Rekognition is an example of a SaaS service.', 'Platform as a Service (PaaS) \n\nCloud Computing can be broadly divided into three types - Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS). \n\nPlatform as a Service (PaaS) removes the need to manage underlying infrastructure (usually hardware and operating systems) and allows you to focus on the deployment and management of your applications. You don\x92t need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application. \n\nPlease review this overview of the types of cloud computing: via - https://aws.amazon.com/types-of-cloud-computing/', 'Infrastructure as a Service (IaaS) - Infrastructure as a Service (IaaS) contains the basic building blocks for cloud IT. It typically provides access to networking features, computers (virtual or on dedicated hardware), and data storage space. Infrastructure as a Service (IaaS) gives the highest level of flexibility and management control over IT resources.']
Question: The IT infrastructure at a university is deployed on AWS Cloud and it's experiencing a read-intensive workload. As a Cloud Practitioner, which AWS service would you use to take the load off databases?
Has explanation: True
Explanation: Amazon ElastiCache 

Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. 

If Amazon EC2 instances are intensively reading data from a database, ElastiCache can cache some values to take the load off the database. 

How Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/))
Has option_explanations: True
Option explanations: ['Amazon EMR - Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. It cannot be used to take the load off the databases. \n\nReference: \n\nhttps://aws.amazon.com/elasticache/', 'Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security, and compatibility they need. It cannot be used to take the load off databases. However, Amazon ElastiCache is often used with Amazon RDS to take the load off RDS.', 'Amazon ElastiCache \n\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. \n\nIf Amazon EC2 instances are intensively reading data from a database, ElastiCache can cache some values to take the load off the database. \n\nHow Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/))', 'AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. It cannot be used to take the load off the databases.']
Question: Which security control tool can be used to deny traffic from a specific IP address?
Has explanation: True
Explanation: Network Access Control List (network ACL) 

A Network Access Control List (network ACL) is an optional layer of security for your virtual private cloud (VPC) that acts as a firewall for controlling traffic in and out of one or more subnets (i.e. it works at the subnet level). A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. 

Network Access Control List (network ACL) Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html
Has option_explanations: True
Option explanations: ['Security Group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not at the subnet level. You can specify allow rules, but not deny rules. You can specify separate rules for inbound and outbound traffic.', "VPC Flow Logs - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon Simple Storage Service (Amazon S3). After you've created a flow log, you can retrieve and view its data in the chosen destination. However, it cannot deny traffic from a specific IP address. \n\nReference: \n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html", 'Network Access Control List (network ACL) \n\nA Network Access Control List (network ACL) is an optional layer of security for your virtual private cloud (VPC) that acts as a firewall for controlling traffic in and out of one or more subnets (i.e. it works at the subnet level). A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. \n\nNetwork Access Control List (network ACL) Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html', 'Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. Amazon GuardDuty also detects potentially compromised instances or reconnaissance by attackers. It cannot deny traffic from a specific IP address. \n\n']
Question: A company would like to reserve Amazon Elastic Compute Cloud (Amazon EC2) compute capacity for three years to reduce costs. The company also plans to increase their workloads during this period. As a Cloud Practitioner, which Amazon Elastic Compute Cloud (Amazon EC2) reserved instance (RI) type would you recommend?
Has explanation: True
Explanation: Convertible reserved instance (RI)

Purchase convertible reserved instance (RI) if you need additional flexibility, such as the ability to use different instance families, operating systems, or tenancies over the reserved instance (RI) term. Convertible reserved instance (RI) provides you with a significant discount (up to 54%) compared to an on-demand instance and can be purchased for a 1-year or 3-year term.

Convertible reserved instance (RI) can be useful when workloads are likely to change. In this case, a convertible reserved instance (RI) enables you to adapt as needs evolve while still obtaining discounts and capacity reservation.

Amazon EC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/
Has option_explanations: True
Option explanations: ['Convertible reserved instance (RI)\n\nPurchase convertible reserved instance (RI) if you need additional flexibility, such as the ability to use different instance families, operating systems, or tenancies over the reserved instance (RI) term. Convertible reserved instance (RI) provides you with a significant discount (up to 54%) compared to an on-demand instance and can be purchased for a 1-year or 3-year term.\n\nConvertible reserved instance (RI) can be useful when workloads are likely to change. In this case, a convertible reserved instance (RI) enables you to adapt as needs evolve while still obtaining discounts and capacity reservation.\n\nAmazon EC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/', 'Standard reserved instance (RI) - Standard reserved instance (RI) provides you with a significant discount (up to 72%) compared to on-demand instance pricing, and can be purchased for a 1-year or 3-year term. Standard reserved instance (RI) do not offer as much flexibility as convertible reserved instance (RI), such as not being able to change the instance family type; and therefore are not best-suited for this use case. \n\nReview the differences between standard reserved instance (RI) and convertible reserved instance (RI): https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/standard-vs.-convertible-offering-classes.html', 'Scheduled reserved instance (RI) - AWS does not support scheduled reserved instance (RI), so this option is ruled out.', 'Adaptable reserved instances (RI) - Adaptable reserved instance (RI) is not a valid type of reserved instance (RI). It is a distractor. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/pricing/']
Question: Which of the following statements is INCORRECT regarding Amazon EBS Elastic Volumes?
Has explanation: True
Explanation: Amazon EBS Elastic Volumes can be bound to several Availability Zones (AZs) 

An Amazon EBS Elastic Volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. 

When using Amazon EBS Elastic Volumes, the volume, and the instance must be in the same Availability Zone (AZ).
Has option_explanations: True
Option explanations: ['Amazon EBS Elastic Volumes are bound to a specific Availability Zone (AZ) - As mentioned, when using Amazon EBS Elastic Volumes, the volume and the instance must be in the same Availability Zone(AZ).', 'Amazon EBS Elastic Volumes can persist data after their termination - Unlike an Amazon EC2 instance store, an Amazon EBS Elastic Volume is off-instance storage that can persist independently from the life of an instance. \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html', 'Amazon EBS Elastic Volumes can be mounted to one instance at a time - At the Certified Cloud Practitioner level, Amazon EBS Elastic Volumes can be mounted to one instance at a time. It is also possible that an Amazon EBS Elastic Volume is not mounted to an instance.', 'Amazon EBS Elastic Volumes can be bound to several Availability Zones (AZs) \n\nAn Amazon EBS Elastic Volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. \n\nWhen using Amazon EBS Elastic Volumes, the volume, and the instance must be in the same Availability Zone (AZ).']
Question: According to the AWS Well-Architected Framework, which of the following action is recommended in the Security pillar?
Has explanation: True
Explanation: Use AWS Key Management Service (AWS KMS) to encrypt data 

The Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. 

Encrypting data is part of the design principle "Protect data in transit and at rest": Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate. 

AWS Key Management Service (AWS KMS) makes it easy for you to create and control keys used for encryption. It is a key service of the Security pillar. 

The AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. 

The AWS Well-Architected Framework is based on six pillars  Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. 

Overview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/
Has option_explanations: True
Option explanations: ['Use AWS Key Management Service (AWS KMS) to encrypt data \n\nThe Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. \n\nEncrypting data is part of the design principle "Protect data in transit and at rest": Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate. \n\nAWS Key Management Service (AWS KMS) makes it easy for you to create and control keys used for encryption. It is a key service of the Security pillar. \n\nThe AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. \n\nThe AWS Well-Architected Framework is based on six pillars \x97 Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. \n\nOverview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/', 'Use Amazon CloudWatch to measure overall efficiency - Amazon CloudWatch is a monitoring and management service built for developers, system operators, site reliability engineers (SRE), and IT managers. Using Amazon CloudWatch to measure overall efficiency relates more to the Reliability pillar.', 'Use AWS Cost Explorer to view and track your usage in detail - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Using Cost Explorer to view and track your usage in detail relates more to the Cost Optimization pillar.', 'Use AWS CloudFormation to automate security best practices - AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. It is not used to automate security best practices. If you want to automate security best practices, you should use Amazon Inspector. \n\nReference: \n\nhttps://aws.amazon.com/architecture/well-architected/ \n\n']
Question: Which of the following statements is CORRECT regarding the scope of an Amazon Virtual Private Cloud (VPC)?
Has explanation: True
Explanation: A VPC spans all Availability Zones (AZs) within an AWS region 

Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. 

An Amazon VPC spans all Availability Zones (AZs) within a region.
Has option_explanations: True
Option explanations: ['A VPC spans all Availability Zones (AZs) within an AWS region \n\nAmazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. \n\nAn Amazon VPC spans all Availability Zones (AZs) within a region.', 'A VPC spans all Availability Zones (AZs) in all AWS regions - A VPC is located within an AWS region.', 'A VPC spans all AWS regions within an Availability Zone (AZ) - AWS has the concept of a Region, which is a physical location around the world where AWS clusters data centers. Each AWS Region consists of multiple (two or more), isolated, and physically separate Availability Zone (AZs) within a geographic area. An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. Therefore, regions cannot be within an Availability Zone. Moreover, a VPC is located within a region. \n\nAWS Regions and Availability Zones (AZs) Overview: via - https://aws.amazon.com/about-aws/global-infrastructure/regions_az/ \n\nReference: \n\nhttps://aws.amazon.com/vpc/', 'Amazon VPC spans all subnets in all AWS regions - A VPC is located within an AWS region.']
Question: A start-up would like to monitor its cost on the AWS Cloud and would like to choose an optimal Savings Plan. As a Cloud Practitioner, which AWS service would you use?
Has explanation: True
Explanation: AWS Cost Explorer 

AWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis and empowers you to dive deeper using several filtering dimensions (e.g., AWS Service, AWS Region, Linked Account, etc.). AWS Cost Explorer also gives you access to a set of default reports to help you get started, while also allowing you to create custom reports from scratch. 

Customers can receive Savings Plan recommendations at the member (linked) account level in addition to the existing AWS organization-level recommendations in AWS Cost Explorer.
Has option_explanations: True
Option explanations: ['AWS Pricing Calculator - AWS Pricing Calculator lets you explore AWS services, and create an estimate for the cost of your use cases on AWS. It does not provide Savings Plan recommendations.', 'AWS Cost Explorer \n\nAWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis and empowers you to dive deeper using several filtering dimensions (e.g., AWS Service, AWS Region, Linked Account, etc.). AWS Cost Explorer also gives you access to a set of default reports to help you get started, while also allowing you to create custom reports from scratch. \n\nCustomers can receive Savings Plan recommendations at the member (linked) account level in addition to the existing AWS organization-level recommendations in AWS Cost Explorer.', 'AWS Budgets - AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reserved instance (RI) utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. It does not provide Savings Plan recommendations.', 'AWS Cost & Usage Report (AWS CUR) - The AWS Cost & Usage Report (AWS CUR) is a single location for accessing comprehensive information about your AWS costs and usage. It does not provide Savings Plan recommendations.']
Question: Which AWS service can inspect Amazon CloudFront distributions running on any HTTP web server?
Has explanation: True
Explanation: AWS Web Application Firewall (AWS WAF)

AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting (XSS).

AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront and lets you control access to your content.

When you use the AWS web application firewall (AWS WAF) on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesnt come at the expense of performance. Blocked requests are stopped before they reach your web servers.

How AWS WAF works: via - https://aws.amazon.com/waf/
Has option_explanations: True
Option explanations: ['Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances.', 'AWS Web Application Firewall (AWS WAF)\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting (XSS).\n\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront and lets you control access to your content.\n\nWhen you use the AWS web application firewall (AWS WAF) on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn\x92t come at the expense of performance. Blocked requests are stopped before they reach your web servers.\n\nHow AWS WAF works: via - https://aws.amazon.com/waf/', 'Elastic Load Balancing (ELB) - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It does not inspect Amazon CloudFront distributions. \n\nReference: \n\nhttps://aws.amazon.com/waf/', 'AWS GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It does not inspect Amazon CloudFront distributions.']
Question: Which Amazon Elastic Compute Cloud (Amazon EC2) Auto Scaling feature can help with fault tolerance?
Has explanation: True
Explanation: Replacing unhealthy Amazon EC2 instances 

Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove Amazon EC2 instances according to the conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. 

Amazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and replace it with a new one.
Has option_explanations: True
Option explanations: ['Lower cost by adjusting the number of Amazon EC2 instances - Amazon EC2 Auto Scaling adds instances only when needed, and can scale across purchase options to optimize performance and cost. However, this will not help with fault tolerance.', 'Distributing load to Amazon EC2 instances - Even though this helps with fault tolerance and is often used with Amazon EC2 Auto Scaling, it is a feature of Elastic Load Balancing (ELB) and not an Amazon EC2 Auto Scaling. Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs).', 'Having the right amount of computing capacity - Amazon EC2 Auto Scaling ensures that your application always has the right amount of computing capacity, so your application can handle the workload. \n\nReference: \n\nhttps://aws.amazon.com/ec2/autoscaling/', 'Replacing unhealthy Amazon EC2 instances \n\nAmazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove Amazon EC2 instances according to the conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. \n\nAmazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and replace it with a new one.']
Question: A company would like to optimize Amazon Elastic Compute Cloud (Amazon EC2) costs. Which of the following actions can help with this task? (Select TWO)
Has explanation: True
Explanation: Set up Auto Scaling groups to align the number of instances with the demand
Has option_explanations: True
Option explanations: ["Build its own servers - Building your own servers is more expensive than using EC2 instances in the cloud. You're more likely to spend more money than saving money. \n\nReferences: \n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html \n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/ \n\nhttps://wa.aws.amazon.com/wat.concept.horizontal-scaling.en.html \n\nhttps://aws.amazon.com/autoscaling/", 'Vertically scale the EC2 instances - Vertically scaling EC2 instances (increasing one computer performance by adding CPUs, memory, and storage) is limited and is way more expensive than scaling horizontally (adding more computers to the system).', 'Opt for a higher AWS Support plan - The AWS Support plans do not help with EC2 costs.', 'Set up Auto Scaling groups to align the number of instances with the demand', 'Purchase Amazon EC2 Reserved instances (RIs) \n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. You can adjust its size to meet demand, either manually or by using automatic scaling. \n\nAWS Auto Scaling can help you optimize your utilization and cost efficiencies when consuming AWS services so you only pay for the resources you need. \n\nHow AWS Auto Scaling works: via - https://aws.amazon.com/autoscaling/ \n\nAmazon EC2 Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone (AZ). \n\nEC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/']
Question: Which AWS service can be used to view the most comprehensive billing details for the past month?
Has explanation: True
Explanation: AWS Cost & Usage Report (AWS CUR)

The AWS Cost & Usage Report (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself.

AWS Cost & Usage Report (AWS CUR) Overview :via - https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html
Has option_explanations: True
Option explanations: ['AWS Cost Explorer - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost Explorer includes a default report that helps you visualize the costs and usage associated with your top five cost-accruing AWS services and gives you a detailed breakdown of all services in the table view. The reports let you adjust the time range to view historical data going back up to twelve months to gain an understanding of your cost trends. AWS Cost Explorer cannot provide granular billing details for the past month.', 'AWS Pricing Calculator - AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator cannot provide billing details for the past month.', 'AWS Cost & Usage Report (AWS CUR)\n\nThe AWS Cost & Usage Report (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself.\n\nAWS Cost & Usage Report (AWS CUR) Overview :via - https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html', 'AWS Budgets - AWS Budgets gives the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reservation utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. Budgets can be created at the monthly, quarterly, or yearly level, and you can customize the start and end dates. You can further refine your budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others. AWS Budgets cannot provide billing details for the past month.']
Question: A company based in Sydney hosts its application on an Amazon Elastic Compute Cloud (Amazon EC2) instance in ap-southeast-2. They would like to deploy the same Amazon EC2 instances in eu-south-1. Which of the following AWS entities can address this use case?
Has explanation: True
Explanation: Amazon Machine Image (AMI) 

An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an Amazon Machine Image (AMI) when you launch an instance. You can launch multiple instances from a single Amazon Machine Image (AMI) when you need multiple instances with the same configuration. 

How to use an Amazon Machine Image (AMI): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html
Has option_explanations: True
Option explanations: ['Amazon EBS Elastic Volume snapshots - An Amazon EBS snapshot is a point-in-time copy of your Amazon EBS volume. EBS snapshots are one of the components of an AMI, but EBS snapshots alone cannot be used to deploy the same EC2 instances across different Availability Zones (AZs). \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html', 'AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used to deploy the same EC2 instances across different Availability Zones (AZs).', 'Amazon Machine Image (AMI) \n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an Amazon Machine Image (AMI) when you launch an instance. You can launch multiple instances from a single Amazon Machine Image (AMI) when you need multiple instances with the same configuration. \n\nHow to use an Amazon Machine Image (AMI): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html', 'Elastic Load Balancing (ELB) - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). It cannot be used to deploy the same EC2 instances across different Availability Zones (AZs).']
Question: Which of the following billing timeframes is applied when running a Windows EC2 on-demand instance?
Has explanation: True
Explanation: Pay per second 

With On-Demand instances, you only pay for the Amazon EC2 instances you use. The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs. 

When running a Windows EC2 on-demand instance, pay-per-second pricing is applied.
Has option_explanations: True
Option explanations: ['Pay per minute - Pay per minute pricing is not available for Windows EC2 on-demand instances, or any other type of on-demand EC2 instance.', 'Pay per hour - When running an Amazon Windows EC2 On-demand instance, pay-per-second pricing is applied. Windows-based EC2 instances used to follow pay-per-hour pricing earlier.', 'Pay per day - Pay per day pricing is not available for Windows EC2 on-demand instances, or any other type of on-demand EC2 instance. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/', 'Pay per second \n\nWith On-Demand instances, you only pay for the Amazon EC2 instances you use. The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs. \n\nWhen running a Windows EC2 on-demand instance, pay-per-second pricing is applied.']
Question: Adding more CPU/RAM to an Amazon Elastic Compute Cloud (Amazon EC2) instance represents which of the following?
Has explanation: True
Explanation: Vertical scaling 

A vertically scalable system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.
Has option_explanations: True
Option explanations: ['Horizontal scaling - A horizontally scalable system is one that can increase capacity by adding more computers to the system.', 'Vertical scaling \n\nA vertically scalable system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.', "Managing increasing volumes of data - Traditional data storage and analytics tools can no longer provide the agility and flexibility required to deliver relevant business insights. That\x92s why many organizations are shifting to a data lake architecture. A data lake is an architectural approach that allows you to store massive amounts of data in a central location so that it's readily available to be categorized, processed, analyzed, and consumed by diverse groups within your organization.", 'Loose coupling - As application complexity increases, a desirable attribute of an IT system is that it can be broken into smaller, loosely coupled components. This means that IT systems should be designed in a way that reduces interdependencies\x97a change or a failure in one component should not cascade to other components. \n\nReference: \n\nhttps://wa.aws.amazon.com/wat.concept.horizontal-scaling.en.html']
Question: A company needs to keep sensitive data in its own data center due to compliance but would still like to deploy resources using AWS. Which Cloud deployment model does this refer to?
Has explanation: True
Explanation: Hybrid Cloud 

A hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. 

Overview of Cloud Computing Deployment Models: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['On-premises - This is not a cloud deployment model. When an enterprise opts for on-premises, it needs to create, upgrade, and scale the on-premise IT infrastructure by investing in sophisticated hardware, compatible software, and robust services. Also, the business needs to deploy dedicated IT staff to upkeep, scale, and manage the on-premise infrastructure continuously. \n\nReference: \n\nhttps://docs.aws.amazon.com/whitepapers/latest/public-sector-cloud-transformation/selecting-the-right-cloud-for-workloads-differences-between-public-private-and-hybrid.html', 'Private Cloud - Unlike a Public cloud, a Private cloud enables businesses to avail IT services that are provisioned and customized according to their precise needs. The business can further avail the IT services securely and reliably over a private IT infrastructure.', 'Public Cloud - A public cloud-based application is fully deployed in the cloud and all parts of the application run in the cloud. Applications in the cloud have either been created in the cloud or have been migrated from an existing infrastructure to take advantage of the benefits of cloud computing.', "Hybrid Cloud \n\nA hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. \n\nOverview of Cloud Computing Deployment Models: via - https://aws.amazon.com/types-of-cloud-computing/"]
Question: Which of the following options are the benefits of using AWS Elastic Load Balancing (ELB)? (Select TWO)
Has explanation: True
Explanation: Fault tolerance 

Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). 

Elastic Load Balancing (ELB) offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant: Application Load Balancer (best suited for HTTP and HTTPS traffic), Network Load Balancer (best suited for TCP traffic), and Classic Load Balancer.
Has option_explanations: True
Option explanations: ['Less costly - AWS Elastic Load Balancing (ELB) does not help with reducing costs', 'Agility - Agility refers to new IT resources being only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes. AWS Elastic Load Balancing (ELB) does not help with agility.', 'Fault tolerance \n\nElastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). \n\nElastic Load Balancing (ELB) offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant: Application Load Balancer (best suited for HTTP and HTTPS traffic), Network Load Balancer (best suited for TCP traffic), and Classic Load Balancer.', 'Storage - AWS Elastic Load Balancing (ELB) does not offer storage benefits. It is not a storage-related service. \n\nReference: \n\nhttps://aws.amazon.com/elasticloadbalancing/', 'High availability']
Question: Which of the following services are provided by Amazon Route 53? (Select Two)
Has explanation: True
Explanation: Health checks and monitoring 

Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. 

Amazon Route 53 offers domain name registration services, where you can search for and register available domain names or transfer in existing domain names to be managed by Route 53. 

Amazon Route 53 can monitor the health and performance of your application as well as your web servers and other resources.
Has option_explanations: True
Option explanations: ['Health checks and monitoring \n\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. \n\nAmazon Route 53 offers domain name registration services, where you can search for and register available domain names or transfer in existing domain names to be managed by Route 53. \n\nAmazon Route 53 can monitor the health and performance of your application as well as your web servers and other resources.', "Transfer acceleration - Transfer acceleration is a feature of Amazon's simple storage service (Amazon S3). Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. \n\nReference: \n\nhttps://aws.amazon.com/route53/", 'Domain registration', 'IP routing - Despite its name, Amazon Route 53 does not offer IP routing. However, it can route traffic based on multiple criteria, such as endpoint health, geographic location, and latency, using routing policies.', 'Load balancing - It is a feature of Elastic Load Balancing (ELB) and not Amazon Route 53. Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs).']
Question: According to the AWS Shared Responsibility Model, which of the following is the responsibility of the customer?
Has explanation: True
Explanation: Firewall & networking configuration of Amazon Elastic Compute Cloud (Amazon EC2) 

The customers are responsible for "Security IN the cloud". It includes the configuration of the operating system, network & firewall of applications.
Has option_explanations: True
Option explanations: ['Protecting hardware infrastructure', 'Edge locations security \n\nAWS is responsible for "Security OF the cloud". It includes the infrastructure, which is composed of the hardware, software, networking, and facilities that run AWS Cloud services. \n\nReference: \n\nhttps://aws.amazon.com/compliance/shared-responsibility-model/', 'Managing Amazon DynamoDB - Amazon DynamoDB is a fully managed service. AWS operates the infrastructure layer, the operating system, and platforms, and customers access the endpoints to store and retrieve data.', 'Firewall & networking configuration of Amazon Elastic Compute Cloud (Amazon EC2) \n\nThe customers are responsible for "Security IN the cloud". It includes the configuration of the operating system, network & firewall of applications.']
Question: According to the AWS Well-Architected Framework, which of the following statements are recommendations in the Operational Excellence pillar? (Select two)
Has explanation: True
Explanation: Anticipate failure
Has option_explanations: True
Option explanations: ['Enable traceability - Monitor, alert, and audit actions and changes to your environment in real-time. Integrate logs and metrics with systems to automatically respond and take action. It is a design principle of the Security pillar.', "Automatically recover from failure - By monitoring a system for key performance indicators (KPIs), you can trigger automation when a threshold is breached. This allows for automatic notification and tracking of failures, and for automated recovery processes that work around or repair the failure. With more sophisticated automation, it's possible to anticipate and remediate failures before they occur. It is a design principle of the Reliability pillar.", 'Anticipate failure', 'Use serverless architectures - In the cloud, serverless architectures remove the need for you to run and maintain servers to carry out traditional compute activities. For example, storage services can act as static websites, removing the need for web servers, and event services can host your code for you. This not only removes the operational burden of managing these servers but also can lower transactional costs because these managed services operate at a cloud scale. It is a design principle of the Performance Efficiency pillar. \n\nReference: \n\nhttps://wa.aws.amazon.com/index.en.html', 'Make frequent, small, reversible changes \n\nThe Operational Excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures. \n\nPerform \x93pre-mortem\x94 exercises to identify potential sources of failure so that they can be removed or mitigated. Test your failure scenarios and validate your understanding of their impact. Test your response procedures to ensure that they are effective, and that teams are familiar with their execution. Set up regular game days to test workloads and team responses to simulated events. \n\nDesign workloads to allow components to be updated regularly. Make changes in small increments that can be reversed if they fail (without affecting customers when possible). \n\nThe AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. \n\nThe AWS Well-Architected Framework is based on six pillars \x97 Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. \n\nOverview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/']
Question: A corporation would like to simplify access management to multiple AWS accounts as well as facilitate AWS Single Sign-On (AWS SSO) access to its AWS accounts. As a Cloud Practitioner, which AWS service would you use for this task?
Has explanation: True
Explanation: AWS IAM Identity Center 

AWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both. 

You can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access their assigned AWS accounts or cloud applications. 

You can use IAM Identity Center to quickly and easily assign and manage your employees access to multiple AWS accounts, SAML-enabled cloud applications (such as Salesforce, Microsoft 365, and Box), and custom-built in-house applications, all from a central place. 

How AWS IAM Identity Center works: via - https://aws.amazon.com/iam/identity-center/
Has option_explanations: True
Option explanations: ['AWS IAM Identity Center \n\nAWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both. \n\nYou can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access their assigned AWS accounts or cloud applications. \n\nYou can use IAM Identity Center to quickly and easily assign and manage your employees\x92 access to multiple AWS accounts, SAML-enabled cloud applications (such as Salesforce, Microsoft 365, and Box), and custom-built in-house applications, all from a central place. \n\nHow AWS IAM Identity Center works: via - https://aws.amazon.com/iam/identity-center/', 'AWS Command Line Interface (CLI) - The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. It is not a central user portal. \n\nReference: \n\nhttps://aws.amazon.com/iam/identity-center/', 'AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (AWS IAM) enables you to securely control access to AWS services and resources for your users. Using AWS IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. It is not used to log in but to manage users and roles.', 'AWS Cognito - Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system. It is an identity management solution for customers/developers building B2C or B2B apps for their customers.']
Question: A company would like to move 50 petabytes (PBs) of data from its on-premises data centers to AWS in the MOST cost-effective way. As a Cloud Practitioner, which of the following solutions would you choose?
Has explanation: True
Explanation: AWS Snowmobile 

AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.
Has option_explanations: True
Option explanations: ['AWS Storage Gateway - AWS Storage Gateway is a hybrid storage service that enables your on-premises applications to seamlessly use AWS cloud storage. You can use the service for backup and archiving, disaster recovery, cloud data processing, storage tiering, and migration. However, data transfer through AWS Storage Gateway takes longer even with great bandwidth. Moreover, transferring 50 PBs of data will be more expensive than using AWS Snowmobile. \n\nReference: \n\nhttps://aws.amazon.com/snowmobile/', 'AWS Snowmobile \n\nAWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.', 'AWS Snowball - AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. The use of Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with AWS Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed Internet. However, one Snowball only provides up to 80 TB of capacity. Therefore, to transfer 50 PBs, AWS Snowball is not the most cost-effective option.', 'AWS Snowball Edge - AWS Snowball Edge is an edge computing and data transfer device provided by the AWS Snowball service. It has onboard storage and compute power that provides select AWS services for use in edge locations. However, one AWS Snowball Edge only provides up to 100 TB of capacity. Therefore, to transfer 50 PBs, AWS Snowball Edge is not the most cost-effective option.']
Question: Which AWS tool can provide best practice recommendations for performance, service limits, and cost optimization?
Has explanation: True
Explanation: AWS Trusted Advisor 

AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. 

How AWS Trusted Advisor works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/
Has option_explanations: True
Option explanations: ['AWS Trusted Advisor \n\nAWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. \n\nHow AWS Trusted Advisor works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on your Amazon EC2 instances. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Nevertheless, it does not provide best practice recommendations.', 'AWS Health Dashboard - Service health - AWS Health Dashboard - Service health publishes most up-to-the-minute information on the status and availability of all AWS services in tabular form for all Regions that AWS is present in. It does not provide best practice recommendations.', 'Amazon CloudWatch - Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. Amazon CloudWatch provides data and actionable insights to monitor applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. This is an excellent service for building Resilient systems. Think resource performance monitoring, events, and alerts; think Amazon CloudWatch. Amazon CloudWatch does not provide best practice recommendations. \n\nReference: \nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor//']
Question: What is the primary use case for Amazon GuardDuty?
Has explanation: True
Explanation: Detecting malicious activity and threats in your AWS accounts and workloads 

Amazon GuardDuty is a managed threat detection service that identifies potential security risks by analyzing logs, such as AWS CloudTrail, VPC Flow Logs, and DNS query logs. It helps organizations detect suspicious or malicious activity in their AWS environments. 

Amazon GuardDuty Overview: via - https://aws.amazon.com/guardduty/
Has option_explanations: True
Option explanations: ['Protecting web applications from common exploits and vulnerabilities such as SQL injection - This is the primary use case for AWS Web Application Firewall (WAF), which protects web applications from vulnerabilities like SQL injection and cross-site scripting. GuardDuty detects threats but does not directly protect applications. \n\nReference: \n\nhttps://aws.amazon.com/guardduty/', 'Encrypting data in transit between AWS services using TLS certificates. - AWS Certificate Manager (ACM) or AWS Key Management Service (KMS) is responsible for managing encryption and TLS certificates. GuardDuty does not handle encryption but focuses on identifying potential security threats.', 'Detecting malicious activity and threats in your AWS accounts and workloads \n\nAmazon GuardDuty is a managed threat detection service that identifies potential security risks by analyzing logs, such as AWS CloudTrail, VPC Flow Logs, and DNS query logs. It helps organizations detect suspicious or malicious activity in their AWS environments. \n\nAmazon GuardDuty Overview: via - https://aws.amazon.com/guardduty/', 'Enforcing secure communication between VPCs using network traffic filtering - This is a feature provided by AWS Network Firewall or Security Groups, which allow you to control network traffic into and out of your VPCs. GuardDuty detects malicious activities but does not enforce network traffic filtering.']
Question: Which of the following AWS Identity and Access Management (AWS IAM) Security Tools allows you to review permissions granted to an IAM user?
Has explanation: True
Explanation: AWS Identity and Access Management (IAM) access advisor 

IAM Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.
Has option_explanations: True
Option explanations: ['AWS Identity and Access Management (IAM) access advisor \n\nIAM Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.', 'Multi-Factor Authentication (MFA) - Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password. with Multi-Factor Authentication (MFA) enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factor\x97what they know), as well as for an authentication code from their AWS MFA device (the second factor\x97what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources. It cannot be used to review permissions granted. \n\nReference: \n\nhttps://aws.amazon.com/about-aws/whats-new/2019/06/now-use-iam-access-advisor-with-aws-organizations-to-set-permission-guardrails-confidently/', 'IAM policy - IAM policies define permissions for an action regardless of the method that you use to perform the operation.', 'IAM credentials report - You can generate and download a credential report that lists all IAM users in your account and the status of their various credentials, including passwords, access keys, and multi-factor authentication (MFA) devices. It is not used to review permissions granted to an IAM user.']
Question: Which of the following are the advantages of using the AWS Cloud? (Select TWO)
Has explanation: True
Explanation: Stop guessing about capacity
Has option_explanations: True
Option explanations: ['Stop guessing about capacity', 'Increase speed and agility', 'Limited scaling - Scaling is not limited in the cloud. You can access as much or as little capacity as you need, and scale up and down as required with only a few minutes\x92 notice.', 'AWS is responsible for security in the cloud - AWS is responsible for the security OF the cloud, which means AWS is responsible for protecting the infrastructure that runs all the services offered in the AWS Cloud.', 'Trade operational expense for capital expense - In the cloud, you trade capital expense (CAPEX) for the operational expense (OPEX). Instead of having to invest heavily in data centers and servers before you know how you\x92re going to use them, you can pay only when you consume computing resources, and pay only for how much you consume. \n\nReference: \n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html']
Question: A company using a hybrid cloud would like to store secondary backup copies of the on-premises data. Which Amazon S3 Storage Class would you use for a cost-optimal yet rapid access solution?
Has explanation: True
Explanation: Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) 

Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. Its a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region using S3 cross-region replication (S3 CRR).
Has option_explanations: True

=== Quiz Submission Debug ===
1. Quiz Title: AWS_CCP_M5
2. Request Method: POST
3. Form Data: ImmutableMultiDict([('attempt_id', '72e34c28-4130-448c-9170-55e18c798bb4'), ('csrf_token', 'ImFlNzdhN2EzMGQwZjE3MmNhOGM0NDM1ZWVlYTk4MmEwNDg5N2RiNGQi.Z7rBTg.Twcxd4BgtFGdnGS9hZpRKOjWDaI'), ('question_f0e72cbf-7ccc-4892-90bf-8e5e7814e009[]', '2'), ('question_f0e72cbf-7ccc-4892-90bf-8e5e7814e009[]', '4'), ('question_75b576ed-2a53-4103-a987-4a0f02708667', '3')])
4. Found quiz: AWS_CCP_M5 (ID: 24c4f1db-43b4-412c-8270-1ec3e3142498)
5. Found attempt ID: 72e34c28-4130-448c-9170-55e18c798bb4
DEBUG - Processed answers (Post-Submission): {'f0e72cbf-7ccc-4892-90bf-8e5e7814e009[]': [2, 4], '75b576ed-2a53-4103-a987-4a0f02708667': [3]}
6. Processed answers: {'f0e72cbf-7ccc-4892-90bf-8e5e7814e009[]': [2, 4], '75b576ed-2a53-4103-a987-4a0f02708667': [3]}
7. Correct questions: ['75b576ed-2a53-4103-a987-4a0f02708667']
8. Successfully saved attempt

=== Quiz Submission Debug ===
1. Quiz Title: AWS_CCP_M5
2. Request Method: POST
3. Form Data: ImmutableMultiDict([('attempt_id', '72e34c28-4130-448c-9170-55e18c798bb4'), ('csrf_token', 'ImFlNzdhN2EzMGQwZjE3MmNhOGM0NDM1ZWVlYTk4MmEwNDg5N2RiNGQi.Z7rBTg.Twcxd4BgtFGdnGS9hZpRKOjWDaI'), ('question_f0e72cbf-7ccc-4892-90bf-8e5e7814e009[]', '2'), ('question_f0e72cbf-7ccc-4892-90bf-8e5e7814e009[]', '4'), ('question_75b576ed-2a53-4103-a987-4a0f02708667', '3'), ('question_bc7d5894-4715-4a54-9904-43a8952413a9', '2'), ('question_63be4fe1-e27b-4291-8211-5e8c1d021e07', '1'), ('question_46fe7088-b5cf-4096-aefd-1f41e71fe495[]', '2'), ('question_46fe7088-b5cf-4096-aefd-1f41e71fe495[]', '3')])
4. Found quiz: AWS_CCP_M5 (ID: 24c4f1db-43b4-412c-8270-1ec3e3142498)
ERROR: No active attempt found for user 190c24e1-3a4f-4c25-930a-56f7e1878dff on quiz 24c4f1db-43b4-412c-8270-1ec3e3142498

=== Quiz Result Debug ===
Question: According to the AWS Shared Responsibility Model, which of the following are the responsibilities of AWS? (Select two)
Has explanation: True
Explanation: Data center security
Has option_explanations: True
Option explanations: ['Encrypting application data - The customers are responsible for encrypting application data.', 'Configuring IAM Roles - The customers are responsible for configuring IAM Roles. \n\nReference: \n\nhttps://aws.amazon.com/compliance/shared-responsibility-model/', 'Data center security', 'Installing security patches of the guest operating system (OS) - The customers are responsible for patching their guest operating system. \n\nPlease review the IT controls under the AWS Shared Responsibility Model: via - https://aws.amazon.com/compliance/shared-responsibility-model/', 'Network operability \n\nAWS responsibility \x93Security OF the Cloud\x94 - AWS is responsible for protecting the infrastructure that runs all of the services offered in the AWS Cloud. This infrastructure is composed of the hardware, software, networking, and facilities that run AWS Cloud services.']
Question: A Cloud Practitioner would like to deploy identical resources across all AWS regions and accounts using templates while estimating costs. Which AWS service can assist with this task?
Has explanation: True
Explanation: AWS CloudFormation

AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.

You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. This provides a single source of truth for all your resources and helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting.

AWS CloudFormation templates allow you to estimate the cost of your resources.

How AWS CloudFormation works: via - https://aws.amazon.com/cloudformation/
Has option_explanations: True
Option explanations: ['AWS CodeDeploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. Unlike AWS CloudFormation, it does not deal with infrastructure configuration and orchestration. \n\nReference: \n\nhttps://aws.amazon.com/cloudformation/', 'Amazon LightSail - Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server with AWS. It is not best suited when deploying more complex resources, while AWS CloudFormation can.', 'AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) - AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD), also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. It is not used to deploy resources.', 'AWS CloudFormation\n\nAWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\n\nYou can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application. This provides a single source of truth for all your resources and helps you to standardize infrastructure components used across your organization, enabling configuration compliance and faster troubleshooting.\n\nAWS CloudFormation templates allow you to estimate the cost of your resources.\n\nHow AWS CloudFormation works: via - https://aws.amazon.com/cloudformation/']
Question: Which AWS service can be used to subscribe to an RSS feed to be notified of the status of all AWS service interruptions?
Has explanation: True
Explanation: AWS Health Dashboard - Service Health 

The AWS Health Dashboard  Service health is the single place to learn about the availability and operations of AWS services. You can view the overall status of AWS services, and you can sign in to view personalized communications about your particular AWS account or organization. 

You can check on this page https://health.aws.amazon.com/health/status to get current status information. 

The AWS Health Dashboard  Service health offers the possibility to subscribe to an RSS feed to be notified of interruptions to each service.
Has option_explanations: True
Option explanations: ["Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. It can be used to deliver notifications, but it does not provide the current services' status.", 'AWS Health Dashboard - Your Account Health - Your AWS Health Dashboard \x96 Your Account Health provides alerts and remediation guidance when AWS is experiencing events that may impact you.', 'AWS Health Dashboard - Service Health \n\nThe AWS Health Dashboard \x96 Service health is the single place to learn about the availability and operations of AWS services. You can view the overall status of AWS services, and you can sign in to view personalized communications about your particular AWS account or organization. \n\nYou can check on this page https://health.aws.amazon.com/health/status to get current status information. \n\nThe AWS Health Dashboard \x96 Service health offers the possibility to subscribe to an RSS feed to be notified of interruptions to each service.', "AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It does not provide all AWS services' status. \n\nReference: \n\nhttps://health.aws.amazon.com/health/status"]
Question: A production company would like to establish an AWS managed virtual private network (VPN) service between its on-premises network and AWS. Which item needs to be set up on the company's side?
Has explanation: True
Explanation: A customer gateway 

A customer gateway device is a physical or software appliance on your side of a Site-to-Site VPN connection. You or your network administrator must configure the device to work with the Site-to-Site VPN connection. 

You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. More on customer gateway device: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html 


Has option_explanations: True
Option explanations: ['A VPC endpoint interface - An interface VPC endpoint (interface endpoint) enables you to connect to services powered by AWS PrivateLink. It is not a component of a connection between on-premises network and AWS.', 'A virtual private gateway (VGW) - A virtual private gateway (VGW) device is a physical or software appliance on AWS side of a Site-to-Site VPN connection. \n\nReferences: \n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html \n\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html', 'A security group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. It is not a component of a connection between on-premises network and AWS.', 'A customer gateway \n\nA customer gateway device is a physical or software appliance on your side of a Site-to-Site VPN connection. You or your network administrator must configure the device to work with the Site-to-Site VPN connection. \n\nYou can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. More on customer gateway device: via - https://docs.aws.amazon.com/vpn/latest/s2svpn/your-cgw.html \n\n']
Question: Which types of monitoring can be provided by Amazon CloudWatch? (Select TWO)
Has explanation: True
Explanation: Resource utilization 

Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon DynamoDB tables, and Amazon Amazon Relational Database Service (Amazon RDS) DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. 

You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. 

How Amazon CloudWatch works: via - https://aws.amazon.com/cloudwatch/
Has option_explanations: True
Option explanations: ['API access - Recording API calls is a feature of AWS CloudTrail, not Amazon CloudWatch.', 'Performance and availability of AWS services - The AWS Health - Your Account Health Dashboard gives you a personalized view of the performance and availability of the AWS services underlying your AWS resources, not Amazon CloudWatch.', 'Resource utilization \n\nAmazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. Amazon CloudWatch can monitor AWS resources such as Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon DynamoDB tables, and Amazon Amazon Relational Database Service (Amazon RDS) DB instances, as well as custom metrics generated by your applications and services, and any log files your applications generate. \n\nYou can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and operational health. You can use these insights to react and keep your application running smoothly. \n\nHow Amazon CloudWatch works: via - https://aws.amazon.com/cloudwatch/', 'Application performance', 'Account management - AWS Identity and Access Management (AWS IAM) is usually used to manage accounts, not Amazon CloudWatch.\n\nReferences:\n\nhttps://aws.amazon.com/cloudwatch/features/\n\nhttps://aws.amazon.com/cloudwatch/']
Question: Which of the following are the best practices when using AWS Organizations? (Select TWO)
Has explanation: True
Explanation: Create AWS accounts per department
Has option_explanations: True
Option explanations: ['Do not use AWS Organizations to automate AWS account creation - AWS Organizations helps you simplify IT operations by automating AWS account creation and management. The AWS Organizations APIs enable you to create new accounts programmatically and to add new accounts to a group. The policies attached to the group are automatically applied to the new account. \n\nReference: \n\nhttps://aws.amazon.com/organizations/', 'Never use tags for billing - You should use tags standards to categorize AWS resources for billing purposes.', 'Disable AWS CloudTrail on several accounts - You should enable AWS CloudTrail to monitor activity on all accounts for governance, compliance, risk, and auditing purposes.', 'Create AWS accounts per department', 'Restrict account privileges using Service Control Policies (SCP) \n\nAWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Whether you are a growing startup or a large enterprise, AWS Organizations help you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts. \n\nUsing AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use AWS Organizations to define central configurations and resource sharing across accounts in your organization. AWS Organizations is available to all AWS customers at no additional charge. \n\nYou should create accounts per department based on regulatory restrictions (using Service Control Policies (SCP)) for better resource isolation, and to have separate per-account service limits. \n\nAWS Organizations allows you to restrict what services and actions are allowed in your accounts. You can use the Service Control Policies (SCP) to apply permission guardrails on AWS Identity and Access Management (IAM) users and roles.']
Question: The development team at a company manages 300 microservices and it is now trying to automate the code reviews to improve the code quality. Which tool/service is the right fit for this requirement?
Has explanation: True
Explanation: Amazon CodeGuru 

Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an applications most expensive lines of code. Integrate Amazon CodeGuru into your existing software development workflow to automate code reviews during application development, continuously monitor application performance in production, provide recommendations and visual clues for improving code quality and application performance, and reduce overall cost. 

Amazon CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. 

Amazon CodeGuru Profiler pinpoints an applications most expensive lines of code by helping developers understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. 

How Amazon CodeGuru works: via - https://aws.amazon.com/codeguru/
Has option_explanations: True
Option explanations: ['AWS CodeBuild - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With AWS CodeBuild, you don\x92t need to provision, manage, and scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.', 'AWS X-Ray - AWS X-Ray helps developers analyze and debug production, and distributed applications, such as those built using a microservices architecture. With AWS X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\x92s underlying components.', 'Amazon CodeGuru \n\nAmazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application\x92s most expensive lines of code. Integrate Amazon CodeGuru into your existing software development workflow to automate code reviews during application development, continuously monitor application performance in production, provide recommendations and visual clues for improving code quality and application performance, and reduce overall cost. \n\nAmazon CodeGuru Reviewer uses machine learning and automated reasoning to identify critical issues, security vulnerabilities, and hard-to-find bugs during application development and provides recommendations to improve code quality. \n\nAmazon CodeGuru Profiler pinpoints an application\x92s most expensive lines of code by helping developers understand the runtime behavior of their applications, identify and remove code inefficiencies, improve performance, and significantly decrease compute costs. \n\nHow Amazon CodeGuru works: via - https://aws.amazon.com/codeguru/', 'AWS Trusted Advisor - AWS Trusted Advisors provides recommendations that help you follow AWS best practices. AWS Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. \n\nReference: \n\nhttps://aws.amazon.com/codeguru/']
Question: Which of the following AWS Support plans is the MOST cost-effective when getting enhanced technical support by Cloud Support Engineers?
Has explanation: True
Explanation: AWS Business Support 

AWS recommends AWS Business Support if you have production workloads on AWS and want 24x7 phone, email, and chat access to technical support and architectural guidance in the context of your specific use cases. You get full access to AWS Trusted Advisor Best Practice Checks. It is also the cheapest support plan to provide enhanced technical support by Cloud Support Engineers.
Has option_explanations: True
Option explanations: ['AWS Developer Support - AWS recommends AWS Developer Support if you are testing or doing early development on AWS and want the ability to get technical support during business hours as well as general architectural guidance as you build and test. It provides enhanced technical support by Cloud Support Associates.', 'AWS Basic Support - The AWS Basic Support plan is included for all AWS customers. It does not provide enhanced technical support.', 'AWS Business Support \n\nAWS recommends AWS Business Support if you have production workloads on AWS and want 24x7 phone, email, and chat access to technical support and architectural guidance in the context of your specific use cases. You get full access to AWS Trusted Advisor Best Practice Checks. It is also the cheapest support plan to provide enhanced technical support by Cloud Support Engineers.', 'AWS Enterprise Support - AWS Enterprise Support provides customers with concierge-like service where the main focus is helping the customer achieve their outcomes and find success in the cloud. With Enterprise Support, you get 24x7 technical support from high-quality engineers, tools, and technology to automatically manage the health of your environment, consultative architectural guidance delivered in the context of your applications and use cases, and a designated Technical Account Manager (TAM) to coordinate access to proactive/preventative programs and AWS subject matter experts. It provides enhanced technical support by Cloud Support Engineers but is more expensive than the Business support plan. \n\nReferences: \n\nhttps://aws.amazon.com/premiumsupport/plans/ \n\nhttps://aws.amazon.com/premiumsupport/plans/business/']
Question: Which AWS service allows you to quickly and easily add user sign-up, sign-in, and access control to web and mobile applications?
Has explanation: True
Explanation: Amazon Cognito 

Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system.
Has option_explanations: True
Option explanations: ['AWS Organizations - AWS Organizations offers policy-based management for multiple AWS accounts. With AWS Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. Organizations enable you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It does not provide user sign-up, sign-in, and access control to web and mobile applications. \n\nReference: \n\nhttps://aws.amazon.com/cognito/', 'AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (AWS IAM) enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. It does not provide user sign-up, sign-in, and access control to web and mobile applications.', 'Amazon Cognito \n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system.', 'AWS IAM Identity Center - AWS IAM Identity Center is the successor to AWS Single Sign-On. It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create, or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both.']
Question: Which of the following AWS services can be used to generate, use, and manage encryption keys on the AWS Cloud?
Has explanation: True
Explanation: AWS CloudHSM 

The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. 

AWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only to you. 

How AWS CloudHSM works: via - https://aws.amazon.com/cloudhsm/
Has option_explanations: True
Option explanations: ['AWS GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It cannot be used to generate, use, and manage encryption keys.', 'AWS CloudHSM \n\nThe AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) instances within the AWS cloud. \n\nAWS CloudHSM allows you to securely generate, store, and manage cryptographic keys used for data encryption in a way that keys are accessible only to you. \n\nHow AWS CloudHSM works: via - https://aws.amazon.com/cloudhsm/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. It cannot be used to generate, use, and manage encryption keys.', 'AWS Secrets Manager - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It is integrated with AWS CloudHSM to generate, use, and manage encryption keys. \n\nReference: \n\nhttps://aws.amazon.com/cloudhsm/']
Question: A company would like to audit requests made to an Amazon Simple Storage Service (Amazon S3) bucket. As a Cloud Practitioner, which Amazon Simple Storage Service (Amazon S3) feature would you recommend addressing this use-case?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) Access Logs 

Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. 

It can also help you learn about your customer base and understand your Amazon S3 bill.
Has option_explanations: True
Option explanations: ['Amazon S3 Bucket Policies - Amazon S3 Bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates. It does not help with auditing requests made to your bucket.', 'S3 cross-region replication (S3 CRR) - S3 cross-region replication (S3 CRR) enables automatic, asynchronous copying of objects across Amazon S3 buckets. Cross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. It does not help with auditing requests made to your bucket.', 'Amazon Simple Storage Service (Amazon S3) Access Logs \n\nServer access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. \n\nIt can also help you learn about your customer base and understand your Amazon S3 bill.', 'S3 Versioning - Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures. It does not help with auditing requests made to your bucket. \n\nReference: \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html']
Question: A company would like to move its infrastructure to AWS Cloud. Which of the following should be included in the Total Cost of Ownership (TCO) estimate? (Select TWO)
Has explanation: True
Explanation: Power/Cooling 

AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator can be accessed at https://calculator.aws/#/. 

AWS Pricing Calculator compares the cost of your applications in an on-premises or traditional hosting environment to AWS: server, storage, network, and IT labor. Therefore, you need to include every element relevant to these points of comparison. 

Server administration is included in the IT labor costs. 

Power/Cooling are included in the server, storage, and network cost.
Has option_explanations: True
Option explanations: ['Power/Cooling \n\nAWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator can be accessed at https://calculator.aws/#/. \n\nAWS Pricing Calculator compares the cost of your applications in an on-premises or traditional hosting environment to AWS: server, storage, network, and IT labor. Therefore, you need to include every element relevant to these points of comparison. \n\nServer administration is included in the IT labor costs. \n\nPower/Cooling are included in the server, storage, and network cost.', 'Electronic equipment at office - The electronic equipment at the office is not relevant for a Total Cost of Ownership (TCO) estimate. \n\nReferences: \n\nhttps://calculator.aws/#/ \n\nhttps://aws.amazon.com/blogs/aws/new-cloud-tco-comparison-calculator-for-web-applications/', 'Server administration', 'Application advertising - The application advertising is not relevant for a Total Cost of Ownership (TCO) estimate.', 'Number of end-users - The number of end-users is not relevant for a Total Cost of Ownership (TCO) estimate.']
Question: An e-commerce company would like to build a chatbot for its customer service using Natural Language Understanding (NLU). As a Cloud Practitioner, which AWS service would you use?
Has explanation: True
Explanation: Amazon Lex

Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Amazon Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language chatbots to new and existing applications.

Amazon Lex Use Cases: via - https://aws.amazon.com/lex/
Has option_explanations: True
Option explanations: ['Amazon SageMaker - Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.', 'Amazon Lex\n\nAmazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Amazon Alexa, Amazon Lex provides high-quality speech recognition and language understanding capabilities, enabling the addition of sophisticated, natural language \x91chatbots\x92 to new and existing applications.\n\nAmazon Lex Use Cases: via - https://aws.amazon.com/lex/', 'Amazon Comprehend - Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing Natural Language Processing (NLP), you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text. \n\nReference: \n\nhttps://aws.amazon.com/lex/', 'Amazon Rekognition - With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos and also detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.']
Question: A growing start-up has trouble identifying and protecting sensitive data at scale. Which AWS fully managed service can assist with this task?
Has explanation: True
Explanation: Amazon Macie 

Amazon Macie is a fully managed data security and data privacy service that uses Machine Learning and pattern matching to discover and protect your sensitive data in AWS. 

Amazon Macie uses Machine Learning and pattern matching to cost-efficiently discover sensitive data at scale. Amazon Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. 

How Amazon Macie works: via - https://aws.amazon.com/macie/
Has option_explanations: True
Option explanations: ['AWS Key Management Service (AWS KMS) - AWS Key Management Service (AWS KMS) makes it easy for you to create and manage keys and control the use of encryption across a wide range of AWS services and in your applications. It is not used to discover and protect sensitive data in AWS. \n\nReference: \n\nhttps://aws.amazon.com/macie/', 'AWS Artifact - AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS\x92 security and compliance reports and selects online agreements. It is not used to discover and protect sensitive data in AWS.', 'Amazon Macie \n\nAmazon Macie is a fully managed data security and data privacy service that uses Machine Learning and pattern matching to discover and protect your sensitive data in AWS. \n\nAmazon Macie uses Machine Learning and pattern matching to cost-efficiently discover sensitive data at scale. Amazon Macie automatically detects a large and growing list of sensitive data types, including personally identifiable information (PII) such as names, addresses, and credit card numbers. It also gives you constant visibility of the data security and data privacy of your data stored in Amazon S3. \n\nHow Amazon Macie works: via - https://aws.amazon.com/macie/', 'AWS Secrets Manager - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It is not used to discover and protect sensitive data in AWS.']
Question: An organization would like to copy data across different Availability Zones (AZs) using Amazon EBS snapshots. Where are Amazon EBS snapshots stored in the AWS Cloud?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) 

You can create a point-in-time snapshot of an Amazon EBS Elastic Volume and use it as a baseline for new volumes or data backup. If you make periodic snapshots of a volume, the snapshots are incrementalthe new snapshot saves only the blocks that have changed since your last snapshot. 

You can back up the data on your Amazon EBS Elastic Volumes to Amazon Simple Storage Service (Amazon S3) by taking point-in-time snapshots.
Has option_explanations: True
Option explanations: ['Amazon Elastic File System (Amazon EFS) - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, elastic file system for Linux-based workloads for use with AWS Cloud services and on-premises resources. Amazon EBS snapshots cannot be stored on Amazon EFS. \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html', 'Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. Amazon EBS snapshots cannot be stored on Amazon RDS.', 'Amazon Simple Storage Service (Amazon S3) \n\nYou can create a point-in-time snapshot of an Amazon EBS Elastic Volume and use it as a baseline for new volumes or data backup. If you make periodic snapshots of a volume, the snapshots are incremental\x97the new snapshot saves only the blocks that have changed since your last snapshot. \n\nYou can back up the data on your Amazon EBS Elastic Volumes to Amazon Simple Storage Service (Amazon S3) by taking point-in-time snapshots.', 'Amazon Elastic Compute Cloud (Amazon EC2) - Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Amazon EBS snapshots cannot be stored on Amazon EC2.']
Question: Which AWS serverless service allows you to prepare data for analytics?
Has explanation: True
Explanation: AWS Glue 

AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. 

How AWS Glue works: via - https://aws.amazon.com/glue/
Has option_explanations: True
Option explanations: ['AWS Glue \n\nAWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. \n\nHow AWS Glue works: via - https://aws.amazon.com/glue/', 'Amazon EMR - Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. EMR is used for analytics and not to prepare data for analytics. \n\nReference: \n\nhttps://aws.amazon.com/glue/', 'Amazon Redshift - Amazon Redshift is a fast and scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift is used for analytics and not to prepare data for analytics.', 'Amazon Athena - Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon Simple Storage Service (Amazon S3) using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. Amazon Athena is used for analytics and not to prepare data for analytics.']
Question: Which of the following statements is an AWS best practice when architecting for the Cloud?
Has explanation: True
Explanation: Automation 

Automation should be implemented to improve both your system's stability and the efficiency of your organization. There are many services to automate application architecture (AWS Elastic Beanstalk, Auto Scaling, AWS Lambda, etc.) to ensure more resiliency, scalability, and performance.
Has option_explanations: True
Option explanations: ["Automation \n\nAutomation should be implemented to improve both your system's stability and the efficiency of your organization. There are many services to automate application architecture (AWS Elastic Beanstalk, Auto Scaling, AWS Lambda, etc.) to ensure more resiliency, scalability, and performance.", 'Servers, not services - The correct best practice is: "Services, not servers". AWS recommends developing, managing, and operating applications, especially at scale, using the broad set of compute, storage, database, analytics, applications, and deployment services offered by AWS to move faster and lower IT costs.', 'Close coupling - The correct best practice is: "Loose coupling". AWS recommends that, as application complexity increases, IT systems should be designed in a way that reduces interdependencies. Therefore, a change or a failure in one component should not cascade to other components.', 'Security comes last - AWS allows you to improve your security in many, more simple ways. Therefore, you should take advantage of this and implement a high level of security. \n\nReference: \n\nhttps://aws.amazon.com/architecture/well-architected/']
Question: A Cloud Practitioner would like to get operational insights of its resources to quickly identify any issues that might impact applications using those resources. Which AWS service can help with this task?
Has explanation: True
Explanation: AWS Systems Manager 

AWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. 

With AWS Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. You can also take action on each resource group depending on your operational needs. AWS Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. 

How AWS Systems Manager works: via - https://aws.amazon.com/systems-manager/
Has option_explanations: True
Option explanations: ['AWS Health Dashboard - Your Account Health - AWS Health Dashboard - Your Account Health provides alerts and remediation guidance when AWS is experiencing events that may impact you. It is not used to get operational insights of AWS resources.', 'AWS Systems Manager \n\nAWS Systems Manager allows you to centralize operational data from multiple AWS services and automate tasks across your AWS resources. You can create logical groups of resources such as applications, different layers of an application stack, or production versus development environments. \n\nWith AWS Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. You can also take action on each resource group depending on your operational needs. AWS Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. \n\nHow AWS Systems Manager works: via - https://aws.amazon.com/systems-manager/', 'AWS Trusted Advisor - AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. It is not used to get operational insights of AWS resources. \n\nReference: \n\nhttps://aws.amazon.com/systems-manager/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not used to get operational insights of AWS resources.']
Question: A research lab needs to be notified in case of a configuration change for security and compliance reasons. Which AWS service can assist with this task?
Has explanation: True
Explanation: AWS Config 

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. 

How AWS Config works: via - https://aws.amazon.com/config/
Has option_explanations: True
Option explanations: ['AWS Config \n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With AWS Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting. \n\nHow AWS Config works: via - https://aws.amazon.com/config/', 'AWS Secrets Manager - AWS Secrets Manager helps you protect the secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. It cannot notify configuration changes. \n\nReference: \n\nhttps://aws.amazon.com/config/', 'AWS Trusted Advisor - AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. It cannot notify configuration changes.', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on your Amazon EC2 instances. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. It cannot notify configuration changes.']
Question: Which of the following criteria are used to calculate the charge for Amazon EBS Volumes? (Select Two)
Has explanation: True
Explanation: Volume type 

Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone (AZ) to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutesall while paying a low price for only what you provision. 

The fundamental charges for EBS volumes are the volume type (based on performance), the storage volume in GB per month provisioned, the number of IOPS provisioned per month, the storage consumed by snapshots, and outbound data transfer.
Has option_explanations: True
Option explanations: ['Volume type \n\nAmazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone (AZ) to protect you from component failure, offering high availability and durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run your workloads. With Amazon EBS, you can scale your usage up or down within minutes\x97all while paying a low price for only what you provision. \n\nThe fundamental charges for EBS volumes are the volume type (based on performance), the storage volume in GB per month provisioned, the number of IOPS provisioned per month, the storage consumed by snapshots, and outbound data transfer.', 'The Amazon EC2 instance type the Amazon EBS Elastic volume is attached to - The Amazon EC2 instance type the Amazon EBS volume is attached to does not influence the EBS volume pricing.', 'Provisioned IOPS', 'Data type - The type of data stored on EBS volumes does not influence the price. \n\nReference: \n\nhttps://aws.amazon.com/ebs/pricing/', 'Data transfer IN - Data transfer-in is always free, including for Amazon EBS Elastic Volumes.']
Question: Which AWS service can be used to send, store, and receive messages between software components at any volume to decouple application tiers?
Has explanation: True
Explanation: Amazon Simple Queue Service (Amazon SQS)

Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.

Using Amazon Simple Queue Service (Amazon SQS), you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.
Has option_explanations: True
Option explanations: ['AWS Organizations - AWS Organizations offers policy-based management for multiple AWS accounts. With AWS Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. AWS Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It is not used to send, store, and receive messages between software components. \n\nReference: \n\nhttps://aws.amazon.com/sqs/', 'AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring. It is not used to send, store, and receive messages between software components.', 'Amazon Simple Queue Service (Amazon SQS)\n\nAmazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.\n\nUsing Amazon Simple Queue Service (Amazon SQS), you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.', 'Amazon Simple Notification Service (Amazon SNS) - Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\n\nPlease review this reference architecture for building a decoupled order processing system using Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS): via - https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/']
Question: A production company with predictable usage would like to reduce the cost of its Amazon Elastic Compute Cloud (Amazon EC2) instances by using reserved instances (RI). Which of the following length terms are available for Amazon EC2 reserved instances (RI)? (Select Two)
Has explanation: True
Explanation: 1 year
Has option_explanations: True
Option explanations: ['2 years - It is not possible to reserve instances for 2 years. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/', '6 months - It is not possible to reserve instances for 6 months.', '5 years - It is not possible to reserve instances for 5 years.', '1 year', '3 years\n\nReserved Instances (RI) provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. Besides, when Reserved Instances (RI) are assigned to a specific Availability Zone (AZ), they provide a capacity reservation, giving you additional confidence in your ability to launch instances when you need them.\n\nStandard and Convertible reserved instances can be purchased for a 1-year or 3-year term.\n\nEC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/']
Question: A data science team would like to build Machine Learning models for its projects. Which AWS service can it use?
Has explanation: True
Explanation: Amazon SageMaker 

Amazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.
Has option_explanations: True
Option explanations: ['Amazon SageMaker \n\nAmazon SageMaker is a fully-managed platform that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. Amazon SageMaker removes all the barriers that typically slow down developers who want to use machine learning.', 'Amazon Comprehend - Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing natural language processing (NLP), you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text.', 'Amazon Connect - Amazon Connect is an omnichannel cloud contact center. You can set up a contact center in a few steps, add agents who are located anywhere, and start engaging with your customers. You can create personalized experiences for your customers using omnichannel communications. Amazon Connect is an open platform that you can integrate with other enterprise applications. \n\nReference: \n\nhttps://aws.amazon.com/sagemaker/', "Amazon Polly - You can use Amazon Polly to turn text into lifelike speech thereby allowing you to create applications that talk. Polly's Text-to-Speech (TTS) service uses advanced deep learning technologies to synthesize natural sounding human speech."]
Question: Which service/tool will you use to create and provide trusted users with temporary security credentials that can control access to your AWS resources?
Has explanation: True
Explanation: AWS Security Token Service (AWS STS) 

AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (AWS IAM) users or for users that you authenticate (federated users). 

You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: 

Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. 

Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so. 

Temporary security credentials are generated by AWS Security Token Service (AWS STS). By default, AWS STS is a global service with a single endpoint at https://sts.amazonaws.com. However, you can also choose to make AWS STS API calls to endpoints in any other supported Region.
Has option_explanations: True
Option explanations: ['Amazon Cognito - Amazon Cognito is a higher level of abstraction than AWS Security Token Service (AWS STS). Amazon Cognito supports the same identity providers as AWS STS, and also supports unauthenticated (guest) access, and lets you migrate user data when a user signs in. Amazon Cognito also provides API operations for synchronizing user data so that it is preserved as users move between devices. Amazon Cognito helps create the user database, which is not possible with STS.', 'AWS Web Application Firewall (AWS WAF) - AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. \n\nReference: \n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html', 'AWS Security Token Service (AWS STS) \n\nAWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (AWS IAM) users or for users that you authenticate (federated users). \n\nYou can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: \n\nTemporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. \n\nTemporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permission to do so. \n\nTemporary security credentials are generated by AWS Security Token Service (AWS STS). By default, AWS STS is a global service with a single endpoint at https://sts.amazonaws.com. However, you can also choose to make AWS STS API calls to endpoints in any other supported Region.', 'AWS IAM Identity Center - AWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (AWS IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In AWS IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both.']
Question: A company needs to use a secure online data transfer tool/service that can automate the ongoing transfers from on-premises systems into AWS while providing support for incremental data backups. 

Which AWS tool/service is an optimal fit for this requirement?
Has explanation: True
Explanation: AWS DataSync 

AWS DataSync is a secure online data transfer service that simplifies, automates, and accelerates copying terabytes of data to and from AWS storage services. Easily migrate or replicate large data sets without having to build custom solutions or oversee repetitive tasks. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. 

You can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, and big data analytics in financial services. AWS DataSync provides timely delivery to ensure dependent processes are not delayed. You can specify exclude filters, include filters, or both, to determine which files, folders, or objects get transferred each time your task runs. 

AWS DataSync employs an AWS-designed transfer protocoldecoupled from the storage protocolto accelerate data movement. The protocol performs optimizations on how, when, and what data is sent over the network. Network optimizations performed by DataSync include incremental transfers, in-line compression, and sparse file detection, as well as in-line data validation and encryption. 

Data Transfer between on-premises and AWS using AWS DataSync: via - https://aws.amazon.com/datasync/
Has option_explanations: True
Option explanations: ['AWS Snowmobile - AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot-long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration.', 'AWS Storage Gateway - AWS Storage Gateway is a set of hybrid cloud services that give you on-premises access to virtually unlimited cloud storage. Customers use AWS Storage Gateway to integrate AWS Cloud storage with existing on-site workloads so they can simplify storage management and reduce costs for key hybrid cloud storage use cases. These include moving backups to the cloud, using on-premises file shares backed by cloud storage, and providing low latency access to data in AWS for on-premises applications.', 'AWS DataSync \n\nAWS DataSync is a secure online data transfer service that simplifies, automates, and accelerates copying terabytes of data to and from AWS storage services. Easily migrate or replicate large data sets without having to build custom solutions or oversee repetitive tasks. DataSync can copy data between Network File System (NFS) shares, or Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. \n\nYou can use AWS DataSync for ongoing transfers from on-premises systems into or out of AWS for processing. DataSync can help speed up your critical hybrid cloud storage workflows in industries that need to move active files into AWS quickly. This includes machine learning in life sciences, video production in media and entertainment, and big data analytics in financial services. AWS DataSync provides timely delivery to ensure dependent processes are not delayed. You can specify exclude filters, include filters, or both, to determine which files, folders, or objects get transferred each time your task runs. \n\nAWS DataSync employs an AWS-designed transfer protocol\x97decoupled from the storage protocol\x97to accelerate data movement. The protocol performs optimizations on how, when, and what data is sent over the network. Network optimizations performed by DataSync include incremental transfers, in-line compression, and sparse file detection, as well as in-line data validation and encryption. \n\nData Transfer between on-premises and AWS using AWS DataSync: via - https://aws.amazon.com/datasync/', 'AWS Snowcone - AWS Snowcone is the smallest member of the AWS Snow Family of edge computing, edge storage, and data transfer devices. Weighing in at 4.5 pounds (2.1 kg), AWS Snowcone is equipped with 8 terabytes of usable storage, while AWS Snowcone Solid State Drive (SSD) supports 14 terabytes of usable storage. Both referred to as AWS Snowcone, the device is ruggedized, secure, and purpose-built for use outside of a traditional data center. Its small form factor makes it a perfect fit for tight spaces or where portability is a necessity and network connectivity is unreliable. You can use AWS Snowcone in backpacks for first responders, or for IoT, vehicular, and drone use cases. You can execute compute applications at the edge, and you can ship the device with data to AWS for offline data transfer, or you can transfer data online with AWS DataSync from edge locations. \n\nReferences: \n\nhttps://aws.amazon.com/datasync/ \n\nhttps://aws.amazon.com/datasync/features/']
Question: An engineering team would like to cost-effectively run hundreds of thousands of batch computing workloads on AWS. As a Cloud Practitioner, which AWS service would you use for this task?
Has explanation: True
Explanation: AWS Batch 

AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. 

You can use AWS Batch to plan, schedule, and execute your batch computing workloads across the full range of AWS compute services. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example - memory optimized instance or CPU) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch provisions compute resources and optimize the job distribution based on the volume and resource requirements of the submitted batch jobs. 

Please review the common use cases for AWS Batch: via - https://aws.amazon.com/batch/
Has option_explanations: True
Option explanations: ['AWS Batch \n\nAWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. \n\nYou can use AWS Batch to plan, schedule, and execute your batch computing workloads across the full range of AWS compute services. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example - memory optimized instance or CPU) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch provisions compute resources and optimize the job distribution based on the volume and resource requirements of the submitted batch jobs. \n\nPlease review the common use cases for AWS Batch: via - https://aws.amazon.com/batch/', 'AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It can be used to run batch jobs but has a time limit and limited runtimes. It is usually used for smaller batch jobs.', 'Amazon Lightsail - Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project \x96 a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address \x96 for a low, predictable price. It is not used to run batch jobs.', 'AWS Fargate - AWS Fargate is a compute engine for Amazon Elastic Container Service (Amazon ECS) that allows you to run containers without having to manage servers or clusters. You can run batch jobs on AWS Fargate, but it is more expensive than AWS Batch. \n\nReference: \n\nhttps://aws.amazon.com/batch/']
Question: A company would like to define a set of rules to manage objects cost-effectively between Amazon Simple Storage Service (Amazon S3) storage classes. As a Cloud Practitioner, which Amazon S3 feature would you use?
Has explanation: True
Explanation: Amazon Simple Storage Service (Amazon S3) Lifecycle configuration 

To manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions (define when objects transition to another storage class) and expiration actions (define when objects expire. Amazon S3 deletes expired objects on your behalf). 

In this particular use case, you would use a transition action.
Has option_explanations: True
Option explanations: ['Amazon Simple Storage Service (Amazon S3) Bucket policies - An S3 bucket policy is a resource-based AWS Identity and Access Management (IAM) policy. You add a bucket policy to a bucket to grant other AWS accounts or IAM users access permissions for the bucket and the objects in it. Object permissions apply only to the objects that the bucket owner creates. It is not used to move objects between storage classes.', 'S3 Cross-Region Replication (S3 CRR) - S3 Cross-Region Replication (S3 CRR) enables automatic, asynchronous copying of objects across Amazon S3 buckets. Cross-Region replication (CRR) is used to copy objects across Amazon S3 buckets in different AWS Regions. It is not used to move objects between storage classes. \n\nReferences: \n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html \n\nhttps://aws.amazon.com/s3/', 'Amazon S3 Transfer Acceleration (Amazon S3TA) - Amazon S3 Transfer Acceleration (Amazon S3TA) enables fast, easy, and secure transfers of files over long distances between your client and an Amazon S3 bucket. It is not used to move objects between storage classes.', 'Amazon Simple Storage Service (Amazon S3) Lifecycle configuration \n\nTo manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions (define when objects transition to another storage class) and expiration actions (define when objects expire. Amazon S3 deletes expired objects on your behalf). \n\nIn this particular use case, you would use a transition action.']
Question: Which AWS tool/service will help you define your cloud infrastructure using popular programming languages such as Python and JavaScript?
Has explanation: True
Explanation: AWS Cloud Development Kit (AWS CDK)

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.

AWS Cloud Development Kit (AWS CDK) uses the familiarity and expressive power of programming languages for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.

In short, you use the AWS CDK framework to author AWS CDK projects which are executed to generate AWS CloudFormation templates.

How Cloud Development Kit (AWS CDK) works:via - https://aws.amazon.com/cdk/
Has option_explanations: True
Option explanations: ['AWS CodeBuild - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With AWS CodeBuild, you don\x92t need to provision, manage, and scale your own build servers. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue. \n\nReference: \n\nhttps://aws.amazon.com/cdk/', 'AWS CloudFormation - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources, and provision and manage them in an orderly and predictable fashion. AWS CloudFormation is designed to allow resource lifecycles to be managed repeatably, predictably, and safely while allowing for automatic rollbacks, automated state management, and management of resources across accounts and regions. AWS Cloud Development Kit (AWS CDK) helps code the same in higher-level languages and converts them into AWS CloudFormation templates.', "AWS Cloud Development Kit (AWS CDK)\n\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.\n\nAWS Cloud Development Kit (AWS CDK) uses the familiarity and expressive power of programming languages for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.\n\nIn short, you use the AWS CDK framework to author AWS CDK projects which are executed to generate AWS CloudFormation templates.\n\nHow Cloud Development Kit (AWS CDK) works:via - https://aws.amazon.com/cdk/", 'AWS Elastic Beanstalk - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, etc. You can simply upload your code in a programming language of your choice and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring.']
Question: A company would like to separate cost for AWS services by the department for cost allocation. Which of the following is the simplest way to achieve this task?
Has explanation: True
Explanation: Create tags for each department 

You can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria. 

Typically, you use business tags such as cost center/business unit, customer, or project to associate AWS costs with traditional cost-allocation dimensions. But a cost allocation report can include any tag. This lets you associate costs with technical or security dimensions, such as specific applications, environments, or compliance programs. 

Example of tagging for cost optimization: via - https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html
Has option_explanations: True
Option explanations: ['Create tags for each department \n\nYou can assign metadata to your AWS resources in the form of tags. Each tag is a label consisting of a user-defined key and value. Tags can help you manage, identify, organize, search for, and filter resources. You can create tags to categorize resources by purpose, owner, environment, or other criteria. \n\nTypically, you use business tags such as cost center/business unit, customer, or project to associate AWS costs with traditional cost-allocation dimensions. But a cost allocation report can include any tag. This lets you associate costs with technical or security dimensions, such as specific applications, environments, or compliance programs. \n\nExample of tagging for cost optimization: via - https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html', 'Create one account for all departments and share this account - Sharing accounts is not a security best practice, and is not recommended.', 'Create different virtual private cloud (VPCs) for different departments - Creating different VPCs will not help with separating costs. \n\nReference: \n\nhttps://docs.aws.amazon.com/general/latest/gr/aws_tagging.html', 'Create different accounts for different departments - Users can belong to several departments. Therefore, having different accounts for different departments would imply some users having several accounts. This is contrary to the security best practice: one physical user = one account. Also, it is much simpler to set up tags for tracking costs for each department.']
Question: An engineering team is new to the AWS Cloud and it would like to launch a dev/test environment with low monthly pricing. Which AWS service can address this use case?
Has explanation: True
Explanation: Amazon LightSail 

Amazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project  a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address  for a low, predictable price. 

It is great for people with little cloud experience to launch quickly a popular IT solution ready to use immediately.
Has option_explanations: True
Option explanations: ['AWS CloudFormation - AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. Using AWS CloudFormation requires experience as resources are deployed within a virtual private cloud (VPC).', 'Amazon Elastic Compute Cloud (Amazon EC2) - Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Deploying a dev/test environment with Amazon EC2 requires experience as instances are deployed within a virtual private cloud (VPC).', 'Amazon LightSail \n\nAmazon Lightsail is designed to be the easiest way to launch and manage a virtual private server (VPS) with AWS. Amazon Lightsail plans include everything you need to jumpstart your project \x96 a virtual machine, SSD- based storage, data transfer, Domain Name System (DNS) management, and a static IP address \x96 for a low, predictable price. \n\nIt is great for people with little cloud experience to launch quickly a popular IT solution ready to use immediately.', 'Amazon Elastic Container Service (Amazon ECS) - Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS. Amazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines. Using Amazon ECS requires experience. Reference: \n\nhttps://aws.amazon.com/lightsail/']
Question: A company would like to create a private, high bandwidth network connection between its on-premises data centers and AWS Cloud. As a Cloud Practitioner, which of the following options would you recommend?
Has explanation: True
Explanation: AWS Direct Connect 

AWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. 

How AWS Direct Connect works: via - https://aws.amazon.com/directconnect/
Has option_explanations: True
Option explanations: ["AWS Site-to-Site VPN - By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection and configuring routing to pass traffic through the connection. It uses the public internet and is therefore not suited for this use case.", 'AWS Direct Connect \n\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections. \n\nHow AWS Direct Connect works: via - https://aws.amazon.com/directconnect/', 'VPC Endpoints - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. It does not connect your on-premises data centers and AWS Cloud.', 'VPC peering connection - A VPC peering connection is a networking connection between two virtual private clouds (VPCs) that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. It is used to connect virtual private clouds (VPCs) together, and not on-premises data centers and AWS Cloud. \n\nReference: \n\nhttps://aws.amazon.com/directconnect/']
Question: A media company wants to enable customized content suggestions for the users of its movie streaming platform. Which AWS service can provide these personalized recommendations based on historic data?
Has explanation: True
Explanation: Amazon Personalize 

Amazon Personalize enables developers to build applications with the same machine learning (ML) technology used by Amazon.com for real-time personalized recommendations. Amazon Personalize can be used to personalize the end-user experience over any digital channel. Examples include product recommendations for e-commerce, news articles and content recommendation for publishing, media, and social networks, hotel recommendations for travel websites, credit card recommendations for banks, and match recommendations for dating sites. These recommendations and personalized experiences can be delivered over websites, mobile apps, or email/messaging. Amazon Personalize can also be used to customize the user experience when user interaction is over a physical channel, e.g., a meal delivery company could personalize weekly meals to users in a subscription plan. 

Amazon Personalize supports the following key use cases: 

Personalized recommendations 
Similar items 
Personalized reranking i.e. rerank a list of items for a user 
Personalized promotions/notifications
Has option_explanations: True
Option explanations: ['Amazon Personalize \n\nAmazon Personalize enables developers to build applications with the same machine learning (ML) technology used by Amazon.com for real-time personalized recommendations. Amazon Personalize can be used to personalize the end-user experience over any digital channel. Examples include product recommendations for e-commerce, news articles and content recommendation for publishing, media, and social networks, hotel recommendations for travel websites, credit card recommendations for banks, and match recommendations for dating sites. These recommendations and personalized experiences can be delivered over websites, mobile apps, or email/messaging. Amazon Personalize can also be used to customize the user experience when user interaction is over a physical channel, e.g., a meal delivery company could personalize weekly meals to users in a subscription plan. \n\nAmazon Personalize supports the following key use cases: \n\nPersonalized recommendations \nSimilar items \nPersonalized reranking i.e. rerank a list of items for a user \nPersonalized promotions/notifications', 'Amazon SageMaker - Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models.', 'Amazon Customize - There is no such service as Amazon Customize. This option has been added as a distractor.', 'Amazon Comprehend - Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover information in unstructured data. Instead of combing through documents, the process is simplified and unseen information is easier to understand. \n\nThe service can identify critical elements in data, including references to language, people, and places, and the text files can be categorized by relevant topics. In real-time, you can automatically and accurately detect customer sentiment in your content. \n\nReference: \n\nhttps://aws.amazon.com/personalize/']
Question: Which of the following options is NOT a feature of Amazon Inspector?
Has explanation: True
Explanation: Track configuration changes

Tracking configuration changes is a feature of AWS Config.

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.

How AWS Config works: via - https://aws.amazon.com/config/
Has option_explanations: True
Option explanations: ['Inspect running operating systems (OS) against known vulnerabilities \n\nThese options are all features of Amazon Inspector. \n\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. \n\nAmazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. \n\nAmazon Inspector also offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess. The agent monitors the behavior of the EC2 instances, including network, file system, and process activity. It also collects a wide set of behavior and configuration data (telemetry). \n\nReferences: \n\nhttps://aws.amazon.com/config/ \n\nhttps://aws.amazon.com/inspector/', 'Track configuration changes\n\nTracking configuration changes is a feature of AWS Config.\n\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.\n\nHow AWS Config works: via - https://aws.amazon.com/config/', 'Analyze against unintended network accessibility', 'Automate security assessments']
Question: Which of the following statements is the MOST accurate when describing AWS Elastic Beanstalk?
Has explanation: True
Explanation: It is a Platform as a Service (PaaS) that allows you to deploy and scale web applications and services 

AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their applications, and AWS Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. 

It is a Platform as a Service (PaaS) as you only manage the applications and the data. 

Please review this overview of the types of Cloud Computing: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['It is an Infrastructure as Code (IaC) that allows you to model and provision resources needed for an application - This is the definition of AWS CloudFormation. AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion. You can use the AWS CloudFormation sample templates or create your own templates to describe your AWS resources, and any associated dependencies or runtime parameters, required to run your application.', 'It is an Infrastructure as a Service (IaaS) that allows you to deploy and scale web applications and services - AWS Elastic Beanstalk allows you to deploy and scale web applications and services, but it is not an Infrastructure as a Service (IaaS). With AWS Elastic Beanstalk, you do not manage the runtime, the middleware, and the operating system. \n\nReference: \n\nhttps://aws.amazon.com/elasticbeanstalk/', 'It is a Platform as a Service (PaaS) that allows you to model and provision resources needed for an application - AWS Elastic Beanstalk is a Platform as a Service (PaaS). However, the service that allows you to model and provision resources needed for an application is AWS CloudFormation.', 'It is a Platform as a Service (PaaS) that allows you to deploy and scale web applications and services \n\nAWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. Developers simply upload their applications, and AWS Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. \n\nIt is a Platform as a Service (PaaS) as you only manage the applications and the data. \n\nPlease review this overview of the types of Cloud Computing: via - https://aws.amazon.com/types-of-cloud-computing/']
Question: A start-up would like to quickly deploy a popular technology on AWS. As a Cloud Practitioner, which AWS tool would you use for this task?
Has explanation: True
Explanation: AWS Partner Solutions (formerly Quick Starts) 

AWS Partner Solutions are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes. 

AWS Partner Solutions are automated reference deployments for key workloads on the AWS Cloud. Each Partner Solution launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.
Has option_explanations: True
Option explanations: ['AWS Whitepapers - AWS Whitepapers are technical content authored by AWS and the AWS community to expand your knowledge of the cloud. They include technical whitepapers, technical guides, reference material, and reference architecture diagrams. You can find useful content for your deployment, but it is not a service that will deploy technologies. \n\nReference: \n\nhttps://aws.amazon.com/quickstart/', 'AWS CodeDeploy - AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. It is not suited to rapidly deploy popular technologies on AWS ready to be used immediately.', 'AWS Partner Solutions (formerly Quick Starts) \n\nAWS Partner Solutions are automated reference deployments built by Amazon Web Services (AWS) solutions architects and AWS Partners. Partner Solutions help you deploy popular technologies to AWS according to AWS best practices. You can reduce hundreds of manual procedures to a few steps and start using your environment within minutes. \n\nAWS Partner Solutions are automated reference deployments for key workloads on the AWS Cloud. Each Partner Solution launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability.', 'AWS Forums - AWS Forums is an AWS community platform where people can help each other. It is not used to deploy technologies on AWS.']
Question: A company is planning to implement Chaos Engineering to expose any blind spots that can disrupt the resiliency of the application. 

Which AWS service will help implement this requirement with the least effort? 


Has explanation: True
Explanation: AWS Fault Injection Simulator (AWS FIS) 

AWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an applications performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as a sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, and monitor blind spots, and performance bottlenecks that are difficult to find in distributed systems. 

AWS Fault Injection Simulator (AWS FIS) simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With AWS Fault Injection Simulator (AWS FIS), teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Simulator (AWS FIS) provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real-world conditions necessary to find hidden weaknesses.
Has option_explanations: True
Option explanations: ['AWS Fault Injection Simulator (AWS FIS) \n\nAWS Fault Injection Simulator (AWS FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application\x92s performance, observability, and resiliency. Fault injection experiments are used in chaos engineering, which is the practice of stressing an application in testing or production environments by creating disruptive events, such as a sudden increase in CPU or memory consumption, observing how the system responds, and implementing improvements. Fault injection experiment helps teams create the real-world conditions needed to uncover the hidden bugs, and monitor blind spots, and performance bottlenecks that are difficult to find in distributed systems. \n\nAWS Fault Injection Simulator (AWS FIS) simplifies the process of setting up and running controlled fault injection experiments across a range of AWS services so teams can build confidence in their application behavior. With AWS Fault Injection Simulator (AWS FIS), teams can quickly set up experiments using pre-built templates that generate the desired disruptions. AWS Fault Injection Simulator (AWS FIS) provides the controls and guardrails that teams need to run experiments in production, such as automatically rolling back or stopping the experiment if specific conditions are met. With a few clicks in the console, teams can run complex scenarios with common distributed system failures happening in parallel or building sequentially over time, enabling them to create the real-world conditions necessary to find hidden weaknesses.', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity.', 'AWS Trusted Advisor - AWS Trusted Advisors provides recommendations that help you follow AWS best practices. AWS Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. You can then follow the check recommendations to optimize your services and resources.', 'Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. \n\nReference: \n\nhttps://aws.amazon.com/fis/features/']
Question: A brand-new startup would like to remove its need to manage the underlying infrastructure and focus on the deployment and management of its applications. Which type of cloud computing does this refer to?
Has explanation: True
Explanation: Platform as a Service (PaaS) 

Cloud Computing can be broadly divided into three types - Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS). 

Platform as a Service (PaaS) removes the need to manage underlying infrastructure (usually hardware and operating systems) and allows you to focus on the deployment and management of your applications. You dont need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application. 

Please review this overview of the types of cloud computing: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['On-premises - When an enterprise opts for on-premises, it needs to create, upgrade, and scale the on-premise IT infrastructure by investing in sophisticated hardware, compatible software, and robust services. Also, the business needs to deploy dedicated IT staff to upkeep, scale, and manage the on-premise infrastructure continuously. \n\nReference: \n\nhttps://aws.amazon.com/types-of-cloud-computing/', 'Software as a Service (SaaS) - Software as a Service (SaaS) provides you with a complete product that is run and managed by the service provider. With a Software as a Service (SaaS) offering, you don\x92t have to think about how the service is maintained or how the underlying infrastructure is managed. You only need to think about how you will use that particular software. Amazon Rekognition is an example of a SaaS service.', 'Platform as a Service (PaaS) \n\nCloud Computing can be broadly divided into three types - Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS). \n\nPlatform as a Service (PaaS) removes the need to manage underlying infrastructure (usually hardware and operating systems) and allows you to focus on the deployment and management of your applications. You don\x92t need to worry about resource procurement, capacity planning, software maintenance, patching, or any of the other undifferentiated heavy lifting involved in running your application. \n\nPlease review this overview of the types of cloud computing: via - https://aws.amazon.com/types-of-cloud-computing/', 'Infrastructure as a Service (IaaS) - Infrastructure as a Service (IaaS) contains the basic building blocks for cloud IT. It typically provides access to networking features, computers (virtual or on dedicated hardware), and data storage space. Infrastructure as a Service (IaaS) gives the highest level of flexibility and management control over IT resources.']
Question: The IT infrastructure at a university is deployed on AWS Cloud and it's experiencing a read-intensive workload. As a Cloud Practitioner, which AWS service would you use to take the load off databases?
Has explanation: True
Explanation: Amazon ElastiCache 

Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. 

If Amazon EC2 instances are intensively reading data from a database, ElastiCache can cache some values to take the load off the database. 

How Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/))
Has option_explanations: True
Option explanations: ['Amazon EMR - Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. It cannot be used to take the load off the databases. \n\nReference: \n\nhttps://aws.amazon.com/elasticache/', 'Amazon Relational Database Service (Amazon RDS) - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security, and compatibility they need. It cannot be used to take the load off databases. However, Amazon ElastiCache is often used with Amazon RDS to take the load off RDS.', 'Amazon ElastiCache \n\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. \n\nIf Amazon EC2 instances are intensively reading data from a database, ElastiCache can cache some values to take the load off the database. \n\nHow Amazon ElastiCache works: via - https://aws.amazon.com/elasticache/))', 'AWS Glue - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. It cannot be used to take the load off the databases.']
Question: Which security control tool can be used to deny traffic from a specific IP address?
Has explanation: True
Explanation: Network Access Control List (network ACL) 

A Network Access Control List (network ACL) is an optional layer of security for your virtual private cloud (VPC) that acts as a firewall for controlling traffic in and out of one or more subnets (i.e. it works at the subnet level). A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. 

Network Access Control List (network ACL) Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html
Has option_explanations: True
Option explanations: ['Security Group - A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not at the subnet level. You can specify allow rules, but not deny rules. You can specify separate rules for inbound and outbound traffic.', "VPC Flow Logs - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon Simple Storage Service (Amazon S3). After you've created a flow log, you can retrieve and view its data in the chosen destination. However, it cannot deny traffic from a specific IP address. \n\nReference: \n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html", 'Network Access Control List (network ACL) \n\nA Network Access Control List (network ACL) is an optional layer of security for your virtual private cloud (VPC) that acts as a firewall for controlling traffic in and out of one or more subnets (i.e. it works at the subnet level). A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. \n\nNetwork Access Control List (network ACL) Overview: via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html', 'Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. Amazon GuardDuty also detects potentially compromised instances or reconnaissance by attackers. It cannot deny traffic from a specific IP address. \n\n']
Question: A company would like to reserve Amazon Elastic Compute Cloud (Amazon EC2) compute capacity for three years to reduce costs. The company also plans to increase their workloads during this period. As a Cloud Practitioner, which Amazon Elastic Compute Cloud (Amazon EC2) reserved instance (RI) type would you recommend?
Has explanation: True
Explanation: Convertible reserved instance (RI)

Purchase convertible reserved instance (RI) if you need additional flexibility, such as the ability to use different instance families, operating systems, or tenancies over the reserved instance (RI) term. Convertible reserved instance (RI) provides you with a significant discount (up to 54%) compared to an on-demand instance and can be purchased for a 1-year or 3-year term.

Convertible reserved instance (RI) can be useful when workloads are likely to change. In this case, a convertible reserved instance (RI) enables you to adapt as needs evolve while still obtaining discounts and capacity reservation.

Amazon EC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/
Has option_explanations: True
Option explanations: ['Convertible reserved instance (RI)\n\nPurchase convertible reserved instance (RI) if you need additional flexibility, such as the ability to use different instance families, operating systems, or tenancies over the reserved instance (RI) term. Convertible reserved instance (RI) provides you with a significant discount (up to 54%) compared to an on-demand instance and can be purchased for a 1-year or 3-year term.\n\nConvertible reserved instance (RI) can be useful when workloads are likely to change. In this case, a convertible reserved instance (RI) enables you to adapt as needs evolve while still obtaining discounts and capacity reservation.\n\nAmazon EC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/', 'Standard reserved instance (RI) - Standard reserved instance (RI) provides you with a significant discount (up to 72%) compared to on-demand instance pricing, and can be purchased for a 1-year or 3-year term. Standard reserved instance (RI) do not offer as much flexibility as convertible reserved instance (RI), such as not being able to change the instance family type; and therefore are not best-suited for this use case. \n\nReview the differences between standard reserved instance (RI) and convertible reserved instance (RI): https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/standard-vs.-convertible-offering-classes.html', 'Scheduled reserved instance (RI) - AWS does not support scheduled reserved instance (RI), so this option is ruled out.', 'Adaptable reserved instances (RI) - Adaptable reserved instance (RI) is not a valid type of reserved instance (RI). It is a distractor. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/pricing/']
Question: Which of the following statements is INCORRECT regarding Amazon EBS Elastic Volumes?
Has explanation: True
Explanation: Amazon EBS Elastic Volumes can be bound to several Availability Zones (AZs) 

An Amazon EBS Elastic Volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. 

When using Amazon EBS Elastic Volumes, the volume, and the instance must be in the same Availability Zone (AZ).
Has option_explanations: True
Option explanations: ['Amazon EBS Elastic Volumes are bound to a specific Availability Zone (AZ) - As mentioned, when using Amazon EBS Elastic Volumes, the volume and the instance must be in the same Availability Zone(AZ).', 'Amazon EBS Elastic Volumes can persist data after their termination - Unlike an Amazon EC2 instance store, an Amazon EBS Elastic Volume is off-instance storage that can persist independently from the life of an instance. \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html', 'Amazon EBS Elastic Volumes can be mounted to one instance at a time - At the Certified Cloud Practitioner level, Amazon EBS Elastic Volumes can be mounted to one instance at a time. It is also possible that an Amazon EBS Elastic Volume is not mounted to an instance.', 'Amazon EBS Elastic Volumes can be bound to several Availability Zones (AZs) \n\nAn Amazon EBS Elastic Volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. \n\nWhen using Amazon EBS Elastic Volumes, the volume, and the instance must be in the same Availability Zone (AZ).']
Question: According to the AWS Well-Architected Framework, which of the following action is recommended in the Security pillar?
Has explanation: True
Explanation: Use AWS Key Management Service (AWS KMS) to encrypt data 

The Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. 

Encrypting data is part of the design principle "Protect data in transit and at rest": Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate. 

AWS Key Management Service (AWS KMS) makes it easy for you to create and control keys used for encryption. It is a key service of the Security pillar. 

The AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. 

The AWS Well-Architected Framework is based on six pillars  Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. 

Overview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/
Has option_explanations: True
Option explanations: ['Use AWS Key Management Service (AWS KMS) to encrypt data \n\nThe Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. \n\nEncrypting data is part of the design principle "Protect data in transit and at rest": Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate. \n\nAWS Key Management Service (AWS KMS) makes it easy for you to create and control keys used for encryption. It is a key service of the Security pillar. \n\nThe AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. \n\nThe AWS Well-Architected Framework is based on six pillars \x97 Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. \n\nOverview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/', 'Use Amazon CloudWatch to measure overall efficiency - Amazon CloudWatch is a monitoring and management service built for developers, system operators, site reliability engineers (SRE), and IT managers. Using Amazon CloudWatch to measure overall efficiency relates more to the Reliability pillar.', 'Use AWS Cost Explorer to view and track your usage in detail - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Using Cost Explorer to view and track your usage in detail relates more to the Cost Optimization pillar.', 'Use AWS CloudFormation to automate security best practices - AWS CloudFormation provides a common language for you to model and provision AWS and third-party application resources in your cloud environment. It is not used to automate security best practices. If you want to automate security best practices, you should use Amazon Inspector. \n\nReference: \n\nhttps://aws.amazon.com/architecture/well-architected/ \n\n']
Question: Which of the following statements is CORRECT regarding the scope of an Amazon Virtual Private Cloud (VPC)?
Has explanation: True
Explanation: A VPC spans all Availability Zones (AZs) within an AWS region 

Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. 

An Amazon VPC spans all Availability Zones (AZs) within a region.
Has option_explanations: True
Option explanations: ['A VPC spans all Availability Zones (AZs) within an AWS region \n\nAmazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. \n\nAn Amazon VPC spans all Availability Zones (AZs) within a region.', 'A VPC spans all Availability Zones (AZs) in all AWS regions - A VPC is located within an AWS region.', 'A VPC spans all AWS regions within an Availability Zone (AZ) - AWS has the concept of a Region, which is a physical location around the world where AWS clusters data centers. Each AWS Region consists of multiple (two or more), isolated, and physically separate Availability Zone (AZs) within a geographic area. An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. Therefore, regions cannot be within an Availability Zone. Moreover, a VPC is located within a region. \n\nAWS Regions and Availability Zones (AZs) Overview: via - https://aws.amazon.com/about-aws/global-infrastructure/regions_az/ \n\nReference: \n\nhttps://aws.amazon.com/vpc/', 'Amazon VPC spans all subnets in all AWS regions - A VPC is located within an AWS region.']
Question: A start-up would like to monitor its cost on the AWS Cloud and would like to choose an optimal Savings Plan. As a Cloud Practitioner, which AWS service would you use?
Has explanation: True
Explanation: AWS Cost Explorer 

AWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis and empowers you to dive deeper using several filtering dimensions (e.g., AWS Service, AWS Region, Linked Account, etc.). AWS Cost Explorer also gives you access to a set of default reports to help you get started, while also allowing you to create custom reports from scratch. 

Customers can receive Savings Plan recommendations at the member (linked) account level in addition to the existing AWS organization-level recommendations in AWS Cost Explorer.
Has option_explanations: True
Option explanations: ['AWS Pricing Calculator - AWS Pricing Calculator lets you explore AWS services, and create an estimate for the cost of your use cases on AWS. It does not provide Savings Plan recommendations.', 'AWS Cost Explorer \n\nAWS Cost Explorer lets you explore your AWS costs and usage at both a high level and at a detailed level of analysis and empowers you to dive deeper using several filtering dimensions (e.g., AWS Service, AWS Region, Linked Account, etc.). AWS Cost Explorer also gives you access to a set of default reports to help you get started, while also allowing you to create custom reports from scratch. \n\nCustomers can receive Savings Plan recommendations at the member (linked) account level in addition to the existing AWS organization-level recommendations in AWS Cost Explorer.', 'AWS Budgets - AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reserved instance (RI) utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. It does not provide Savings Plan recommendations.', 'AWS Cost & Usage Report (AWS CUR) - The AWS Cost & Usage Report (AWS CUR) is a single location for accessing comprehensive information about your AWS costs and usage. It does not provide Savings Plan recommendations.']
Question: Which AWS service can inspect Amazon CloudFront distributions running on any HTTP web server?
Has explanation: True
Explanation: AWS Web Application Firewall (AWS WAF)

AWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting (XSS).

AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront and lets you control access to your content.

When you use the AWS web application firewall (AWS WAF) on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesnt come at the expense of performance. Blocked requests are stopped before they reach your web servers.

How AWS WAF works: via - https://aws.amazon.com/waf/
Has option_explanations: True
Option explanations: ['Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Amazon Inspector security assessments help you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances.', 'AWS Web Application Firewall (AWS WAF)\n\nAWS Web Application Firewall (AWS WAF) is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting (XSS).\n\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront and lets you control access to your content.\n\nWhen you use the AWS web application firewall (AWS WAF) on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn\x92t come at the expense of performance. Blocked requests are stopped before they reach your web servers.\n\nHow AWS WAF works: via - https://aws.amazon.com/waf/', 'Elastic Load Balancing (ELB) - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It does not inspect Amazon CloudFront distributions. \n\nReference: \n\nhttps://aws.amazon.com/waf/', 'AWS GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It does not inspect Amazon CloudFront distributions.']
Question: Which Amazon Elastic Compute Cloud (Amazon EC2) Auto Scaling feature can help with fault tolerance?
Has explanation: True
Explanation: Replacing unhealthy Amazon EC2 instances 

Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove Amazon EC2 instances according to the conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. 

Amazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and replace it with a new one.
Has option_explanations: True
Option explanations: ['Lower cost by adjusting the number of Amazon EC2 instances - Amazon EC2 Auto Scaling adds instances only when needed, and can scale across purchase options to optimize performance and cost. However, this will not help with fault tolerance.', 'Distributing load to Amazon EC2 instances - Even though this helps with fault tolerance and is often used with Amazon EC2 Auto Scaling, it is a feature of Elastic Load Balancing (ELB) and not an Amazon EC2 Auto Scaling. Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs).', 'Having the right amount of computing capacity - Amazon EC2 Auto Scaling ensures that your application always has the right amount of computing capacity, so your application can handle the workload. \n\nReference: \n\nhttps://aws.amazon.com/ec2/autoscaling/', 'Replacing unhealthy Amazon EC2 instances \n\nAmazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove Amazon EC2 instances according to the conditions you define. You can use the fleet management features of Amazon EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of Amazon EC2 Auto Scaling to add or remove EC2 instances. \n\nAmazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and replace it with a new one.']
Question: A company would like to optimize Amazon Elastic Compute Cloud (Amazon EC2) costs. Which of the following actions can help with this task? (Select TWO)
Has explanation: True
Explanation: Set up Auto Scaling groups to align the number of instances with the demand
Has option_explanations: True
Option explanations: ["Build its own servers - Building your own servers is more expensive than using EC2 instances in the cloud. You're more likely to spend more money than saving money. \n\nReferences: \n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html \n\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/ \n\nhttps://wa.aws.amazon.com/wat.concept.horizontal-scaling.en.html \n\nhttps://aws.amazon.com/autoscaling/", 'Vertically scale the EC2 instances - Vertically scaling EC2 instances (increasing one computer performance by adding CPUs, memory, and storage) is limited and is way more expensive than scaling horizontally (adding more computers to the system).', 'Opt for a higher AWS Support plan - The AWS Support plans do not help with EC2 costs.', 'Set up Auto Scaling groups to align the number of instances with the demand', 'Purchase Amazon EC2 Reserved instances (RIs) \n\nAn Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for automatic scaling and management. You can adjust its size to meet demand, either manually or by using automatic scaling. \n\nAWS Auto Scaling can help you optimize your utilization and cost efficiencies when consuming AWS services so you only pay for the resources you need. \n\nHow AWS Auto Scaling works: via - https://aws.amazon.com/autoscaling/ \n\nAmazon EC2 Reserved Instances (RI) provide a significant discount (up to 72%) compared to On-Demand pricing and provide a capacity reservation when used in a specific Availability Zone (AZ). \n\nEC2 Pricing Options Overview: via - https://aws.amazon.com/ec2/pricing/']
Question: Which AWS service can be used to view the most comprehensive billing details for the past month?
Has explanation: True
Explanation: AWS Cost & Usage Report (AWS CUR)

The AWS Cost & Usage Report (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself.

AWS Cost & Usage Report (AWS CUR) Overview :via - https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html
Has option_explanations: True
Option explanations: ['AWS Cost Explorer - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. AWS Cost Explorer includes a default report that helps you visualize the costs and usage associated with your top five cost-accruing AWS services and gives you a detailed breakdown of all services in the table view. The reports let you adjust the time range to view historical data going back up to twelve months to gain an understanding of your cost trends. AWS Cost Explorer cannot provide granular billing details for the past month.', 'AWS Pricing Calculator - AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can model your solutions before building them, explore the price points and calculations behind your estimate, and find the available instance types and contract terms that meet your needs. This enables you to make informed decisions about using AWS. You can plan your AWS costs and usage or price out by setting up a new set of instances and services. AWS Pricing Calculator cannot provide billing details for the past month.', 'AWS Cost & Usage Report (AWS CUR)\n\nThe AWS Cost & Usage Report (AWS CUR) contains the most comprehensive set of cost and usage data available. You can use Cost and Usage Reports to publish your AWS billing reports to an Amazon Simple Storage Service (Amazon S3) bucket that you own. You can receive reports that break down your costs by the hour or month, by product or product resource, or by tags that you define yourself.\n\nAWS Cost & Usage Report (AWS CUR) Overview :via - https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html', 'AWS Budgets - AWS Budgets gives the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. You can also use AWS Budgets to set reservation utilization or coverage targets and receive alerts when your utilization drops below the threshold you define. Budgets can be created at the monthly, quarterly, or yearly level, and you can customize the start and end dates. You can further refine your budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others. AWS Budgets cannot provide billing details for the past month.']
Question: A company based in Sydney hosts its application on an Amazon Elastic Compute Cloud (Amazon EC2) instance in ap-southeast-2. They would like to deploy the same Amazon EC2 instances in eu-south-1. Which of the following AWS entities can address this use case?
Has explanation: True
Explanation: Amazon Machine Image (AMI) 

An Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an Amazon Machine Image (AMI) when you launch an instance. You can launch multiple instances from a single Amazon Machine Image (AMI) when you need multiple instances with the same configuration. 

How to use an Amazon Machine Image (AMI): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html
Has option_explanations: True
Option explanations: ['Amazon EBS Elastic Volume snapshots - An Amazon EBS snapshot is a point-in-time copy of your Amazon EBS volume. EBS snapshots are one of the components of an AMI, but EBS snapshots alone cannot be used to deploy the same EC2 instances across different Availability Zones (AZs). \n\nReference: \n\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html', 'AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. It cannot be used to deploy the same EC2 instances across different Availability Zones (AZs).', 'Amazon Machine Image (AMI) \n\nAn Amazon Machine Image (AMI) provides the information required to launch an instance. You must specify an Amazon Machine Image (AMI) when you launch an instance. You can launch multiple instances from a single Amazon Machine Image (AMI) when you need multiple instances with the same configuration. \n\nHow to use an Amazon Machine Image (AMI): https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html', 'Elastic Load Balancing (ELB) - Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). It cannot be used to deploy the same EC2 instances across different Availability Zones (AZs).']
Question: Which of the following billing timeframes is applied when running a Windows EC2 on-demand instance?
Has explanation: True
Explanation: Pay per second 

With On-Demand instances, you only pay for the Amazon EC2 instances you use. The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs. 

When running a Windows EC2 on-demand instance, pay-per-second pricing is applied.
Has option_explanations: True
Option explanations: ['Pay per minute - Pay per minute pricing is not available for Windows EC2 on-demand instances, or any other type of on-demand EC2 instance.', 'Pay per hour - When running an Amazon Windows EC2 On-demand instance, pay-per-second pricing is applied. Windows-based EC2 instances used to follow pay-per-hour pricing earlier.', 'Pay per day - Pay per day pricing is not available for Windows EC2 on-demand instances, or any other type of on-demand EC2 instance. \n\nReference: \n\nhttps://aws.amazon.com/ec2/pricing/', 'Pay per second \n\nWith On-Demand instances, you only pay for the Amazon EC2 instances you use. The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs. \n\nWhen running a Windows EC2 on-demand instance, pay-per-second pricing is applied.']
Question: Adding more CPU/RAM to an Amazon Elastic Compute Cloud (Amazon EC2) instance represents which of the following?
Has explanation: True
Explanation: Vertical scaling 

A vertically scalable system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.
Has option_explanations: True
Option explanations: ['Horizontal scaling - A horizontally scalable system is one that can increase capacity by adding more computers to the system.', 'Vertical scaling \n\nA vertically scalable system is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory, or storage.', "Managing increasing volumes of data - Traditional data storage and analytics tools can no longer provide the agility and flexibility required to deliver relevant business insights. That\x92s why many organizations are shifting to a data lake architecture. A data lake is an architectural approach that allows you to store massive amounts of data in a central location so that it's readily available to be categorized, processed, analyzed, and consumed by diverse groups within your organization.", 'Loose coupling - As application complexity increases, a desirable attribute of an IT system is that it can be broken into smaller, loosely coupled components. This means that IT systems should be designed in a way that reduces interdependencies\x97a change or a failure in one component should not cascade to other components. \n\nReference: \n\nhttps://wa.aws.amazon.com/wat.concept.horizontal-scaling.en.html']
Question: A company needs to keep sensitive data in its own data center due to compliance but would still like to deploy resources using AWS. Which Cloud deployment model does this refer to?
Has explanation: True
Explanation: Hybrid Cloud 

A hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. 

Overview of Cloud Computing Deployment Models: via - https://aws.amazon.com/types-of-cloud-computing/
Has option_explanations: True
Option explanations: ['On-premises - This is not a cloud deployment model. When an enterprise opts for on-premises, it needs to create, upgrade, and scale the on-premise IT infrastructure by investing in sophisticated hardware, compatible software, and robust services. Also, the business needs to deploy dedicated IT staff to upkeep, scale, and manage the on-premise infrastructure continuously. \n\nReference: \n\nhttps://docs.aws.amazon.com/whitepapers/latest/public-sector-cloud-transformation/selecting-the-right-cloud-for-workloads-differences-between-public-private-and-hybrid.html', 'Private Cloud - Unlike a Public cloud, a Private cloud enables businesses to avail IT services that are provisioned and customized according to their precise needs. The business can further avail the IT services securely and reliably over a private IT infrastructure.', 'Public Cloud - A public cloud-based application is fully deployed in the cloud and all parts of the application run in the cloud. Applications in the cloud have either been created in the cloud or have been migrated from an existing infrastructure to take advantage of the benefits of cloud computing.', "Hybrid Cloud \n\nA hybrid deployment is a way to connect infrastructure and applications between cloud-based resources and existing resources that are not located in the cloud. The most common method of hybrid deployment is between the cloud and existing on-premises infrastructure to extend, and grow, an organization's infrastructure into the cloud while connecting cloud resources to the internal system. \n\nOverview of Cloud Computing Deployment Models: via - https://aws.amazon.com/types-of-cloud-computing/"]
Question: Which of the following options are the benefits of using AWS Elastic Load Balancing (ELB)? (Select TWO)
Has explanation: True
Explanation: Fault tolerance 

Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). 

Elastic Load Balancing (ELB) offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant: Application Load Balancer (best suited for HTTP and HTTPS traffic), Network Load Balancer (best suited for TCP traffic), and Classic Load Balancer.
Has option_explanations: True
Option explanations: ['Less costly - AWS Elastic Load Balancing (ELB) does not help with reducing costs', 'Agility - Agility refers to new IT resources being only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes. AWS Elastic Load Balancing (ELB) does not help with agility.', 'Fault tolerance \n\nElastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs). \n\nElastic Load Balancing (ELB) offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant: Application Load Balancer (best suited for HTTP and HTTPS traffic), Network Load Balancer (best suited for TCP traffic), and Classic Load Balancer.', 'Storage - AWS Elastic Load Balancing (ELB) does not offer storage benefits. It is not a storage-related service. \n\nReference: \n\nhttps://aws.amazon.com/elasticloadbalancing/', 'High availability']
Question: Which of the following services are provided by Amazon Route 53? (Select Two)
Has explanation: True
Explanation: Health checks and monitoring 

Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. 

Amazon Route 53 offers domain name registration services, where you can search for and register available domain names or transfer in existing domain names to be managed by Route 53. 

Amazon Route 53 can monitor the health and performance of your application as well as your web servers and other resources.
Has option_explanations: True
Option explanations: ['Health checks and monitoring \n\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating names like www.example.com into the numeric IP addresses like 192.0.2.1 that computers use to connect to each other. \n\nAmazon Route 53 offers domain name registration services, where you can search for and register available domain names or transfer in existing domain names to be managed by Route 53. \n\nAmazon Route 53 can monitor the health and performance of your application as well as your web servers and other resources.', "Transfer acceleration - Transfer acceleration is a feature of Amazon's simple storage service (Amazon S3). Amazon S3 Transfer Acceleration (Amazon S3TA) can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. \n\nReference: \n\nhttps://aws.amazon.com/route53/", 'Domain registration', 'IP routing - Despite its name, Amazon Route 53 does not offer IP routing. However, it can route traffic based on multiple criteria, such as endpoint health, geographic location, and latency, using routing policies.', 'Load balancing - It is a feature of Elastic Load Balancing (ELB) and not Amazon Route 53. Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone (AZ) or across multiple Availability Zones (AZs).']
Question: According to the AWS Shared Responsibility Model, which of the following is the responsibility of the customer?
Has explanation: True
Explanation: Firewall & networking configuration of Amazon Elastic Compute Cloud (Amazon EC2) 

The customers are responsible for "Security IN the cloud". It includes the configuration of the operating system, network & firewall of applications.
Has option_explanations: True
Option explanations: ['Protecting hardware infrastructure', 'Edge locations security \n\nAWS is responsible for "Security OF the cloud". It includes the infrastructure, which is composed of the hardware, software, networking, and facilities that run AWS Cloud services. \n\nReference: \n\nhttps://aws.amazon.com/compliance/shared-responsibility-model/', 'Managing Amazon DynamoDB - Amazon DynamoDB is a fully managed service. AWS operates the infrastructure layer, the operating system, and platforms, and customers access the endpoints to store and retrieve data.', 'Firewall & networking configuration of Amazon Elastic Compute Cloud (Amazon EC2) \n\nThe customers are responsible for "Security IN the cloud". It includes the configuration of the operating system, network & firewall of applications.']
Question: According to the AWS Well-Architected Framework, which of the following statements are recommendations in the Operational Excellence pillar? (Select two)
Has explanation: True
Explanation: Anticipate failure
Has option_explanations: True
Option explanations: ['Enable traceability - Monitor, alert, and audit actions and changes to your environment in real-time. Integrate logs and metrics with systems to automatically respond and take action. It is a design principle of the Security pillar.', "Automatically recover from failure - By monitoring a system for key performance indicators (KPIs), you can trigger automation when a threshold is breached. This allows for automatic notification and tracking of failures, and for automated recovery processes that work around or repair the failure. With more sophisticated automation, it's possible to anticipate and remediate failures before they occur. It is a design principle of the Reliability pillar.", 'Anticipate failure', 'Use serverless architectures - In the cloud, serverless architectures remove the need for you to run and maintain servers to carry out traditional compute activities. For example, storage services can act as static websites, removing the need for web servers, and event services can host your code for you. This not only removes the operational burden of managing these servers but also can lower transactional costs because these managed services operate at a cloud scale. It is a design principle of the Performance Efficiency pillar. \n\nReference: \n\nhttps://wa.aws.amazon.com/index.en.html', 'Make frequent, small, reversible changes \n\nThe Operational Excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures. \n\nPerform \x93pre-mortem\x94 exercises to identify potential sources of failure so that they can be removed or mitigated. Test your failure scenarios and validate your understanding of their impact. Test your response procedures to ensure that they are effective, and that teams are familiar with their execution. Set up regular game days to test workloads and team responses to simulated events. \n\nDesign workloads to allow components to be updated regularly. Make changes in small increments that can be reversed if they fail (without affecting customers when possible). \n\nThe AWS Well-Architected Framework helps you understand the pros and cons of the decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. \n\nThe AWS Well-Architected Framework is based on six pillars \x97 Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability. \n\nOverview of the six pillars of the AWS Well-Architected Framework: via - https://aws.amazon.com/architecture/well-architected/']
Question: A corporation would like to simplify access management to multiple AWS accounts as well as facilitate AWS Single Sign-On (AWS SSO) access to its AWS accounts. As a Cloud Practitioner, which AWS service would you use for this task?
Has explanation: True
Explanation: AWS IAM Identity Center 

AWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both. 

You can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access their assigned AWS accounts or cloud applications. 

You can use IAM Identity Center to quickly and easily assign and manage your employees access to multiple AWS accounts, SAML-enabled cloud applications (such as Salesforce, Microsoft 365, and Box), and custom-built in-house applications, all from a central place. 

How AWS IAM Identity Center works: via - https://aws.amazon.com/iam/identity-center/
Has option_explanations: True
Option explanations: ['AWS IAM Identity Center \n\nAWS IAM Identity Center is the successor to AWS Single Sign-On (AWS SSO). It is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create or connect, your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, or to both. \n\nYou can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. Your workforce users get a user portal to access their assigned AWS accounts or cloud applications. \n\nYou can use IAM Identity Center to quickly and easily assign and manage your employees\x92 access to multiple AWS accounts, SAML-enabled cloud applications (such as Salesforce, Microsoft 365, and Box), and custom-built in-house applications, all from a central place. \n\nHow AWS IAM Identity Center works: via - https://aws.amazon.com/iam/identity-center/', 'AWS Command Line Interface (CLI) - The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. It is not a central user portal. \n\nReference: \n\nhttps://aws.amazon.com/iam/identity-center/', 'AWS Identity and Access Management (AWS IAM) - AWS Identity and Access Management (AWS IAM) enables you to securely control access to AWS services and resources for your users. Using AWS IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. It is not used to log in but to manage users and roles.', 'AWS Cognito - Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. With Amazon Cognito, you also have the option to authenticate users through social identity providers such as Facebook, Twitter, or Amazon, with SAML identity solutions, or by using your own identity system. It is an identity management solution for customers/developers building B2C or B2B apps for their customers.']
Question: A company would like to move 50 petabytes (PBs) of data from its on-premises data centers to AWS in the MOST cost-effective way. As a Cloud Practitioner, which of the following solutions would you choose?
Has explanation: True
Explanation: AWS Snowmobile 

AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.
Has option_explanations: True
Option explanations: ['AWS Storage Gateway - AWS Storage Gateway is a hybrid storage service that enables your on-premises applications to seamlessly use AWS cloud storage. You can use the service for backup and archiving, disaster recovery, cloud data processing, storage tiering, and migration. However, data transfer through AWS Storage Gateway takes longer even with great bandwidth. Moreover, transferring 50 PBs of data will be more expensive than using AWS Snowmobile. \n\nReference: \n\nhttps://aws.amazon.com/snowmobile/', 'AWS Snowmobile \n\nAWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. AWS Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.', 'AWS Snowball - AWS Snowball is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of AWS. The use of Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns. Transferring data with AWS Snowball is simple, fast, secure, and can be as little as one-fifth the cost of high-speed Internet. However, one Snowball only provides up to 80 TB of capacity. Therefore, to transfer 50 PBs, AWS Snowball is not the most cost-effective option.', 'AWS Snowball Edge - AWS Snowball Edge is an edge computing and data transfer device provided by the AWS Snowball service. It has onboard storage and compute power that provides select AWS services for use in edge locations. However, one AWS Snowball Edge only provides up to 100 TB of capacity. Therefore, to transfer 50 PBs, AWS Snowball Edge is not the most cost-effective option.']
Question: Which AWS tool can provide best practice recommendations for performance, service limits, and cost optimization?
Has explanation: True
Explanation: AWS Trusted Advisor 

AWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. 

How AWS Trusted Advisor works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/
Has option_explanations: True
Option explanations: ['AWS Trusted Advisor \n\nAWS Trusted Advisor is an online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment. AWS Trusted Advisor provides real-time guidance to help you provision your resources following AWS best practices. \n\nHow AWS Trusted Advisor works: via - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/', 'Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on your Amazon EC2 instances. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices. Nevertheless, it does not provide best practice recommendations.', 'AWS Health Dashboard - Service health - AWS Health Dashboard - Service health publishes most up-to-the-minute information on the status and availability of all AWS services in tabular form for all Regions that AWS is present in. It does not provide best practice recommendations.', 'Amazon CloudWatch - Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. Amazon CloudWatch provides data and actionable insights to monitor applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. This is an excellent service for building Resilient systems. Think resource performance monitoring, events, and alerts; think Amazon CloudWatch. Amazon CloudWatch does not provide best practice recommendations. \n\nReference: \nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor//']
Question: What is the primary use case for Amazon GuardDuty?
Has explanation: True
Explanation: Detecting malicious activity and threats in your AWS accounts and workloads 

Amazon GuardDuty is a managed threat detection service that identifies potential security risks by analyzing logs, such as AWS CloudTrail, VPC Flow Logs, and DNS query logs. It helps organizations detect suspicious or malicious activity in their AWS environments. 

Amazon GuardDuty Overview: via - https://aws.amazon.com/guardduty/
Has option_explanations: True
Option explanations: ['Protecting web applications from common exploits and vulnerabilities such as SQL injection - This is the primary use case for AWS Web Application Firewall (WAF), which protects web applications from vulnerabilities like SQL injection and cross-site scripting. GuardDuty detects threats but does not directly protect applications. \n\nReference: \n\nhttps://aws.amazon.com/guardduty/', 'Encrypting data in transit between AWS services using TLS certificates. - AWS Certificate Manager (ACM) or AWS Key Management Service (KMS) is responsible for managing encryption and TLS certificates. GuardDuty does not handle encryption but focuses on identifying potential security threats.', 'Detecting malicious activity and threats in your AWS accounts and workloads \n\nAmazon GuardDuty is a managed threat detection service that identifies potential security risks by analyzing logs, such as AWS CloudTrail, VPC Flow Logs, and DNS query logs. It helps organizations detect suspicious or malicious activity in their AWS environments. \n\nAmazon GuardDuty Overview: via - https://aws.amazon.com/guardduty/', 'Enforcing secure communication between VPCs using network traffic filtering - This is a feature provided by AWS Network Firewall or Security Groups, which allow you to control network traffic into and out of your VPCs. GuardDuty detects malicious activities but does not enforce network traffic filtering.']
Question: Which of the following AWS Identity and Access Management (AWS IAM) Security Tools allows you to review permissions granted to an IAM user?
Has explanation: True
Explanation: AWS Identity and Access Management (IAM) access advisor 

IAM Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.
Has option_explanations: True
Option explanations: ['AWS Identity and Access Management (IAM) access advisor \n\nIAM Access advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies. To summarize, you can identify unnecessary permissions so that you can revise your IAM policies accordingly.', 'Multi-Factor Authentication (MFA) - Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password. with Multi-Factor Authentication (MFA) enabled, when a user signs in to an AWS Management Console, they will be prompted for their user name and password (the first factor\x97what they know), as well as for an authentication code from their AWS MFA device (the second factor\x97what they have). Taken together, these multiple factors provide increased security for your AWS account settings and resources. It cannot be used to review permissions granted. \n\nReference: \n\nhttps://aws.amazon.com/about-aws/whats-new/2019/06/now-use-iam-access-advisor-with-aws-organizations-to-set-permission-guardrails-confidently/', 'IAM policy - IAM policies define permissions for an action regardless of the method that you use to perform the operation.', 'IAM credentials report - You can generate and download a credential report that lists all IAM users in your account and the status of their various credentials, including passwords, access keys, and multi-factor authentication (MFA) devices. It is not used to review permissions granted to an IAM user.']
Question: Which of the following are the advantages of using the AWS Cloud? (Select TWO)
Has explanation: True
Explanation: Stop guessing about capacity
Has option_explanations: True
Option explanations: ['Stop guessing about capacity', 'Increase speed and agility', 'Limited scaling - Scaling is not limited in the cloud. You can access as much or as little capacity as you need, and scale up and down as required with only a few minutes\x92 notice.', 'AWS is responsible for security in the cloud - AWS is responsible for the security OF the cloud, which means AWS is responsible for protecting the infrastructure that runs all the services offered in the AWS Cloud.', 'Trade operational expense for capital expense - In the cloud, you trade capital expense (CAPEX) for the operational expense (OPEX). Instead of having to invest heavily in data centers and servers before you know how you\x92re going to use them, you can pay only when you consume computing resources, and pay only for how much you consume. \n\nReference: \n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html']
Question: A company using a hybrid cloud would like to store secondary backup copies of the on-premises data. Which Amazon S3 Storage Class would you use for a cost-optimal yet rapid access solution?
Has explanation: True
Explanation: Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) 

Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single Availability Zone (AZ) and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. Its a good choice for storing secondary backup copies of on-premises data or easily re-creatable data. You can also use it as cost-effective storage for data that is replicated from another AWS Region using S3 cross-region replication (S3 CRR).
Has option_explanations: True
[2025-02-23 06:48:53 +0000] [70108] [ERROR] Worker (pid:70110) was sent SIGHUP!
[2025-02-23 06:48:53 +0000] [70108] [ERROR] Worker (pid:70111) was sent SIGHUP!
[2025-02-23 06:48:53 +0000] [70108] [ERROR] Worker (pid:70109) was sent SIGHUP!
[2025-02-23 06:48:53 +0000] [70718] [INFO] Booting worker with pid: 70718
[2025-02-23 06:48:54 +0000] [70731] [INFO] Booting worker with pid: 70731
[2025-02-23 06:48:54 +0000] [70108] [INFO] Handling signal: hup
[2025-02-23 06:48:54 +0000] [70108] [INFO] Hang up: Master
[2025-02-23 06:48:54 +0000] [70751] [INFO] Booting worker with pid: 70751
[2025-02-23 06:48:54 +0000] [70752] [INFO] Booting worker with pid: 70752
[2025-02-23 06:48:54 +0000] [70753] [INFO] Booting worker with pid: 70753
[2025-02-23 06:48:56 +0000] [70718] [INFO] Worker exiting (pid: 70718)
[2025-02-23 06:48:56 +0000] [70731] [INFO] Worker exiting (pid: 70731)
[2025-02-23 06:48:56 +0000] [70108] [ERROR] Worker (pid:70718) was sent SIGTERM!
[2025-02-23 06:48:56 +0000] [70108] [ERROR] Worker (pid:70731) was sent SIGTERM!
[2025-02-23 11:11:50 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:70753)
[2025-02-23 11:11:50 +0000] [70753] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 274, in parse
    line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 326, in read_line
    self.get_data(unreader, buf)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-23 11:11:50 +0000] [70753] [INFO] Worker exiting (pid: 70753)
[2025-02-23 11:11:50 +0000] [79750] [INFO] Booting worker with pid: 79750
[2025-02-23 14:55:59 +0000] [70752] [WARNING] Invalid request from ip=199.45.154.124: Invalid HTTP Version: (2, 0)
[2025-02-23 14:56:07 +0000] [79750] [WARNING] Invalid request from ip=199.45.154.124: Invalid HTTP Version: (2, 0)
[2025-02-24 11:49:54 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:79750)
[2025-02-24 11:49:54 +0000] [79750] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 274, in parse
    line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 326, in read_line
    self.get_data(unreader, buf)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-24 11:49:54 +0000] [79750] [INFO] Worker exiting (pid: 79750)
Login POST request received
Form data: ImmutableMultiDict([('csrf_token', 'IjlkY2Q2ZDE2OWJkMjlmZjg0OTJjYjAxNzdlYTg2NDY4MWY5NjIwYzIi.Z7tNyA.0kKuZJok5j74cwSipsmZH2Otfms'), ('email', 'samotra2@illinois.edu'), ('password', 'Aamish@123')])
CSRF token: IjlkY2Q2ZDE2OWJkMjlmZjg0OTJjYjAxNzdlYTg2NDY4MWY5NjIwYzIi.Z7tNyA.0kKuZJok5j74cwSipsmZH2Otfms
[2025-02-24 11:49:54 +0000] [130450] [INFO] Booting worker with pid: 130450
[2025-02-25 04:52:24 +0000] [70751] [WARNING] Invalid request from ip=162.142.125.42: Invalid HTTP Version: (2, 0)
[2025-02-25 04:52:58 +0000] [130450] [WARNING] Invalid request from ip=162.142.125.42: Invalid HTTP Version: (2, 0)
[2025-02-25 05:34:26 +0000] [70751] [WARNING] Invalid request from ip=206.168.34.82: Invalid HTTP Version: (2, 0)
[2025-02-25 05:34:40 +0000] [130450] [WARNING] Invalid request from ip=206.168.34.82: Invalid HTTP Version: (2, 0)
[2025-02-25 06:00:57 +0000] [70751] [WARNING] Invalid request from ip=8.222.148.186: Invalid HTTP request line: ''
[2025-02-26 03:14:34,957] ERROR in app: Exception on /login [HEAD]
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/flask/app.py", line 1455, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/flask/app.py", line 870, in full_dispatch_request
    return self.finalize_request(rv)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/flask/app.py", line 889, in finalize_request
    response = self.make_response(rv)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/flask/app.py", line 1161, in make_response
    raise TypeError(
TypeError: The view function for 'auth.login' did not return a valid response. The function either returned None or ended without a return statement.
[2025-02-26 03:24:18 +0000] [70751] [WARNING] Invalid request from ip=47.237.15.131: Invalid HTTP request line: ''
[2025-02-26 06:32:55 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:70752)
[2025-02-26 06:32:55 +0000] [70752] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 274, in parse
    line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 326, in read_line
    self.get_data(unreader, buf)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-26 06:32:55 +0000] [70752] [INFO] Worker exiting (pid: 70752)
[2025-02-26 06:32:55 +0000] [220160] [INFO] Booting worker with pid: 220160
[2025-02-27 02:07:31 +0000] [70751] [WARNING] Invalid request from ip=47.237.5.190: Invalid HTTP request line: ''
[2025-02-27 05:39:12 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:220160)
[2025-02-27 05:39:12 +0000] [220160] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 274, in parse
    line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 326, in read_line
    self.get_data(unreader, buf)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-27 05:39:12 +0000] [220160] [INFO] Worker exiting (pid: 220160)
[2025-02-27 05:39:12 +0000] [268194] [INFO] Booting worker with pid: 268194
[2025-02-27 05:39:22 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:130450)
[2025-02-27 05:39:22 +0000] [130450] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 274, in parse
    line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 326, in read_line
    self.get_data(unreader, buf)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-27 05:39:22 +0000] [130450] [INFO] Worker exiting (pid: 130450)
[2025-02-27 05:39:23 +0000] [268259] [INFO] Booting worker with pid: 268259
[2025-02-27 05:39:33 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:70751)
[2025-02-27 05:39:33 +0000] [70751] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 274, in parse
    line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 326, in read_line
    self.get_data(unreader, buf)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-27 05:39:33 +0000] [70751] [INFO] Worker exiting (pid: 70751)
[2025-02-27 05:39:33 +0000] [268261] [INFO] Booting worker with pid: 268261
[2025-02-27 06:08:20 +0000] [268194] [WARNING] Invalid request from ip=167.94.138.164: Invalid HTTP Version: (2, 0)
[2025-02-27 06:08:42 +0000] [268194] [WARNING] Invalid request from ip=167.94.138.164: Invalid HTTP Version: (2, 0)
[2025-02-28 02:56:05 +0000] [268194] [WARNING] Invalid request from ip=52.15.131.40: Invalid HTTP request line: 'SSH-2.0-Go'
[2025-02-28 05:15:34 +0000] [268194] [WARNING] Invalid request from ip=8.219.190.90: Invalid HTTP request line: ''
[2025-02-28 14:49:36 +0000] [70108] [CRITICAL] WORKER TIMEOUT (pid:268261)
[2025-02-28 14:49:36 +0000] [268261] [ERROR] Error handling request (no URI read)
Traceback (most recent call last):
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/sync.py", line 133, in handle
    req = next(parser)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/parser.py", line 41, in __next__
    self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 259, in __init__
    super().__init__(cfg, unreader, peer_addr)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 60, in __init__
    unused = self.parse(self.unreader)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 271, in parse
    self.get_data(unreader, buf, stop=True)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/message.py", line 262, in get_data
    data = unreader.read()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 36, in read
    d = self.chunk()
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/http/unreader.py", line 63, in chunk
    return self.sock.recv(self.mxchunk)
  File "/home/ec2-user/cert-prep/venv/lib64/python3.9/site-packages/gunicorn/workers/base.py", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
[2025-02-28 14:49:36 +0000] [268261] [INFO] Worker exiting (pid: 268261)
[2025-02-28 14:49:37 +0000] [335967] [INFO] Booting worker with pid: 335967
[2025-02-28 17:53:14 +0000] [268194] [WARNING] Invalid request from ip=167.94.138.122: Invalid HTTP Version: (2, 0)
[2025-02-28 17:53:23 +0000] [268259] [WARNING] Invalid request from ip=167.94.138.122: Invalid HTTP Version: (2, 0)
